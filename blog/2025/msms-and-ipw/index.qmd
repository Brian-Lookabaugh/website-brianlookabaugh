---
title: "Explaining Everything About Marginal Structural Models and Inverse Probability Weighting in Plain English"
date: 2025-03-14
description: "BLOG IN PROGRESS - Marginal structural models are a necessary method in the toolkit of panel/longitudinal data analysis. Despite this, outside of a few disciplines, they are rarely utilized! Check this blog out to learn more about these tools and why they are so crucial to understand."
image: ""
toc: true
execute:
  warning: false
  error: false
  message: false
toc-location: "left"
toc-title: "Contents"
draft: true
categories:
  - causal inference
  - ipw
  - msm
  - panel data
  - sensitivity analysis
---

Over the past year, I have tried to pay extra attention towards a problem that, unfortunately, flies under the radar for many data/social scientists. Many are familiar with the problem of *confounding*; when an exposure ($X$) shares a common cause with an outcome ($Y$). Confounding is present in any observational study and the burden is on the observational researcher to identify these confounders and control for them to avoid spurious causal inferences. 

However, what is less appreciated are the unique problems that arise as a consequence of *time-varying* confounding. In short, time-varying confounding refers to any feedback loop between two variables (whether they are confounders, exposures, outcomes, etc.) that results in confounding. This could be a direct loop between a confounder and an exposure ($Z_{t-1} \rightarrow X_{t-1} \rightarrow Z{t} \rightarrow X_{t}$) or a loop between an exposure and an outcome which render past outcome values as confounders of the contemporaneous exposure $rightarrow$ outcome relationship ($X_{t-1} \rightarrow Y_{t-1} \rightarrow X{t} \rightarrow Y_{t}$). 

In panel/longitudinal settings, it is almost a guarantee that some degree of time-varying confounding is present. And, as if identifying and controlling for *all* confounders wasn't enough, time-varying confounding renders all of our comfortable and standard estimation methods inapplicable. I won't go into great detail about what time-varying confounding is *here*, but I dedicated a much greater volume of discussion about this very topic in a [prior blog post](https://brian-lookabaugh.github.io/website-brianlookabaugh/blog/2024/simulating-panel-data/#the-problems-with-panel-data) and I highly recommend you check out the section on time-varying confounding if you are not already familiar. In fact, this blog is basically a direct sequel.

# What is a Marginal Structural Model?

We'll start by breaking down the mystique of what a "marginal structural model" even is. And the easiest way to do that is to go word-by-word and explain what the "marginal" and "structural" refer to.

The term "marginal" has got to be one of the most confusing words in statistics because it can refer to several different things. In the case of MSMs, the "marginal" refers to *population-level* effects the model seeks to estimate. In particular, the MSM attempts to model the marginal (population-level) mean of each potential outcome. In less confusing language, the model we are trying to estimate seeks to provide the average outcome value for hypothetical populations in which *all* units are treated and the average outcome value for another hypothetical population in which *no* units are treated. If treatment is randomly assigned (and, if not, if all sources of confounding are controlled for), then the MSM provides the difference in outcome between these two populations. In other words, this difference if our average causal effect.

If you're wondering, yes, MSMs will estimate the average treatment effect (ATE) that you might be familiar with as the target estimand for a randomized controlled trial. In fact, the reason why we estimate the ATE with MSMs rather than other estimands popular under selection on observables (such as the average treatment effect on the treated - ATT) highlights the difference between MSMs and other more standard methods, such as regression adjustment or matching. 

Under methods like regression adjustment with average marginal effects, we might estimate something like the ATT, where we have a population of treated units and, using the non-treated units, effectively predict what the outcome would be like for these *actual* treated units had they *not* been treated. With a MSM, we instead estimate *pseudo-populations* that reflect the distribution of the outcome for treated and non-treated units *as if* they had been randomly assigned. Notice the key difference? While these methods are the similar in that they rely on the researcher to identify a set of variables to adjust for the de-confound the estimate, they adjust for such confounders in very different ways.

Standard regression adjustment uses a model that adjusts for confounders to predict counterfactual states for *observed* units. With this approach, we have one *real* population and use our model to impute one *counterfactual* population. Both of these populations are confounded, but we deal with that by controlling for these confounders. In contrast, the MSM models the difference between *pseudo-populations* (aka: not real populations observed in real life) that are *not* confounded. If the concept of a "pseudo-population" makes you feel sketched-out, know that 1) the pseudo-populations are constructed using real-word data, and 2) a randomized controlled trial (RCT) also leads to comparing outcomes between pseudo-populations because actual treatments (such as medicinal treatments) are rarely truly randomly assigned in the real-world. Even if the pseudo-populations don't reflect real-world populations, they are helpful in determining whether a treatment *will have an effect in the real world*! After all, causal inference is all about estimating unknowable things that didn't happen (that's the whole point of the counterfactual approach to casual inference), so the seasoned causal inference researcher should not be disturbed by the interpretation of an MSM.

These approaches represent two strategies designed to adjust for confounding and, assuming that all confounders are specified, are equally valid. By "equally valid", I mean that they are both fully capable of estimating causal effects of interest. With regression adjustment, we get our causal effects by estimating a regression model with the outcome on the left-hand side of the equation and the treatment and confounders on the right-hand side of the equation. We then use the model to predict the outcome for each of the treated units, but we instead change the treatment value for these units to "non-treated" in the equation and compare the average difference between the actual predictions for the treated units versus the counterfactual non-treated predictions. But how do we go about actually estimating the MSM? How do we go about creating psuedo-populations that are not confounded? How does the MSM approach handle the problem of time-varying confounding in the way that standard methods don't? That'll be tackled in the following section! 

# How Do We Estimate MSMs? (IPW)

# Step 1: Identify an Adjustment Set

# Step 2: Estimate the Propensity Score

# Step 3: Invert the Propensity Score

# Step 4: Run the Weighted Model

# Step 5: Model Diagnostics

# Step 6: Sensitivity Analysis

# Simulation Study Set-Up

# Simulation Analysis

# Conclusion