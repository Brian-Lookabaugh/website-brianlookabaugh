[
  {
    "objectID": "resume/index.html",
    "href": "resume/index.html",
    "title": "Brian Lookabaugh",
    "section": "",
    "text": "Download"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Brian Lookabaugh",
    "section": "",
    "text": "Greetings and welcome to my personal website! My name is Brian Lookabaugh and I am a Research Analyst at Fors Marsh. In my position, I leverage my skills in data analysis and statistical modeling to solve a variety of business problems. These include descriptive tasks (creating visualizations to communicate trends and patterns), predictive problems (utilizing machine learning to predict outcomes and forecast), and causal questions (drawing on my background in causal inference to answer questions such as “did X have an impact on Y?”).\n\nOutside of my professional role, I routinely refine and expand my methodological toolkit by researching new methods and applying my skill set to other recreational interests of mine. You can find some of these passion projects under the “Blog” section of this site. If you have any questions about my research, please feel free to reach out to me!"
  },
  {
    "objectID": "blog/2025/chiefs-refs-bias/index.html",
    "href": "blog/2025/chiefs-refs-bias/index.html",
    "title": "Do NFL Referees Favor the Kansas City Chiefs?",
    "section": "",
    "text": "Code\n# Load Libraries\npacman::p_load(\n  \"dplyr\", # Data Manipulation\n  \"tidyr\", # More Data Manipulation\n  \"ggplot2\", # Data Visualization\n  \"nflverse\", # NFL Data and Visualization\n  \"ggimage\", # More Data Visualization\n  install = FALSE\n)\n\n# Define a Custom Theme - Taken From Andrew Heiss's Blogs\nblog_theme &lt;- function() {\n  theme_bw() +  # Start with theme_bw\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      plot.title = element_text(face = \"bold\"),\n      axis.title = element_text(face = \"bold\"),\n      strip.text = element_text(face = \"bold\"),\n      strip.background = element_rect(fill = \"grey80\", color = NA),\n      legend.title = element_text(face = \"bold\")\n    )\n}\n\n# Clear Cache for NFL Data\nnflreadr::.clear_cache()\n\n\nThe Kansas City Chiefs are heading to their 5th Super Bowl in the past 6 years. This upcoming Super Bowl appearance against the Philadelphia Eagles is a re-match from two years ago, where the Chiefs emerged victories. If the Chiefs can beat the Eagles again, they will go down in history as the first NFL team to win three consecutive Super Bowls (a “three-peat”).\nDespite this unprecedented greatness, many fans, players, and coaches alike are skeptical. It seems that, week-after-week, a referee call just happens to go in the favor of Kansas City, and sometimes, these calls seem to impact the game. This has lead to many fans and members of the NFL to question the integrity of the game.\nIn fairness, I don’t think that anyone is claiming that the Chiefs are cheating. Instead, the claim is that, for some reason, the NFL wants the Chiefs to do well, and so they decide to give Kansas City small nudges here and there that go their way and inflate their success. Primarily, skeptics claim that the NFL does so by weaponizing the referee system. “It seems like the Chiefs get all of the calls” or “the calls just all happen to go KC’s way” or “the refs don’t call clear penalties against KC like they do towards other teams”. Is any of this true? Have your eyes deceived you? I think so! And, if you don’t believe me, let’s dig into the data!\nIn this review, we are going to look at four major claims that people make and we’re going to see if the data supports such claims. First, outright, do the refs overall favor the Chiefs above any other team? Second, do the refs help out the Chiefs in “tight spots” when it really matters (3rd downs, 4th downs, and overtime). Third, do the refs go out of their way to protect Patrick Mahomes in particular? And, lastly, does the NFL treat the Chiefs differently because of Taylor Swift and her relationship with Travis Kelce?\n\nClaim 1: Do the Refs Help Out the Chiefs?\nLet’s start looking into this claim by evaluating penalty yard differentials throughout Mahomes’s tenure as KC’s starting quarterback. Why penalty yard differential (PYD)? Here is the logic: while some calls may go your way in a given game, some calls will go the other way. What we care about is this: which team got helped out by the refs more? PYD is a way to evaluate that. If a team has a positive PYD, that means they got more yards awarded to them by the refs than the other team. If there is a negative PYD, the other team got more yards awarded to them by the refs. The larger the PYD, the greater the discrepancy.\nNow, in a single game, you might either get unlucky, or your team was just poorly coached in that game and truly deserved all the penalties. That’s why we’re going to take a look overall.\n\n\nCode\n# Load Play-by-Play (PBP) Data\npbp_data_2018_2024 &lt;- load_pbp(seasons = 2018:2024)\n\n# Calculate Penalty Yard Differential by Team\npenalty_differential_2018_2024 &lt;- pbp_data_2018_2024 %&gt;%\n  # Only Keep Penalty Plays\n  filter(penalty == 1 & !is.na(penalty_yards)) %&gt;%\n  # Create a Long Form Version with a \"Team\" Column and a Possession/Defense Dummy\n  gather(key = \"side\", value = \"team\", posteam, defteam) %&gt;%\n  mutate(side_dummy = ifelse(side == \"posteam\", \"offense\", \"defense\")) %&gt;%\n  # Modify Penalty Yards So That If the Team Committed the Penalty, It is Negative\n  # For Them and Positive for Their Opponent\n  mutate(\n    penalty_yards_adjusted = case_when(\n      penalty_team == team ~ -penalty_yards, \n      penalty_team != team ~ penalty_yards, \n      TRUE ~ 0\n    )\n  ) %&gt;%\n  # Collapse to the Game Level\n  group_by(game_id, team, season) %&gt;%\n  summarize(\n    penalty_yard_differential = sum(penalty_yards_adjusted, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  # Collapse to the Team Level\n  group_by(team) %&gt;%\n  summarize(\n    total_penalty_yard_differential = sum(penalty_yard_differential, na.rm = TRUE),\n    .groups = 'drop'\n  )\n\n# Add Team Logos for Plotting\nteam_logos &lt;- nflfastR::teams_colors_logos %&gt;%\n  select(team_abbr, team_logo_espn)\n\n# Merge Penalty Data with Logos\npenalty_differential_2018_2024 &lt;- penalty_differential_2018_2024 %&gt;%\n  left_join(team_logos, by = c(\"team\" = \"team_abbr\"))\n\n# Create a Diverging Bar Chart\nggplot(penalty_differential_2018_2024, aes(x = reorder(team, total_penalty_yard_differential), y = total_penalty_yard_differential)) +\n  geom_bar(stat = \"identity\", aes(fill = total_penalty_yard_differential)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", \n             color = \"black\", linewidth = 1.2) +\n  geom_image(aes(image = team_logo_espn), size = 0.05) +\n  scale_fill_gradient2(\n    low = \"#8b0100\",  \n    high = \"#0f3860\",\n    midpoint = 0\n  ) +\n  labs(\n    x = \"\",\n    y = \"Penalty Yard Differential\"\n  ) +\n  blog_theme() + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")\n\n\n\n\n\nPenalty Yard Differential by Team (2018-2024)\n\n\n\n\nNow, that’s interesting! Not only is KC nowhere near the top of PYD over the past 7 years, they are near the very bottom! Okay, but you might think “back then, the Chiefs weren’t the favorites in the NFL, but now they are. And, I’ve watched their games all year this season and clearly see them getting bailed out.” Fair enough… so let’s look at PYD for just the 2024 NFL season.\n\n\nCode\n# Load Play-by-Play (PBP) Data\npbp_data_2024 &lt;- load_pbp(seasons = 2024)\n\n# Calculate Penalty Yard Differential by Team\npenalty_differential_2024 &lt;- pbp_data_2024 %&gt;%\n  # Only Keep Penalty Plays\n  filter(penalty == 1 & !is.na(penalty_yards)) %&gt;%\n  # Create a Long Form Version with a \"Team\" Column and a Possession/Defense Dummy\n  gather(key = \"side\", value = \"team\", posteam, defteam) %&gt;%\n  mutate(side_dummy = ifelse(side == \"posteam\", \"offense\", \"defense\")) %&gt;%\n  # Modify Penalty Yards So That If the Team Comitted the Penalty, It is Negative\n  # For Them and Positive for Their Opponent\n  mutate(\n    penalty_yards_adjusted = case_when(\n      penalty_team == team ~ -penalty_yards, \n      penalty_team != team ~ penalty_yards, \n      TRUE ~ 0\n    )\n  ) %&gt;%\n  # Collapse to the Game Level\n  group_by(game_id, team, season) %&gt;%\n  summarize(\n    penalty_yard_differential = sum(penalty_yards_adjusted, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  # Collapse to the Season Level\n  group_by(team) %&gt;%\n  summarize(\n    total_penalty_yard_differential = sum(penalty_yard_differential, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  left_join(team_logos, by = c(\"team\" = \"team_abbr\"))\n\nggplot(penalty_differential_2024, aes(x = reorder(team, total_penalty_yard_differential), \n             y = total_penalty_yard_differential)) +\n  geom_bar(stat = \"identity\", aes(fill = total_penalty_yard_differential)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", \n             color = \"black\", linewidth = 1.2) +\n  geom_image(aes(image = team_logo_espn), size = 0.05) +\n  scale_fill_gradient2(\n    low = \"#8b0100\",  \n    high = \"#0f3860\",\n    midpoint = 0\n  ) +\n  labs(\n    x = \"\",\n    y = \"Penalty Yard Differential\"\n  ) +\n  blog_theme() + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")\n\n\n\n\n\nPenalty Yard Differential by Team (2024)\n\n\n\n\nLook, the Chiefs are 8th in PYD for this season! Yes, they are. However, they are only 8th… If the NFL is using the refs to help out KC, then are they also doing the same to help out their rivals, Buffalo? What about the Cinderella story of the Vikings this year? They are nowhere close to anyone else. Or, what about my Dallas Cowboys, who have a near identical PYD to the Chiefs? Is the NFL engaged in a conspiracy to help them out? I don’t think so… So, we’ve seen that the Chiefs have a low PYD overall but had a high one this year. What about each year throughout Mahomes’s career?\n\n\nCode\n# Calculate Penalty Yard Differential by Team\npenalty_differential_over_time &lt;- pbp_data_2018_2024 %&gt;%\n  # Only Keep Penalty Plays\n  filter(penalty == 1 & !is.na(penalty_yards)) %&gt;%\n  # Create a Long Form Version with a \"Team\" Column and a Possession/Defense Dummy\n  gather(key = \"side\", value = \"team\", posteam, defteam) %&gt;%\n  mutate(side_dummy = ifelse(side == \"posteam\", \"offense\", \"defense\")) %&gt;%\n  # Modify Penalty Yards So That If the Team Comitted the Penalty, It is Negative\n  # For Them and Positive for Their Opponent\n  mutate(\n    penalty_yards_adjusted = case_when(\n      penalty_team == team ~ -penalty_yards, \n      penalty_team != team ~ penalty_yards, \n      TRUE ~ 0\n    )\n  ) %&gt;%\n  # Collapse to the Game Level\n  group_by(game_id, team, season) %&gt;%\n  summarize(\n    penalty_yard_differential = sum(penalty_yards_adjusted, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  # Collapse to the Season Level\n  group_by(team, season) %&gt;%\n  summarize(\n    total_penalty_yard_differential = sum(penalty_yard_differential, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  left_join(team_logos, by = c(\"team\" = \"team_abbr\")) %&gt;%\n  filter(team == \"KC\")\n\n# Create a Time Series Plot of Chiefs Average and League Average\nggplot(penalty_differential_over_time, aes(x = season, y = total_penalty_yard_differential)) +\n  # Plot Chiefs Line\n  geom_line(\n    color = \"#E31837\",\n    linewidth = 1 \n  ) +\n    geom_hline(yintercept = 0, linetype = \"dashed\", \n             color = \"black\", linewidth = 1.2) +\n  geom_image(aes(image = team_logo_espn), size = 0.1) +  \n  labs(\n    title = \"\",\n    x = \"\",\n    y = \"Penalty Yard Differential\",\n  ) +\n  scale_x_continuous(breaks = seq(2018, 2024, by = 1)) +\n  blog_theme() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"none\"\n  )\n\n\n\n\n\nChiefs Penalty Yard Differential Each Year (2018-2024)\n\n\n\n\nWell, that’s not entirely compelling, is it? The Chiefs are pretty consistently negativley impacted by the refs. In fact, in their three seasons where they won the Super Bowl (2019, 2022, 2023) they had a negative PYD every season. If they beat the Eagles next week, it will be the first Super Bowl victory with a positive PYD. As you can see, this is not helping the anti-Chiefs case out so far. But, let’s take a deeper look into the context of the penalties themselves. A holding call in the 1st Quarter of the game isn’t near as impactful as a holding call on 3rd down in a game-winning drive, right? So, let’s see if the refs help the Chiefs out in tight spots.\n\n\nClaim 2: Do the Refs Help Out the Chiefs in Tight Spots?\nWe’ll start by looking at PYD during the 4th Quarter. Throughout Mahomes’s entire career, have the Chiefs been given a boost during the 4th Quarter?\n\n\nCode\n# Calculate Penalty Yard Differential by Team\npenalty_differential_2018_2024_4th &lt;- pbp_data_2018_2024 %&gt;%\n  filter(qtr == 4) %&gt;%\n  filter(penalty == 1) %&gt;%\n  # Create a Long Form Version with a \"Team\" Column and a Possession/Defense Dummy\n  gather(key = \"side\", value = \"team\", posteam, defteam) %&gt;%\n  mutate(side_dummy = ifelse(side == \"posteam\", \"offense\", \"defense\")) %&gt;%\n  # Modify Penalty Yards So That If the Team Committed the Penalty, It is Negative\n  # For Them and Positive for Their Opponent\n  mutate(\n    penalty_yards_adjusted = case_when(\n      penalty_team == team ~ -penalty_yards, \n      penalty_team != team ~ penalty_yards, \n      TRUE ~ 0\n    )\n  ) %&gt;%\n  # Collapse to the Game Level\n  group_by(game_id, team, season) %&gt;%\n  summarize(\n    penalty_yard_differential = sum(penalty_yards_adjusted, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  # Collapse to the Team Level\n  group_by(team) %&gt;%\n  summarize(\n    total_penalty_yard_differential = sum(penalty_yard_differential, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  left_join(team_logos, by = c(\"team\" = \"team_abbr\"))\n\n# Create a Diverging Bar Chart\nggplot(penalty_differential_2018_2024_4th, aes(x = reorder(team, total_penalty_yard_differential), y = total_penalty_yard_differential)) +\n  geom_bar(stat = \"identity\", aes(fill = total_penalty_yard_differential)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", \n             color = \"black\", linewidth = 1.2) +\n  geom_image(aes(image = team_logo_espn), size = 0.05) +\n  scale_fill_gradient2(\n    low = \"#8b0100\",  \n    high = \"#0f3860\",\n    midpoint = 0\n  ) +\n  labs(\n    x = \"\",\n    y = \"Penalty Yard Differential\"\n  ) +\n  blog_theme() + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")\n\n\n\n\n\nChiefs Penalty Yard Differential in the 4th Quarter (2018-2024)\n\n\n\n\nYet again, we see the Chiefs near the bottom. But, I hear you, so let’s see if that was any different this season in particular.\n\n\nCode\n# Calculate Penalty Yard Differential by Team\npenalty_differential_2024_4th &lt;- pbp_data_2024 %&gt;%\n   filter(qtr == 4) %&gt;%\n  # Only Keep Penalty Plays\n  filter(penalty == 1 & !is.na(penalty_yards)) %&gt;%\n  # Create a Long Form Version with a \"Team\" Column and a Possession/Defense Dummy\n  gather(key = \"side\", value = \"team\", posteam, defteam) %&gt;%\n  mutate(side_dummy = ifelse(side == \"posteam\", \"offense\", \"defense\")) %&gt;%\n  # Modify Penalty Yards So That If the Team Comitted the Penalty, It is Negative\n  # For Them and Positive for Their Opponent\n  mutate(\n    penalty_yards_adjusted = case_when(\n      penalty_team == team ~ -penalty_yards, \n      penalty_team != team ~ penalty_yards, \n      TRUE ~ 0\n    )\n  ) %&gt;%\n  # Collapse to the Game Level\n  group_by(game_id, team, season) %&gt;%\n  summarize(\n    penalty_yard_differential = sum(penalty_yards_adjusted, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  # Collapse to the Season Level\n  group_by(team) %&gt;%\n  summarize(\n    total_penalty_yard_differential = sum(penalty_yard_differential, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  left_join(team_logos, by = c(\"team\" = \"team_abbr\"))\n\n# Create a Diverging Bar Chart\nggplot(penalty_differential_2024_4th, aes(x = reorder(team, total_penalty_yard_differential), y = total_penalty_yard_differential)) +\n  geom_bar(stat = \"identity\", aes(fill = total_penalty_yard_differential)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", \n             color = \"black\", linewidth = 1.2) +\n  geom_image(aes(image = team_logo_espn), size = 0.05) +\n  scale_fill_gradient2(\n    low = \"#8b0100\",  \n    high = \"#0f3860\",\n    midpoint = 0\n  ) +\n  labs(\n    x = \"\",\n    y = \"Penalty Yard Differential\"\n  ) +\n  blog_theme() + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")\n\n\n\n\n\nChiefs Penalty Yard Differential in the 4th Quarter (2024)\n\n\n\n\nYep, about what I expected. Even during this season (when all of the “Chiefs get all the calls” rhetoric started), the refs still give KC’s opponents more penalty yards in the 4th Quarter than the Chiefs. Next, we’re going to shift our attention towards downs. In particular, both 3rd and 4th downs since converting these are very important for sustaining drives and preventing turnover on downs.\n\n\nCode\n# Calculate Penalty Yard Differential by Team\npenalty_differential_2018_2024_3d &lt;- pbp_data_2018_2024 %&gt;%\n  filter(down == 3) %&gt;%\n  # Only Keep Penalty Plays\n  filter(penalty == 1 & !is.na(penalty_yards)) %&gt;%\n  # Create a Long Form Version with a \"Team\" Column and a Possession/Defense Dummy\n  gather(key = \"side\", value = \"team\", posteam, defteam) %&gt;%\n  mutate(side_dummy = ifelse(side == \"posteam\", \"offense\", \"defense\")) %&gt;%\n  # Modify Penalty Yards So That If the Team Committed the Penalty, It is Negative\n  # For Them and Positive for Their Opponent\n  mutate(\n    penalty_yards_adjusted = case_when(\n      penalty_team == team ~ -penalty_yards, \n      penalty_team != team ~ penalty_yards, \n      TRUE ~ 0\n    )\n  ) %&gt;%\n  # Collapse to the Game Level\n  group_by(game_id, team, season) %&gt;%\n  summarize(\n    penalty_yard_differential = sum(penalty_yards_adjusted, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  # Collapse to the Team Level\n  group_by(team) %&gt;%\n  summarize(\n    total_penalty_yard_differential = sum(penalty_yard_differential, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  left_join(team_logos, by = c(\"team\" = \"team_abbr\"))\n\n# Create a Diverging Bar Chart\nggplot(penalty_differential_2018_2024_3d, aes(x = reorder(team, total_penalty_yard_differential), y = total_penalty_yard_differential)) +\n  geom_bar(stat = \"identity\", aes(fill = total_penalty_yard_differential)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", \n             color = \"black\", linewidth = 1.2) +\n  geom_image(aes(image = team_logo_espn), size = 0.05) +\n  scale_fill_gradient2(\n    low = \"#8b0100\",  \n    high = \"#0f3860\",\n    midpoint = 0\n  ) +\n  labs(\n    x = \"\",\n    y = \"Penalty Yard Differential\"\n  ) +\n  blog_theme() + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")\n\n\n\n\n\nChiefs Penalty Yard Differential on 3rd Down (2018-2024)\n\n\n\n\nNo shocker here… KC ranks 3rd to last on PYD on 3rd down throughout Mahomes’s career and, as you can see in the plot below, the Chiefs are basically right in the middle for the 2024 season.\n\n\nCode\n# Calculate Penalty Yard Differential by Team\npenalty_differential_2024_3d &lt;- pbp_data_2024 %&gt;%\n  filter(down == 3) %&gt;%\n  # Only Keep Penalty Plays\n  filter(penalty == 1 & !is.na(penalty_yards)) %&gt;%\n  # Create a Long Form Version with a \"Team\" Column and a Possession/Defense Dummy\n  gather(key = \"side\", value = \"team\", posteam, defteam) %&gt;%\n  mutate(side_dummy = ifelse(side == \"posteam\", \"offense\", \"defense\")) %&gt;%\n  # Modify Penalty Yards So That If the Team Comitted the Penalty, It is Negative\n  # For Them and Positive for Their Opponent\n  mutate(\n    penalty_yards_adjusted = case_when(\n      penalty_team == team ~ -penalty_yards, \n      penalty_team != team ~ penalty_yards, \n      TRUE ~ 0\n    )\n  ) %&gt;%\n  # Collapse to the Game Level\n  group_by(game_id, team, season) %&gt;%\n  summarize(\n    penalty_yard_differential = sum(penalty_yards_adjusted, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  # Collapse to the Season Level\n  group_by(team) %&gt;%\n  summarize(\n    total_penalty_yard_differential = sum(penalty_yard_differential, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  left_join(team_logos, by = c(\"team\" = \"team_abbr\"))\n\n# Create a Diverging Bar Chart\nggplot(penalty_differential_2024_3d, aes(x = reorder(team, total_penalty_yard_differential), y = total_penalty_yard_differential)) +\n  geom_bar(stat = \"identity\", aes(fill = total_penalty_yard_differential)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", \n             color = \"black\", linewidth = 1.2) +\n  geom_image(aes(image = team_logo_espn), size = 0.05) +\n  scale_fill_gradient2(\n    low = \"#8b0100\",  \n    high = \"#0f3860\",\n    midpoint = 0\n  ) +\n  labs(\n    x = \"\",\n    y = \"Penalty Yard Differential\"\n  ) +\n  blog_theme() + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")\n\n\n\n\n\nChiefs Penalty Yard Differential on 3rd Down (2024)\n\n\n\n\nWhat about fourth downs?\n\n\nCode\n# Calculate Penalty Yard Differential by Team\npenalty_differential_2018_2024_4d &lt;- pbp_data_2018_2024 %&gt;%\n  filter(down == 4) %&gt;%\n  # Only Keep Penalty Plays\n  filter(penalty == 1 & !is.na(penalty_yards)) %&gt;%\n  # Create a Long Form Version with a \"Team\" Column and a Possession/Defense Dummy\n  gather(key = \"side\", value = \"team\", posteam, defteam) %&gt;%\n  mutate(side_dummy = ifelse(side == \"posteam\", \"offense\", \"defense\")) %&gt;%\n  # Modify Penalty Yards So That If the Team Committed the Penalty, It is Negative\n  # For Them and Positive for Their Opponent\n  mutate(\n    penalty_yards_adjusted = case_when(\n      penalty_team == team ~ -penalty_yards, \n      penalty_team != team ~ penalty_yards, \n      TRUE ~ 0\n    )\n  ) %&gt;%\n  # Collapse to the Game Level\n  group_by(game_id, team, season) %&gt;%\n  summarize(\n    penalty_yard_differential = sum(penalty_yards_adjusted, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  # Collapse to the Team Level\n  group_by(team) %&gt;%\n  summarize(\n    total_penalty_yard_differential = sum(penalty_yard_differential, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  left_join(team_logos, by = c(\"team\" = \"team_abbr\"))\n\n# Create a Diverging Bar Chart\nggplot(penalty_differential_2018_2024_4d, aes(x = reorder(team, total_penalty_yard_differential), y = total_penalty_yard_differential)) +\n  geom_bar(stat = \"identity\", aes(fill = total_penalty_yard_differential)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", \n             color = \"black\", linewidth = 1.2) +\n  geom_image(aes(image = team_logo_espn), size = 0.05) +\n  scale_fill_gradient2(\n    low = \"#8b0100\",  \n    high = \"#0f3860\",\n    midpoint = 0\n  ) +\n  labs(\n    x = \"\",\n    y = \"Penalty Yard Differential\"\n  ) +\n  blog_theme() + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")\n\n\n\n\n\nChiefs Penalty Yard Differential on 4th Down (2018-2024)\n\n\n\n\nThe Chiefs are dead-last for Mahomes’s career (lol) and right in the center for 2024, which largely tracks with the PYD on 3rd down.\n\n\nCode\n# Calculate Penalty Yard Differential by Team\npenalty_differential_2024_4d &lt;- pbp_data_2024 %&gt;%\n  filter(down == 4) %&gt;%\n  # Only Keep Penalty Plays\n  filter(penalty == 1 & !is.na(penalty_yards)) %&gt;%\n  # Create a Long Form Version with a \"Team\" Column and a Possession/Defense Dummy\n  gather(key = \"side\", value = \"team\", posteam, defteam) %&gt;%\n  mutate(side_dummy = ifelse(side == \"posteam\", \"offense\", \"defense\")) %&gt;%\n  # Modify Penalty Yards So That If the Team Comitted the Penalty, It is Negative\n  # For Them and Positive for Their Opponent\n  mutate(\n    penalty_yards_adjusted = case_when(\n      penalty_team == team ~ -penalty_yards, \n      penalty_team != team ~ penalty_yards, \n      TRUE ~ 0\n    )\n  ) %&gt;%\n  # Collapse to the Game Level\n  group_by(game_id, team, season) %&gt;%\n  summarize(\n    penalty_yard_differential = sum(penalty_yards_adjusted, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  # Collapse to the Season Level\n  group_by(team) %&gt;%\n  summarize(\n    total_penalty_yard_differential = sum(penalty_yard_differential, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  left_join(team_logos, by = c(\"team\" = \"team_abbr\"))\n\n# Create a Diverging Bar Chart\nggplot(penalty_differential_2024_4d, aes(x = reorder(team, total_penalty_yard_differential), y = total_penalty_yard_differential)) +\n  geom_bar(stat = \"identity\", aes(fill = total_penalty_yard_differential)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", \n             color = \"black\", linewidth = 1.2) +\n  geom_image(aes(image = team_logo_espn), size = 0.05) +\n  scale_fill_gradient2(\n    low = \"#8b0100\",  \n    high = \"#0f3860\",\n    midpoint = 0\n  ) +\n  labs(\n    x = \"\",\n    y = \"Penalty Yard Differential\"\n  ) +\n  blog_theme() + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")\n\n\n\n\n\nChiefs Penalty Yard Differential on 4th Down (2024)\n\n\n\n\nBut, what about overtime? Maybe we can finally find some bit of evidence that the refs favor the Chiefs in a high-stakes situation? Nope, as you can see below, just barely above average. Note that I am not showing OT data for this season because a lot of teams never went into OT this season and, among those who did, it only occurred once. So, the sample sizes are much too small to be useful. However, looking in the aggregate over 7 seasons can yield some insights.\n\n\nCode\n# Calculate Penalty Yard Differential by Team\npenalty_differential_2018_2024_OT &lt;- pbp_data_2018_2024 %&gt;%\n   filter(qtr == 5) %&gt;%\n  # Only Keep Penalty Plays\n  filter(penalty == 1 & !is.na(penalty_yards)) %&gt;%\n  # Create a Long Form Version with a \"Team\" Column and a Possession/Defense Dummy\n  gather(key = \"side\", value = \"team\", posteam, defteam) %&gt;%\n  mutate(side_dummy = ifelse(side == \"posteam\", \"offense\", \"defense\")) %&gt;%\n  # Modify Penalty Yards So That If the Team Committed the Penalty, It is Negative\n  # For Them and Positive for Their Opponent\n  mutate(\n    penalty_yards_adjusted = case_when(\n      penalty_team == team ~ -penalty_yards, \n      penalty_team != team ~ penalty_yards, \n      TRUE ~ 0\n    )\n  ) %&gt;%\n  # Collapse to the Game Level\n  group_by(game_id, team, season) %&gt;%\n  summarize(\n    penalty_yard_differential = sum(penalty_yards_adjusted, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  # Collapse to the Team Level\n  group_by(team) %&gt;%\n  summarize(\n    total_penalty_yard_differential = sum(penalty_yard_differential, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  left_join(team_logos, by = c(\"team\" = \"team_abbr\"))\n\n# Create a Diverging Bar Chart\nggplot(penalty_differential_2018_2024_OT, aes(x = reorder(team, total_penalty_yard_differential), y = total_penalty_yard_differential)) +\n  geom_bar(stat = \"identity\", aes(fill = total_penalty_yard_differential)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", \n             color = \"black\", linewidth = 1.2) +\n  geom_image(aes(image = team_logo_espn), size = 0.05) +\n  scale_fill_gradient2(\n    low = \"#8b0100\",  \n    high = \"#0f3860\",\n    midpoint = 0\n  ) +\n  labs(\n    x = \"\",\n    y = \"Penalty Yard Differential\"\n  ) +\n  blog_theme() + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")\n\n\n\n\n\nPenalty Yard Differential in Over Time (2018-2024)\n\n\n\n\nLastly, to evaluate whether the Chiefs are helped by the refs in tough spots (so far, everything has indicated the opposite), let’s look at penalties that are called when a team is trailing within one score (7 points are less). When the game is close enough, perhaps the refs are more trigger happy to make certain calls or refuse to make certain calls in order to benefit the Chiefs.\n\n\nCode\n# Calculate Penalty Yard Differential by Team\npenalty_differential_2018_2024_trail &lt;- pbp_data_2018_2024 %&gt;%\n  filter(score_differential &gt;= -7 & score_differential &lt;= 0) %&gt;%\n  # Only Keep Penalty Plays\n  filter(penalty == 1 & !is.na(penalty_yards)) %&gt;%\n  # Create a Long Form Version with a \"Team\" Column and a Possession/Defense Dummy\n  gather(key = \"side\", value = \"team\", posteam, defteam) %&gt;%\n  mutate(side_dummy = ifelse(side == \"posteam\", \"offense\", \"defense\")) %&gt;%\n  # Modify Penalty Yards So That If the Team Committed the Penalty, It is Negative\n  # For Them and Positive for Their Opponent\n  mutate(\n    penalty_yards_adjusted = case_when(\n      penalty_team == team ~ -penalty_yards, \n      penalty_team != team ~ penalty_yards, \n      TRUE ~ 0\n    )\n  ) %&gt;%\n  # Collapse to the Game Level\n  group_by(game_id, team, season) %&gt;%\n  summarize(\n    penalty_yard_differential = sum(penalty_yards_adjusted, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  # Collapse to the Team Level\n  group_by(team) %&gt;%\n  summarize(\n    total_penalty_yard_differential = sum(penalty_yard_differential, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  left_join(team_logos, by = c(\"team\" = \"team_abbr\"))\n\n# Create a Diverging Bar Chart\nggplot(penalty_differential_2018_2024_trail, aes(x = reorder(team, total_penalty_yard_differential), y = total_penalty_yard_differential)) +\n  geom_bar(stat = \"identity\", aes(fill = total_penalty_yard_differential)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", \n             color = \"black\", linewidth = 1.2) +\n  geom_image(aes(image = team_logo_espn), size = 0.05) +\n  scale_fill_gradient2(\n    low = \"#8b0100\",  \n    high = \"#0f3860\",\n    midpoint = 0\n  ) +\n  labs(\n    x = \"\",\n    y = \"Penalty Yard Differential\"\n  ) +\n  blog_theme() + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")\n\n\n\n\n\nPenalty Yard Differential When Trailing (2018-2024)\n\n\n\n\nThroughout Mahomes’s career, again, the Chiefs are just barely above average and, for the 2024 season, we actually get a shred of evidence that the Chiefs are positively impacted by ref calls. However, I would stress that, if one wants to make a statement like “the refs favor the Chiefs” based on one statistic, that conviction should also be interesting in the apparent collusion between the NFL and Minnesota Vikings… (I say sarcastically).\n\n\nCode\n# Calculate Penalty Yard Differential by Team\npenalty_differential_2024_trail &lt;- pbp_data_2024 %&gt;%\n  filter(score_differential &gt;= -7 & score_differential &lt;= 0) %&gt;%\n  # Only Keep Penalty Plays\n  filter(penalty == 1 & !is.na(penalty_yards)) %&gt;%\n  # Create a Long Form Version with a \"Team\" Column and a Possession/Defense Dummy\n  gather(key = \"side\", value = \"team\", posteam, defteam) %&gt;%\n  mutate(side_dummy = ifelse(side == \"posteam\", \"offense\", \"defense\")) %&gt;%\n  # Modify Penalty Yards So That If the Team Comitted the Penalty, It is Negative\n  # For Them and Positive for Their Opponent\n  mutate(\n    penalty_yards_adjusted = case_when(\n      penalty_team == team ~ -penalty_yards, \n      penalty_team != team ~ penalty_yards, \n      TRUE ~ 0\n    )\n  ) %&gt;%\n  # Collapse to the Game Level\n  group_by(game_id, team, season) %&gt;%\n  summarize(\n    penalty_yard_differential = sum(penalty_yards_adjusted, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  # Collapse to the Season Level\n  group_by(team) %&gt;%\n  summarize(\n    total_penalty_yard_differential = sum(penalty_yard_differential, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  left_join(team_logos, by = c(\"team\" = \"team_abbr\"))\n\n# Create a Diverging Bar Chart\nggplot(penalty_differential_2024_trail, aes(x = reorder(team, total_penalty_yard_differential), y = total_penalty_yard_differential)) +\n  geom_bar(stat = \"identity\", aes(fill = total_penalty_yard_differential)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", \n             color = \"black\", linewidth = 1.2) +\n  geom_image(aes(image = team_logo_espn), size = 0.05) +\n  scale_fill_gradient2(\n    low = \"#8b0100\",  \n    high = \"#0f3860\",\n    midpoint = 0\n  ) +\n  labs(\n    x = \"\",\n    y = \"Penalty Yard Differential\"\n  ) +\n  blog_theme() + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")\n\n\n\n\n\nPenalty Yard Differential When Trailing (2024)\n\n\n\n\n\n\nClaim 3: Do the Refs Protect Patrick Mahomes?\nAt this point, it should be very obvious that the refs, overall, do not favor the Chiefs and, in addition, they do not favor the Chiefs in tight spots. This is apparent throughout Mahomes’s entire career and in the current 2024 NFL season. But what we’ve looked at so far are team-level stats. Patrick Mahomes is becoming (already is?) the face of the NFL and perhaps the league wants to protect their “golden boy poster child”.\nTo evaluate this, I took a look at roughing the passer and defensive pass interference calls. If the refs wanted to protect Mahomes, surely Mahomes would have a higher rate of roughing the passer calls go in his favor. In addition, if the league wanted to help out the Chiefs, surely pass attempts by Mahomes would result in a higher rate of “fluke” defensive pass interference calls being made. Below, I plot the average number of roughing the passer calls (standardized by quarterback drop-backs) and the average number of defensive pass interference calls (standardized by passing attempts) over the 2018-2024 seasons. In particular, I compare Mahomes to some of his contemporaries who never seem to get heat for calls going their way…\n\n\nCode\nqb_data &lt;- pbp_data_2018_2024 %&gt;%\n  filter(passer %in% c(\"P.Mahomes\", \"J.Allen\", \"L.Jackson\", \"J.Burrow\")) %&gt;%\n  group_by(passer, season) %&gt;%\n  summarize(\n    drop_backs = sum(qb_dropback, na.rm = TRUE),\n    pass_attempts = sum(pass_attempt, na.rm = TRUE),\n    roughing_calls = sum(penalty == 1 & penalty_type == \"Roughing the Passer\", na.rm = TRUE),\n    dpi_calls = sum(penalty == 1 & penalty_type == \"Defensive Pass Interference\", na.rm = TRUE),\n    rc_per_db = (roughing_calls / drop_backs),\n    dpi_per_pa = (dpi_calls / pass_attempts)\n  ) %&gt;%\n  filter(drop_backs &gt;= 100) %&gt;%\n  ungroup()\n\nleague_avg &lt;- pbp_data_2018_2024 %&gt;%\n  group_by(passer, season) %&gt;%\n  summarize(\n    drop_backs = sum(qb_dropback, na.rm = TRUE),\n    pass_attempts = sum(pass_attempt, na.rm = TRUE),\n    roughing_calls = sum(penalty == 1 & penalty_type == \"Roughing the Passer\", na.rm = TRUE),\n    dpi_calls = sum(penalty == 1 & penalty_type == \"Defensive Pass Interference\", na.rm = TRUE),\n    rc_per_db = (roughing_calls / drop_backs),\n    dpi_per_pa = (dpi_calls / pass_attempts)\n  ) %&gt;%\n  filter(drop_backs &gt;= 100) %&gt;%\n  group_by(season) %&gt;%\n  summarise(rc_per_db = mean(rc_per_db),\n            dpi_per_pa = mean(dpi_per_pa)) %&gt;%\n  ungroup() %&gt;%\n  mutate(passer = \"Average\")\n\n# Merge These Data Sources Together\nqb_data &lt;- qb_data %&gt;%\n  full_join(league_avg, by = c(\"passer\", \"season\", \"rc_per_db\", \"dpi_per_pa\")) %&gt;%\n  # Create a Team to Player Logo\n  mutate(team_logo_espn = case_when(\n      passer == \"P.Mahomes\" ~ \"https://a.espncdn.com/i/teamlogos/nfl/500/kc.png\",\n      passer == \"J.Allen\" ~ \"https://a.espncdn.com/i/teamlogos/nfl/500/buf.png\",\n      passer == \"L.Jackson\" ~ \"https://a.espncdn.com/i/teamlogos/nfl/500/bal.png\",\n      passer == \"J.Burrow\" ~ \"https://a.espncdn.com/i/teamlogos/nfl/500/cin.png\",\n      TRUE ~ NA\n  ))\n\nggplot(qb_data, aes(x = season, y = rc_per_db)) +\n  # Plot League Average\n  geom_line(\n    data = league_avg,\n    color = \"grey\",  \n    linetype = \"dashed\",  \n    linewidth = 1\n  ) +\n  geom_point(data = league_avg, size = 3, color = \"grey\") + \n  # Plot Mahomes Line\n  geom_line(\n    data = qb_data %&gt;% filter(passer == \"P.Mahomes\"),\n    color = \"#E31837\",\n    linewidth = 1 \n  ) +\n  # Plot Allen Line\n  geom_line(\n    data = qb_data %&gt;% filter(passer == \"J.Allen\"), \n    color = \"#00338D\",\n    linewidth = 1 \n  ) +\n  # Plot Jackson Line\n  geom_line(\n    data = qb_data %&gt;% filter(passer == \"L.Jackson\"),\n    color = \"#9E7C0C\",\n    linewidth = 1 \n  ) +\n   # Plot Burrow Line\n  geom_line(\n    data = qb_data %&gt;% filter(passer == \"J.Burrow\"),\n    color = \"#fb4f14\",\n    linewidth = 1 \n  ) +\n  geom_image(aes(image = team_logo_espn), size = 0.07) + \n  labs(\n    title = \"\",\n    x = \"\",\n    y = \"Roughing Calls per QB Drop Backs\"\n  ) +\n  scale_x_continuous(breaks = seq(2018, 2024, by = 1)) +\n  blog_theme() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.title = element_text(face = \"bold\")  \n  )\n\n\n\n\n\nRoughing the Passer Calls per Drop Backs\n\n\n\n\nAs you can see, Mahomes goes above and below the average for roughing the passer calls per drop-backs. Certainly, his seasons do not suggest a systematic attempt of the refs to influence games to impact Mahomes. Josh Allen (I would say, among fans, the true “golden boy”) has finished with a lower roughing the passer call rate below Mahomes once over the past seven years! That certainly doesn’t fit the narrative! What about defensive pass intereference (DPI)?\n\n\nCode\nggplot(qb_data, aes(x = season, y = dpi_per_pa)) +\n  # Plot League Average\n  geom_line(\n    data = league_avg,\n    color = \"grey\",  \n    linetype = \"dashed\",  \n    linewidth = 1\n  ) +\n  geom_point(data = league_avg, size = 3, color = \"grey\") + \n  # Plot Mahomes Line\n  geom_line(\n    data = qb_data %&gt;% filter(passer == \"P.Mahomes\"),\n    color = \"#E31837\",\n    linewidth = 1 \n  ) +\n  # Plot Allen Line\n  geom_line(\n    data = qb_data %&gt;% filter(passer == \"J.Allen\"), \n    color = \"#00338D\",\n    linewidth = 1 \n  ) +\n  # Plot Jackson Line\n  geom_line(\n    data = qb_data %&gt;% filter(passer == \"L.Jackson\"),\n    color = \"#9E7C0C\",\n    linewidth = 1 \n  ) +\n   # Plot Burrow Line\n  geom_line(\n    data = qb_data %&gt;% filter(passer == \"J.Burrow\"),\n    color = \"#fb4f14\",\n    linewidth = 1 \n  ) +\n  geom_image(aes(image = team_logo_espn), size = 0.07) + \n  labs(\n    title = \"\",\n    x = \"\",\n    y = \"DPI Calls per Pass Attempt\"\n  ) +\n  scale_x_continuous(breaks = seq(2018, 2024, by = 1)) +\n  blog_theme() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.title = element_text(face = \"bold\")  \n  )\n\n\n\n\n\nDPI Calls per Pass Attempts\n\n\n\n\nWell, this information is even less convenient than the roughing the passer data for the Chiefs skeptics. Since Joe Burrow joined the league in 2020, he has finished with a higher DPI call rate than Mahomes in every season. Josh Allen has finished with a higher rate in 5/7 seasons. And even Lamar Jackson has finished with a higher DPI rate than Mahomes a majority of season (4/7). On Claim #3, it still seems like we fail to find any support for the “refs help out the Chiefs” claim.\n\n\nClaim 4: Does Taylor Swift Impact the Game?\nEver since Taylor Swift and Travis Kelce became an item, NFL fans have claimed that her public association with the league somehow incentives the NFL to violate the integrity of the game. I’m not sure what the logic is here. So long as Swift and Kelce are together, I imagine she will still be attending games and publicly supporting the Chiefs, regardless of how well they perform. But, I still want to evaluate the claim.\nTo do this, I made reference to this article which claims to document each Chiefs game that Taylor Swift has attended. Then, I looked at the PYD for each game that Swift attended and the PYD when Swift was not in attendance. You can see this information in the plot below.\n\n\nCode\n# Load Play-by-Play (PBP) Data\npbp_data_2023_2024 &lt;- load_pbp(seasons = 2023:2024)\n\npenalty_differential_by_game &lt;- pbp_data_2023_2024 %&gt;%\n  # Only Keep Penalty Plays\n  filter(penalty == 1 & !is.na(penalty_yards)) %&gt;%\n  # Create a Long Form Version with a \"Team\" Column and a Possession/Defense Dummy\n  gather(key = \"side\", value = \"team\", posteam, defteam) %&gt;%\n  mutate(side_dummy = ifelse(side == \"posteam\", \"offense\", \"defense\")) %&gt;%\n  # Modify Penalty Yards So That If the Team Comitted the Penalty, It is Negative\n  # For Them and Positive for Their Opponent\n  mutate(\n    penalty_yards_adjusted = case_when(\n      penalty_team == team ~ -penalty_yards, \n      penalty_team != team ~ penalty_yards, \n      TRUE ~ 0\n    )\n  ) %&gt;%\n  # Collapse to the Game Level\n  group_by(game_id, team, season) %&gt;%\n  summarize(\n    penalty_yard_differential = sum(penalty_yards_adjusted, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  left_join(team_logos, by = c(\"team\" = \"team_abbr\")) %&gt;%\n  filter(team == \"KC\") %&gt;%\n  mutate(tay_tay = case_when(\n    game_id == \"2024_21_BUF_KC\" ~ 1,\n    game_id == \"2024_20_HOU_KC\" ~ 1,\n    game_id == \"2024_16_HOU_KC\" ~ 1,\n    game_id == \"2024_13_LV_KC\" ~ 1,\n    game_id == \"2024_10_DEN_KC\" ~ 1,\n    game_id == \"2024_09_TB_KC\" ~ 1,\n    game_id == \"2024_05_NO_KC\" ~ 1,\n    game_id == \"2024_02_CIN_KC\" ~ 1,\n    game_id == \"2024_01_BAL_KC\" ~ 1,\n    game_id == \"2023_22_SF_KC\" ~ 1,\n    game_id == \"2023_21_KC_BAL\" ~ 1,\n    game_id == \"2023_20_KC_BUF\" ~ 1,\n    game_id == \"2023_19_MIA_KC\" ~ 1,\n    game_id == \"2023_17_CIN_KC\" ~ 1,\n    game_id == \"2023_16_LV_KC\" ~ 1,\n    game_id == \"2023_15_KC_NE\" ~ 1,\n    game_id == \"2023_14_BUF_KC\" ~ 1,\n    game_id == \"2023_13_KC_GB\" ~ 1,\n    game_id == \"2023_07_LAC_KC\" ~ 1,\n    game_id == \"2023_06_DEN_KC\" ~ 1,\n    game_id == \"2023_04_KC_NYJ\" ~ 1,\n    game_id == \"2023_03_CHI_KC\" ~ 1,\n    TRUE ~ 0\n  )) %&gt;%\n  mutate(game_id = factor(game_id, levels = game_id),\n         game_number = row_number())\n\navg_differential &lt;- penalty_differential_by_game %&gt;%\n  group_by(tay_tay) %&gt;%\n  summarize(avg_penalty_yard_differential = mean(penalty_yard_differential))\n\nggplot(penalty_differential_by_game, aes(x = game_id, y = penalty_yard_differential)) +\n  geom_hline(yintercept = 0, color = \"black\", linewidth = 1.2) + \n  geom_hline(\n    data = avg_differential,\n    aes(yintercept = avg_penalty_yard_differential, color = factor(tay_tay)),\n    linetype = \"dashed\", linewidth = 1, show.legend = FALSE\n  ) +\n  geom_line(aes(group = 1), color = \"gray70\", linewidth = 1) + \n  geom_point(aes(color = factor(tay_tay)), size = 3.5) + \n  labs(\n    title = \"\",\n    subtitle = \"Horizontal dashed lines indicate averages when Taylor Swift is & is not present.\",\n    x = \"\",\n    y = \"Penalty Yard Differential\",\n    color = \"Was Taylor Swift At This Game?\" \n  ) +\n  scale_color_manual(\n    values = c(\"1\" = \"#8b0100\", \"0\" = \"#0f3860\"), \n    labels = c(\"Yes\", \"No\"), \n    breaks = c(\"1\", \"0\"),\n    guide = guide_legend(title.position = \"top\")\n  ) +\n  scale_y_continuous(breaks = seq(-80, 80, by = 20)) +\n  blog_theme() + \n  theme(\n    plot.subtitle = element_text(face = \"bold\"),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"bottom\",\n    legend.title = element_text(hjust = 0.5),\n    legend.text.align = 0.5\n  )\n\n\n\n\n\nTaylor Swift Attendance and Penalty Yard Differential (2023-2024)\n\n\n\n\nThe colored horizontal lines reflect the average PYD when Taylor Swift is present (red) and when she is not present (blue). This is kind of fun, because you do see a difference and it isn’t entirely negligible. It’s a roughly 40-yard difference which means that, on average throughout the entirety of the 2023-2024 NFL season, the Chiefs got 40 more yards from penalties than their opponents did.\nIs this evidence that, even if the NFL doesn’t rig games for the Chiefs, they are at least more trigger happy to help the Chiefs when Taylor is in attendance? Who knows? I would say that the difference in PYD is certainly interesting, but it’s impossible to know unless you adjust for other factors (I have a whole blog post about this.)\n\n\nWhy You Think the Game is Rigged\nOverall, the overwhelming evidence suggests that the Chiefs are not being helped by the refs. In many cases, they rank very poorly in PYD, in some case, they are right in the middle and, in a minority of cases, they are near the top in PYD. Further, plenty of other teams consistently records a larger PYD than the Chiefs (such as the Bills), but we fail to hear about any conspiracy regarding these teams? Why? Further, why might you think the league helps out the Chiefs when the data suggests otherwise?\nThis likely just has to do with both selection and confirmation bias. If you watch a disproportionately higher number of Chiefs games (which, you likely do because they are on prime-time more often), then you see more questionable ref calls being made. Built up over time, and you might begin to think that the Chiefs sure do get a lot of calls. However, by not watching other teams as often as the Chiefs, you are cognitively discounting how many other questionable calls are made in the favor of other teams.\nThis leads to the second problem, which is confirmation bias. Once you have made the (false) connection that all the calls go the Chiefs’ way, every additional questionable call you see favor the Chiefs only goes to confirm your suspicions. Looking at the game like this makes you susceptible to arriving at false conclusions. You become so focused on the Chiefs, that you fail to see when other teams get away with missed calls or have their opponents called more often or have questionable calls go their way.\nSo, what can you take from this? I would say, stop complaining (because there is no reason to) and enjoy history being made! This team has been a blast to watch and, as a Dallas Cowboys fan, I sincerely hope that the Chiefs do what they do best and send Philly to Cancun."
  },
  {
    "objectID": "blog/2024/nfl-workflow/index.html",
    "href": "blog/2024/nfl-workflow/index.html",
    "title": "Predicting the Outcome of NFL Games in the 2024-2025 Season",
    "section": "",
    "text": "Code\n# Load Libraries\npacman::p_load(\n  \"dplyr\", # Data Manipulation\n  \"nflverse\", # NFL Verse Environment\n  \"gt\", # Nice Tables\n  \"tidyr\", # Reshaping Data\n  \"stringr\", # Working with Strings\n  \"caret\", # Machine Learning\n  \"scales\", # Percent Formatting\n  \"readxl\", # Reading Excel Files\n  \"writexl\", # Writing Excel Files\n  install = FALSE\n)\n\n# Define a Custom Theme - Taken From Andrew Heiss's Blogs\nblog_theme &lt;- function() {\n  theme_bw() +  # Start with theme_bw\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      plot.title = element_text(face = \"bold\"),\n      axis.title = element_text(face = \"bold\"),\n      strip.text = element_text(face = \"bold\"),\n      strip.background = element_rect(fill = \"grey80\", color = NA),\n      legend.title = element_text(face = \"bold\")\n    )\n}\n\n\nI am a massive fan of NFL football. I look forward to the inaugural start of the regular season every September and it feels all too soon when the season ends when the Super Bowl is played in February. As much as I love the game-play, the sports shows spinning their narratives, and the social aspect of NFL Sundays, I have been looking for excuses to get my hands on NFL data and having fun with an additional aspect of the game.\nRecently, in pursuit of this goal, I went to the Playoff Predictors website, where you can go game-by-game and pick who you think will win each game. It’s a fun exercise that I look forward to every year when the NFL schedule is released and it gives me a picture of what I intuitively think the standings might look like at the conclusion of the upcoming season. Once I got these standings, I played around with the {nflreadr} and {gt} packages to present my predicted standings in a more aesthetically pleasing way.\n\n\nCode\n# Create My Vibes Tribble - Adding Extra Spacing\nvibes &lt;- tribble(\n  ~east, ~record_1, ~space_1, ~north, ~record_2, ~space_2, ~south, ~record_3, ~space_3, ~west, ~record_4, ~conf,\n  \"BUF\", \"10-7\", \" \", \"BAL\", \"13-4\", \" \", \"IND\", \"13-4\", \" \", \"KC\", \"12-5\", \"AFC\",\n  \"NYJ\", \"9-8\", \" \", \"CIN\", \"10-7\", \" \", \"HOU\", \"11-6\", \" \", \"LAC\", \"11-6\", \"AFC\",\n  \"MIA\", \"7-10\", \" \", \"PIT\", \"8-9\", \" \",\"TEN\", \"9-8\", \" \", \"DEN\", \"7-10\", \"AFC\",\n  \"NE\", \"3-14\", \" \", \"CLE\", \"8-9\", \" \", \"JAX\", \"9-8\", \" \", \"LV\", \"6-11\", \"AFC\",\n  \"PHI\", \"9-8\", \" \", \"GB\", \"12-5\", \" \", \"ATL\", \"9-8\", \" \", \"LAR\", \"11-6\", \"NFC\",\n  \"WSH\", \"8-9\", \" \", \"DET\", \"11-6\", \" \", \"TB\", \"7-10\", \" \", \"SF\", \"10-7\", \"NFC\",\n  \"DAL\", \"7-10\", \" \", \"CHI\", \"9-8\", \" \", \"CAR\", \"5-12\", \" \", \"ARZ\", \"9-8\", \"NFC\",\n  \"NYG\", \"5-12\", \" \", \"MIN\", \"6-11\", \" \", \"NO\", \"4-13\", \" \", \"SEA\", \"4-13\", \"NFC\"\n)\n\nvibes %&gt;%\n  # Group By Conference\n  gt(groupname_col = \"conf\") %&gt;%\n  # Create Columns Labels\n  cols_label(\n    east = \"\",\n    record_1 = \"East\",\n    space_1 = \"\",\n    north = \"\",\n    record_2 = \"North\",\n    space_2 = \"\",\n    south = \"\",\n    record_3 = \"South\",\n    space_3 = \"\",\n    west = \"\",\n    record_4 = \"West\"\n  ) %&gt;%\n  # Align Column Title Text\n  tab_style(style = cell_text(align = \"center\"), locations = cells_column_labels()) %&gt;%\n  # Align Body Text\n  tab_style(style = cell_text(align = \"center\"), locations = cells_body()) %&gt;%\n  # Distinguish Division Rows\n  tab_style(\n    style = list(\n      cell_fill(color = \"#bcc0be\")),\n    locations = cells_body(rows = which(vibes$east %in% c(\"AFC\", \"NFC\")))) %&gt;%\n  # Add Team Logos\n  nflplotR::gt_nfl_logos(columns = c(\"east\", \"north\", \"south\", \"west\"))\n\n\n\n\n\n\n\n\n\nEast\n\n\nNorth\n\n\nSouth\n\n\nWest\n\n\n\n\nAFC\n\n\n\n10-7\n\n\n13-4\n\n\n13-4\n\n\n12-5\n\n\n\n9-8\n\n\n10-7\n\n\n11-6\n\n\n11-6\n\n\n\n7-10\n\n\n8-9\n\n\n9-8\n\n\n7-10\n\n\n\n3-14\n\n\n8-9\n\n\n9-8\n\n\n6-11\n\n\nNFC\n\n\n\n9-8\n\n\n12-5\n\n\n9-8\n\n\n11-6\n\n\n\n8-9\n\n\n11-6\n\n\n7-10\n\n\n10-7\n\n\n\n7-10\n\n\n9-8\n\n\n5-12\n\n\n9-8\n\n\n\n5-12\n\n\n6-11\n\n\n4-13\n\n\n4-13\n\n\n\n\n\n\n\nI like to refer to these as my “vibes-based” predictions because that’s really all they are. However, as a trained social scientist, I am well aware that “vibes” are not wholly informative, well-defined, nor do they contain a great deal of explanatory or predictive power. So, I thought, why not get my hands on more NFL data and try to work up a machine learning based approach? And that is what this blog is for.\n\nData/Feature Collection\nPrior to any fancy modeling, I need to collect some data to predict who wins each game. I want to start off with a major caveat here. I am doing this for fun and educational purposes. Undoubtedly, the predictors I have selected are not reflective of the most advanced analytics nor are they comprehensive. I chose the “lowest hanging fruit” for ease of access. This is probably going to hurt the predictive power of the models (models predict better with more predictive data), but again, humor me!\nOverall, I am using the following variables as predictors: whether a team is playing at home, QBR (quarterback rating), passing EPA (expected points added), rushing EPA, receiving EPA, forced fumbles, sacks, interceptions, and passes broken up. Because each prediction is at the game-level, I am using a differenced variable for computational ease (i.e., rather than include the home team’s passing EPA and the away team’s passing EPA in the same model, I just create a difference between the two and use this difference as a predictor for each team). Regardless of which method is used, the predictive performance remained the same after testing.\nThis selection leaves a lot to be desired. What about more advanced metrics like ELO? What about schematic data (like what type of offense the home team runs v. what type of defense the away team runs, etc.)? What about circumstantial data like whether a key player is out? These are all great things to add that will need to be included in the future! If you’re curious about the data collection syntax, check out the code fold below!\n\n\nCode\n# Load and Clean the QBR Data\nqbr &lt;- load_espn_qbr(\n  # Select the 2006-2023 Seasons as Training Data\n  seasons = 2006:2023,\n  # Aggregate at the Week-Level\n  summary_type = c(\"week\")) %&gt;%\n  # Exclude Playoff Games\n  filter(season_type == \"Regular\") %&gt;%\n  # Select Relevant Columns\n  select(c(team_abb, season, game_week, qbr_total, pts_added)) %&gt;%\n  # Create Cumulative Averages\n  group_by(season, team_abb) %&gt;%\n  mutate(\n    moving_qbr_mean = cumsum(qbr_total) / game_week,\n    moving_pts_added = cumsum(pts_added / game_week),\n    # Rename Washington for Merging\n    team_abb = ifelse(team_abb == \"WSH\", \"WAS\", team_abb))\n\n# Load and Clean Offensive Stats Data\noffensive &lt;- load_player_stats(\n  # Select the 2006-2023 Seasons as Training Data\n  seasons = 2006:2023,\n  # Filter to Offense\n  stat_type = \"offense\") %&gt;%\n  # Exclude the Playoffs\n  filter(season_type == \"REG\") %&gt;%\n  # Create Team-Level Stats\n  group_by(season, recent_team, week) %&gt;%\n  summarise(\n    passing_epa = sum(passing_epa, na.rm = TRUE),\n    rushing_epa = sum(rushing_epa, na.rm = TRUE),\n    receiving_epa = sum(receiving_epa, na.rm = TRUE)\n  ) %&gt;%\n  ungroup() %&gt;%\n  # Create Cumulative Averages\n  group_by(season, recent_team) %&gt;%\n  mutate(\n    moving_passing_epa = cumsum(passing_epa) / week,\n    moving_rushing_epa = cumsum(rushing_epa) / week,\n    moving_receiving_epa = cumsum(receiving_epa) / week) %&gt;%\n  # Keep Relevant Columns\n  select(season, recent_team, week, passing_epa, rushing_epa, receiving_epa, \n         moving_passing_epa, moving_rushing_epa, moving_receiving_epa) %&gt;%\n  # Convert Team Abbreviations to a More Standard Form for Merging\n  mutate(recent_team = ifelse(recent_team == \"LA\", \"LAR\", recent_team))\n\n# Load and Clean Defensive Stats Data\ndefensive &lt;- load_player_stats(\n  # Select the 2006-2023 Seasons as Training Data\n  seasons = 2006:2023,\n  # Filter to Defense\n  stat_type = \"defense\") %&gt;%\n  # Exclude Playoff Games\n  filter(season_type == \"REG\") %&gt;%\n  # Create Team-Level Stats\n  group_by(season, team, week) %&gt;%\n  summarise(\n    tackles = sum(def_tackles, na.rm = TRUE),\n    forced_fumbles = sum(def_fumbles_forced, na.rm = TRUE),\n    sacks = sum(def_sacks, na.rm = TRUE),\n    ints = sum(def_interceptions, na.rm = TRUE),\n    pass_broken = sum( def_pass_defended, na.rm = TRUE)\n  ) %&gt;%\n  ungroup() %&gt;%\n  # Create Cumulative Averages\n  group_by(season, team) %&gt;%\n  mutate(\n    moving_tackles = cumsum(tackles) / week,\n    moving_forced_fumbles = cumsum(forced_fumbles) / week,\n    moving_sacks = cumsum(sacks) / week,\n    moving_ints = cumsum(ints) / week,\n    moving_pass_broken = cumsum(pass_broken) / week) %&gt;%\n  # Keep Relevant Columns\n  select(season, team, week, tackles, forced_fumbles, sacks, ints, pass_broken, moving_tackles, \n         moving_forced_fumbles, moving_sacks, moving_ints, moving_pass_broken) %&gt;%\n  # Convert Team Abbreviations to a More Standard Form for Merging\n  mutate(team = ifelse(team == \"LA\", \"LAR\", team))\n\n# Load and Clean Schedules Data\nseasons &lt;- load_schedules(seasons = 2006:2023)\n\n# Convert the Data From Dyadic to Monadic\nseasons &lt;- clean_homeaway(seasons) %&gt;%\n  # Exclude Playoff Games\n  filter(game_type == \"REG\") %&gt;%\n  # Create a Home Team Variable\n  mutate(home = ifelse(location == \"home\", 1, 0),\n         # Create a Win Variable\n         win = ifelse(team_score &gt; opponent_score, 1, 0)) %&gt;%\n  # Keep Relevant Columns\n  select(game_id, season, week, team, opponent, home, win) %&gt;%\n   # Convert Team Abbreviations to a More Standard Form for Merging\n  mutate(team = ifelse(team == \"LA\", \"LAR\", team),\n         opponent = ifelse(opponent == \"LA\", \"LAR\", opponent))\n\n# Merge This Data\nmerged &lt;- inner_join(seasons, qbr, by = c(\"season\", \"team\" = \"team_abb\", \"week\" = \"game_week\")) %&gt;%\n  inner_join(offensive, by = c(\"season\", \"team\" = \"recent_team\", \"week\")) %&gt;%\n  inner_join(defensive, by = c(\"season\", \"team\", \"week\"))\n\nmerged &lt;- merged %&gt;%\n  group_by(game_id) %&gt;%\n  # Create Opponent Columns\n  # This Work Because Each Team Opponent Is In a Paired Set of Rows\n  # The Opponent Is Always the Second Observation\n  # Basically, This Just Reverses Cumulative Stats For Each Team Under a Different Name\n  mutate(opp_qbr = lead(moving_qbr_mean),\n         opp_qbr = ifelse(is.na(opp_qbr), lag(moving_qbr_mean), opp_qbr),\n         opp_pass_epa = lead(moving_passing_epa),\n         opp_pass_epa = ifelse(is.na(opp_pass_epa), \n                               lag(moving_passing_epa), opp_pass_epa),\n         opp_rushing_epa = lead(moving_rushing_epa),\n         opp_rushing_epa = ifelse(is.na(opp_rushing_epa), \n                                  lag(moving_rushing_epa), opp_rushing_epa),\n         opp_receiving_epa = lead(moving_receiving_epa),\n         opp_receiving_epa = ifelse(is.na(opp_receiving_epa), \n                                    lag(moving_receiving_epa), opp_receiving_epa),\n         opp_tackles = lead(moving_tackles),\n         opp_tackles = ifelse(is.na(opp_tackles), \n                              lag(moving_tackles), opp_tackles),\n         opp_forced_fumbles = lead(moving_forced_fumbles),\n         opp_forced_fumbles = ifelse(is.na(opp_forced_fumbles), \n                                     lag(moving_forced_fumbles), opp_forced_fumbles),\n         opp_sacks = lead(moving_sacks),\n         opp_sacks = ifelse(is.na(opp_sacks), \n                            lag(moving_sacks), opp_sacks),\n         opp_ints = lead(moving_ints),\n         opp_ints = ifelse(is.na(opp_ints), \n                           lag(moving_ints), opp_ints),\n         opp_pass_broken = lead(moving_pass_broken),\n         opp_pass_broken = ifelse(is.na(opp_pass_broken), \n                                  lag(moving_pass_broken), opp_pass_broken)\n         ) %&gt;%\n  # Create Differenced Columns\n  mutate(\n    qbr_diff = moving_qbr_mean - opp_qbr,\n    pass_epa_diff = moving_passing_epa - opp_pass_epa,\n    rushing_epa_diff = moving_rushing_epa - opp_rushing_epa,\n    receiving_epa_diff = moving_receiving_epa - opp_receiving_epa,\n    tackles_diff = moving_tackles - opp_tackles,\n    forced_fumbles_diff = moving_forced_fumbles - opp_forced_fumbles,\n    sacks_diff = moving_sacks - opp_sacks,\n    ints_diff = moving_ints - opp_ints,\n    pass_broken_diff = moving_pass_broken - opp_pass_broken\n  ) %&gt;%\n  # Make the Outcome Column Suitable for Classification\n  mutate(win = factor(win, levels = c(0, 1), labels = c(\"Lose\", \"Win\"))) %&gt;%\n  # Drop NAs Because They Will Create Problems\n  drop_na()\n\n\n\n\nMachine Learning Algorithms Limitations\nOkay, now onto the actual machine learning algorithms that will be used. Again, nothing super fancy here. In the interest of keeping things simple at first, I chose to just explore how predictive accuracy fluctuates between four popular ML algorithms (logistic regression… which makes me cringe to refer to it as “ML”, random forest, support vector machine (SVM), and XGBoost). For those curious, I did engage in hyper-parameter tuning, but, no amount of tuning really improved the model results that much, and I felt that, in the interest of simplicity and computational time, it would be best to just include four basic ML algorithms for now.\n\n# For Reproducibility\nset.seed(1234)\n\n# Establish a Cross-Validation Method\ncv_method &lt;- trainControl(method = \"cv\",\n                          number = 10,\n                          classProbs = TRUE,\n                          summaryFunction = twoClassSummary)\n\n# Fit Models\n# Logistic Regression\nlog_fit &lt;- train(win ~ home + qbr_diff + pass_epa_diff + rushing_epa_diff + receiving_epa_diff +\n                       forced_fumbles_diff + sacks_diff + ints_diff + pass_broken_diff, \n                  data = merged,\n                  method = \"glm\",\n                  family = \"binomial\",\n                  trControl = cv_method,\n                  metric = \"ROC\")\n\n# Save Model Results So I Don't Have to Re-Train Every Time\nsaveRDS(log_fit, \"data-and-analysis/log_fit_model.rds\")\n\n# Random Forest\nrf_fit &lt;- train(win ~ home + qbr_diff + pass_epa_diff + rushing_epa_diff + receiving_epa_diff +\n                       forced_fumbles_diff + sacks_diff + ints_diff + pass_broken_diff, \n                  data = merged,\n                  method = \"rf\",\n                  trControl = cv_method,\n                  metric = \"ROC\")\n\nsaveRDS(rf_fit, \"data-and-analysis/rf_fit_model.rds\")\n\n# Support Vector Machine\nsv_fit &lt;- train(win ~ home + qbr_diff + pass_epa_diff + rushing_epa_diff + receiving_epa_diff +\n                       forced_fumbles_diff + sacks_diff + ints_diff + pass_broken_diff, \n                data = merged,\n                method = \"svmLinear\",\n                trControl = cv_method,\n                metric = \"ROC\")\n\nsaveRDS(sv_fit, \"data-and-analysis/sv_fit_model.rds\")\n\n# XGBoost\nxgb_fit &lt;- train(win ~ qbr_diff + pass_epa_diff + rushing_epa_diff + receiving_epa_diff +\n                        forced_fumbles_diff + sacks_diff + ints_diff + pass_broken_diff, \n                 data = merged,\n                 method = \"xgbTree\",\n                 trControl = cv_method,\n                 metric = \"ROC\")\n\nsaveRDS(xgb_fit, \"data-and-analysis/xgb_fit_model.rds\")\n\n# Store the Predictive Accuracy Results in a Table\nresults &lt;- tibble(\n  Model = c(\"Logistic Regression\", \"Random Forest\", \"SVM\", \"XGBoost\"),\n  # Store ROC Metrics\n  ROC = c(\n    # which.max() Doesn't Do Anything Here, But It Would If I Had Tons of Different\n    # Models for Each Model Type. It Would Select the Model with the Highest Predictive\n    # Power. Not Helpful Here Since I Am Only Running One Model of Each Type, But It's\n    # A Useful Reference That I Want to Keep for the Future\n    log_fit$results[which.max(log_fit$results$ROC), \"ROC\"], \n    rf_fit$results[which.max(rf_fit$results$ROC), \"ROC\"], \n    sv_fit$results[which.max(sv_fit$results$ROC), \"ROC\"], \n    xgb_fit$results[which.max(xgb_fit$results$ROC), \"ROC\"]\n  ),\n  # Store Accurate Predictions Percentage\n  Accuracy = c(\n    (log_fit$results$Spec[which.max(log_fit$results$ROC)] + \n       log_fit$results$Sens[which.max(log_fit$results$ROC)]) / 2,\n    (rf_fit$results$Spec[which.max(rf_fit$results$ROC)] + \n       rf_fit$results$Sens[which.max(rf_fit$results$ROC)]) / 2,\n    (sv_fit$results$Spec[which.max(sv_fit$results$ROC)] + \n       sv_fit$results$Sens[which.max(sv_fit$results$ROC)]) / 2,\n    (xgb_fit$results$Spec[which.max(xgb_fit$results$ROC)] + \n       xgb_fit$results$Sens[which.max(xgb_fit$results$ROC)]) / 2\n  )\n)\n\n\n\nModel Evaluation\nSo, how did these models fair? Eh… not great, as you can see below\n\nresults    \n\n# A tibble: 4 × 3\n  Model                 ROC Accuracy\n  &lt;chr&gt;               &lt;dbl&gt;    &lt;dbl&gt;\n1 Logistic Regression 0.798    0.718\n2 Random Forest       0.785    0.715\n3 SVM                 0.798    0.717\n4 XGBoost             0.792    0.720\n\n\n70-ish% isn’t terrible. It’s better than a coin flip. But really, how impressive is that? Just off of vibes, anyone who sort of knows the NFL will probably get 70% of game predictions right. Honestly, you might even do better if you just follow Vegas and predict the winner based on who is the betting favorite to win. That’s not very satisfying is it? A truly impressive ML algorithm should be able to predict not only when a favorite wins but also when a favorite does not win. These very crude models don’t appear to have that predictive complexity. Why is that the case? I can think of three reasons.\nFirst, as already stated, better predictors would go a long way. The good news is that this is probably the easiest fix. I just need to put the time in to research and collect the data.\nSecond, there may have been more complex hyper-parameter tuning I could have engaged with. Given that I come from a causal inference background, machine learning is not my specialty, and I do not have a wealth of information lodged in my head about all the tuning options for each ML algorithm. However, I’m sure that predictive gains could be there with some hyper-parameter tuning.\nLastly, I think that a different modeling approach could go a long way. And, to demonstrate my reasoning, let’s look at how my trained models are predicting the outcomes of the upcoming Week 2 games.\n\n\nCode\n# To Do This, I Need to Load In 2024 \"Test\" Data That the Model Was Not Trained On\n# This Is Just a Repeat of the Prior Data Cleaning Process for the Training Data\n# So I Don't Annotate Code Here\nqbr_2024 &lt;- load_espn_qbr(\n  seasons = 2024,\n  summary_type = c(\"week\")) %&gt;%\n  filter(season_type == \"Regular\") %&gt;%\n  select(c(team_abb, season, game_week, qbr_total, pts_added)) %&gt;%\n  group_by(season, team_abb) %&gt;%\n  mutate(\n    moving_qbr_mean = cumsum(qbr_total) / game_week,\n    moving_pts_added = cumsum(pts_added / game_week),\n    # Rename Washington for Merging\n    team_abb = ifelse(team_abb == \"WSH\", \"WAS\", team_abb)) %&gt;%\n  # Keep Last Week's Data\n  filter(game_week == 1) %&gt;%\n  # Convert Lagged Game Week to Current Since We're Using Last Week's Predictors\n  mutate(game_week = 2)\n\noffensive_2024 &lt;- load_player_stats(\n  seasons = 2024,\n  stat_type = \"offense\") %&gt;%\n  filter(season_type == \"REG\") %&gt;%\n  group_by(season, recent_team, week) %&gt;%\n  summarise(\n    passing_epa = sum(passing_epa, na.rm = TRUE),\n    rushing_epa = sum(rushing_epa, na.rm = TRUE),\n    receiving_epa = sum(receiving_epa, na.rm = TRUE)\n  ) %&gt;%\n  ungroup() %&gt;%\n  group_by(season, recent_team) %&gt;%\n  mutate(\n    moving_passing_epa = cumsum(passing_epa) / week,\n    moving_rushing_epa = cumsum(rushing_epa) / week,\n    moving_receiving_epa = cumsum(receiving_epa) / week) %&gt;%\n  select(season, recent_team, week, passing_epa, rushing_epa, receiving_epa, moving_passing_epa,\n         moving_rushing_epa, moving_receiving_epa) %&gt;%\n  filter(week == 1) %&gt;%\n  mutate(week = 2) %&gt;%\n  mutate(recent_team = ifelse(recent_team == \"LA\", \"LAR\", recent_team))\n\ndefensive_2024 &lt;- load_player_stats(\n  seasons = 2024,\n  stat_type = \"defense\") %&gt;%\n  filter(season_type == \"REG\") %&gt;%\n  group_by(season, team, week) %&gt;%\n  summarise(\n    tackles = sum(def_tackles, na.rm = TRUE),\n    forced_fumbles = sum(def_fumbles_forced, na.rm = TRUE),\n    sacks = sum(def_sacks, na.rm = TRUE),\n    ints = sum(def_interceptions, na.rm = TRUE),\n    pass_broken = sum( def_pass_defended, na.rm = TRUE)\n  ) %&gt;%\n  ungroup() %&gt;%\n  group_by(season, team) %&gt;%\n  mutate(\n    moving_tackles = cumsum(tackles) / week,\n    moving_forced_fumbles = cumsum(forced_fumbles) / week,\n    moving_sacks = cumsum(sacks) / week,\n    moving_ints = cumsum(ints) / week,\n    moving_pass_broken = cumsum(pass_broken) / week) %&gt;%\n  select(season, team, week, tackles, forced_fumbles, sacks, ints, pass_broken, moving_tackles, \n         moving_forced_fumbles, moving_sacks, moving_ints, moving_pass_broken) %&gt;%\n  filter(week == 1) %&gt;%\n  mutate(week = 2) %&gt;%\n  mutate(team = ifelse(team == \"LA\", \"LAR\", team))\n\nseason_2024 &lt;- load_schedules(seasons = 2024)\n\n# Convert the Data From Dyadic to Monadic\nseason_2024 &lt;- clean_homeaway(season_2024) %&gt;%\n  filter(game_type == \"REG\") %&gt;%\n  mutate(home = ifelse(location == \"home\", 1, 0),\n         win = ifelse(team_score &gt; opponent_score, 1, 0)) %&gt;%\n  select(game_id, season, week, team, opponent, home, win) %&gt;%\n  filter(week == 2) %&gt;%\n  mutate(team = ifelse(team == \"LA\", \"LAR\", team),\n         opponent = ifelse(opponent == \"LA\", \"LAR\", opponent))\n\nmerged_2024 &lt;- inner_join(season_2024, qbr_2024, \n                          by = c(\"team\" = \"team_abb\", \"week\" = \"game_week\", \"season\")) %&gt;%\n  inner_join(offensive_2024, by = c(\"team\" = \"recent_team\", \"week\", \"season\")) %&gt;%\n  inner_join(defensive_2024, by = c(\"team\", \"week\", \"season\")) %&gt;%\n  group_by(game_id) %&gt;%\n  mutate(opp_qbr = lead(moving_qbr_mean),\n         opp_qbr = ifelse(is.na(opp_qbr), \n                          lag(moving_qbr_mean), opp_qbr),\n         opp_pass_epa = lead(moving_passing_epa),\n         opp_pass_epa = ifelse(is.na(opp_pass_epa), \n                               lag(moving_passing_epa), opp_pass_epa),\n         opp_rushing_epa = lead(moving_rushing_epa),\n         opp_rushing_epa = ifelse(is.na(opp_rushing_epa), \n                                  lag(moving_rushing_epa), opp_rushing_epa),\n         opp_receiving_epa = lead(moving_receiving_epa),\n         opp_receiving_epa = ifelse(is.na(opp_receiving_epa), \n                                    lag(moving_receiving_epa), opp_receiving_epa),\n         opp_tackles = lead(moving_tackles),\n         opp_tackles = ifelse(is.na(opp_tackles), \n                              lag(moving_tackles), opp_tackles),\n         opp_forced_fumbles = lead(moving_forced_fumbles),\n         opp_forced_fumbles = ifelse(is.na(opp_forced_fumbles), \n                                     lag(moving_forced_fumbles), opp_forced_fumbles),\n         opp_sacks = lead(moving_sacks),\n         opp_sacks = ifelse(is.na(opp_sacks), \n                            lag(moving_sacks), opp_sacks),\n         opp_ints = lead(moving_ints),\n         opp_ints = ifelse(is.na(opp_ints), \n                           lag(moving_ints), opp_ints),\n         opp_pass_broken = lead(moving_pass_broken),\n         opp_pass_broken = ifelse(is.na(opp_pass_broken), \n                                  lag(moving_pass_broken), opp_pass_broken)\n         ) %&gt;%\n  mutate(\n    qbr_diff = moving_qbr_mean - opp_qbr,\n    pass_epa_diff = moving_passing_epa - opp_pass_epa,\n    rushing_epa_diff = moving_rushing_epa - opp_rushing_epa,\n    receiving_epa_diff = moving_receiving_epa - opp_receiving_epa,\n    tackles_diff = moving_tackles - opp_tackles,\n    forced_fumbles_diff = moving_forced_fumbles - opp_forced_fumbles,\n    sacks_diff = moving_sacks - opp_sacks,\n    ints_diff = moving_ints - opp_ints,\n    pass_broken_diff = moving_pass_broken - opp_pass_broken\n  ) %&gt;%\n  mutate(win = factor(win, levels = c(0, 1), labels = c(\"Lose\", \"Win\"))) %&gt;%\n  # Remove Games with Missing Feature Data\n  filter(!is.na(opp_qbr))\n\n\n\nlog_preds &lt;- predict(log_fit, merged_2024, type = \"prob\")[,2]\nrf_preds &lt;- predict(rf_fit, merged_2024, type = \"prob\")[,2]\nsv_preds &lt;- predict(sv_fit, merged_2024, type = \"prob\")[,2]\nxgb_preds &lt;- predict(xgb_fit, merged_2024, type = \"prob\")[,2]\n\n# Combine Predictions Into a Data Frame\npredictions &lt;- tibble(\n  Team = merged_2024$team,\n  Game_ID = merged_2024$game_id,\n  Logistic_Regression = paste0(round(log_preds * 100, 2), \"%\"),\n  Random_Forest = paste0(round(rf_preds * 100, 2), \"%\"),\n  SVM = paste0(round(sv_preds * 100, 2), \"%\"),\n  XGBoost = paste0(round(xgb_preds * 100, 2), \"%\")\n)\n\n# Load in NFL Logo Data to Make Cool Tables with {gt}\nteam_logos &lt;- nflfastR::teams_colors_logos %&gt;% \n  select(team_abbr, team_logo_espn)\n\nweek2_preds &lt;- predictions %&gt;%\n  left_join(team_logos, by = c(\"Team\" = \"team_abbr\")) %&gt;%\n  select(Team, team_logo_espn, Game_ID, Logistic_Regression, Random_Forest, SVM, XGBoost)\n\n# I Want to Create a Table That Has Teams Who Are Playing Each Other In the Same Row\n# To Do This, I'll Need to Reshape the Data\nreshaped_week2 &lt;- week2_preds %&gt;%\n    group_by(Game_ID) %&gt;%\n    summarise(\n        Team_1 = first(Team),\n        Team_1_Logo = first(team_logo_espn),\n        Team_1_Logistic_Regression = first(Logistic_Regression),\n        Team_1_Random_Forest = first(Random_Forest),\n        Team_1_SVM = first(SVM),\n        Team_1_XGBoost = first(XGBoost),\n        Team_2 = last(Team),\n        Team_2_Logo = last(team_logo_espn),\n        Team_2_Logistic_Regression = last(Logistic_Regression),\n        Team_2_Random_Forest = last(Random_Forest),\n        Team_2_SVM = last(SVM),\n        Team_2_XGBoost = last(XGBoost)\n    )\n\n# Now, I Can Use {gt} To Make a Nice Table\nreshaped_week2 %&gt;%\n  # Start a {gt} Table\n    gt() %&gt;%\n  # Modify Logo Settings\n    text_transform(\n        locations = cells_body(vars(Team_1_Logo, Team_2_Logo)),\n        fn = function(x) {\n            web_image(url = x, height = 40)  # Adjust the height as needed\n        }\n    ) %&gt;%\n  # Remove Columns From the Table\n    cols_hide(\n        columns = c(Game_ID, Team_1, Team_2)\n    ) %&gt;%\n  # Label the Columns\n    cols_label(\n        Team_1_Logo = \"Home\",\n        Team_1_Logistic_Regression = \"Logit\",\n        Team_1_Random_Forest = \"Random Forest\",\n        Team_1_SVM = \"SVM\",\n        Team_1_XGBoost = \"XGBoost\",\n        Team_2_Logo = \"Away\",\n        Team_2_Logistic_Regression = \"Logit\",\n        Team_2_Random_Forest = \"Random Forest\",\n        Team_2_SVM = \"SVM\",\n        Team_2_XGBoost = \"XGBoost\"\n    ) %&gt;%\n  # Create a Title for the Table\n    tab_header(\n        title = \"Predicted Win Probability by Game and Model\"\n    ) %&gt;%\n  # Column Formatting\n    tab_style(\n        style = list(\n            cell_text(weight = \"bold\")\n        ),\n        locations = cells_column_labels(everything())  \n    ) %&gt;%\n    cols_align(\n        align = \"center\",\n        columns = everything()\n    ) %&gt;%\n  # Adjust Column Widths \n    cols_width(\n        Team_1_Logo ~ px(100),  # Adjust as needed\n        Team_1_Logistic_Regression ~ px(85),  # Adjust as needed\n        Team_1_Random_Forest ~ px(85),  # Adjust as needed\n        Team_1_SVM ~ px(85),  # Adjust as needed\n        Team_1_XGBoost ~ px(85),  # Adjust as needed\n        Team_2_Logo ~ px(100),  # Adjust as needed\n        Team_2_Logistic_Regression ~ px(85),  # Adjust as needed\n        Team_2_Random_Forest ~ px(85),  # Adjust as needed\n        Team_2_SVM ~ px(85),  # Adjust as needed\n        Team_2_XGBoost ~ px(85)  # Adjust as needed\n    )\n\nWarning: Since gt v0.3.0, `columns = vars(...)` has been deprecated.\n• Please use `columns = c(...)` instead.\n\n\n\n\n\n\n\n\nPredicted Win Probability by Game and Model\n\n\nHome\nLogit\nRandom Forest\nSVM\nXGBoost\nAway\nLogit\nRandom Forest\nSVM\nXGBoost\n\n\n\n\n\n98.21%\n91.6%\n97.95%\n93.03%\n\n1.66%\n6%\n1.92%\n4.92%\n\n\n\n15.42%\n12.8%\n15.35%\n7.54%\n\n83.61%\n85.2%\n83.75%\n85.47%\n\n\n\n93.46%\n91.8%\n93.25%\n83.63%\n\n6.11%\n8%\n6.34%\n14.28%\n\n\n\n89.19%\n80.2%\n88.54%\n83.76%\n\n10.13%\n19.6%\n10.79%\n11.25%\n\n\n\n90.95%\n89.8%\n91.02%\n79.28%\n\n8.47%\n11.4%\n8.44%\n14.85%\n\n\n\n79.87%\n71%\n76.22%\n64.98%\n\n18.99%\n31.2%\n22.57%\n30.99%\n\n\n\n2.47%\n4.2%\n2.68%\n2.32%\n\n97.35%\n94.4%\n97.14%\n97.02%\n\n\n\n45.8%\n46%\n47.71%\n31.15%\n\n52.4%\n45.4%\n50.59%\n62.23%\n\n\n\n96.01%\n69.8%\n96.07%\n85.33%\n\n3.72%\n26.4%\n3.68%\n13.92%\n\n\n\n21.73%\n19.6%\n20.61%\n14.24%\n\n77.01%\n83%\n78.26%\n81.43%\n\n\n\n92.83%\n87.6%\n92.69%\n78.2%\n\n6.7%\n14%\n6.87%\n17.81%\n\n\n\n10.32%\n24.6%\n10.73%\n13.51%\n\n88.98%\n73.2%\n88.6%\n82.98%\n\n\n\n8.04%\n9.6%\n8.97%\n6.5%\n\n91.41%\n86.4%\n90.46%\n91.34%\n\n\n\n38.9%\n49.2%\n40.41%\n51.87%\n\n59.36%\n48.6%\n57.94%\n54.9%\n\n\n\n92.36%\n88.2%\n91.65%\n81.26%\n\n7.15%\n15.4%\n7.85%\n17.28%\n\n\n\n17.74%\n16.4%\n17.38%\n20.76%\n\n81.18%\n83.6%\n81.63%\n77.53%\n\n\n\n\n\n\n\nAs you can see, there are some wacky predictions for week 2 game outcomes. The Saints are massive favorites over the Cowboys? The Vikings are massive favorites to the 49ers? What?! Well, the answer is not very surprising. In predicting week 2 games, we use all data from prior weeks in the season. In week 2, this means we only have one week of data to draw from. That means that, if a team does exceptionally well in week 1, this great performance is going to impact predictions for week 2. Both the Saints and Vikings had great offensive and defensive performances in week 1, which explains why this model is so bullish on these teams. It stands to reason that such model predictions would probably not show up later on in the season.\nThis gets to my third point on my model performance. When a model is solely impacted by the data, and the available data is not incredibly informative, we are going to get predictions that are pretty counter-intuitive. Basically, please do not put any money down on the Saints or Vikings outright winning this week! I think something to explore in the future would be Bayesian methods to incorporate prior information (i.e. the Cowboys perform well in the early regular season, the 49ers are really good, etc.) that can stabilize the existing limited data with prior knowledge. As the causal inference folks are quick to say… data are dumb, especially when such limited data. Especially early in the season, Bayesian methods may prove really helpful in preventing predictions that are generated from an outlier or two.\nOut of curiosity, I wanted to check how well the model was able to predict the outcome of games by each week in the season. The expectation would be that the model becomes more accurate as the season goes on (we get more information). Below is a plot of the average percent of games whose outcomes are correctly predicted each week from the 2006-2023 seasons.\n\n\nCode\nmerged$log_preds &lt;- predict(log_fit, merged, type = \"prob\")[,2]\nmerged$rf_preds &lt;- predict(rf_fit, merged, type = \"prob\")[,2]\nmerged$rf_preds2 &lt;- ifelse(merged$rf_preds &gt;= 0.5, 1, 0)\nmerged$sv_preds &lt;- predict(sv_fit, merged, type = \"prob\")[,2]\nmerged$xgb_preds &lt;- predict(xgb_fit, merged, type = \"prob\")[,2]\n\nmerged %&gt;%\n  filter(week != 1) %&gt;%\n  mutate(log_class = ifelse(log_preds &gt;= 0.5, \"Win\", \"Lose\"),\n         rf_class = ifelse(rf_preds &gt;= 0.5, \"Win\", \"Lose\"),\n         sv_class = ifelse(sv_preds &gt;= 0.5, \"Win\", \"Lose\"),\n         xgb_class = ifelse(xgb_preds &gt;= 0.5, \"Win\", \"Lose\")) %&gt;%\n  mutate(log_right = ifelse(win == log_class, 1, 0),\n         rf_right = ifelse(win == rf_class, 1, 0),\n         sv_right = ifelse(win == sv_class, 1, 0),\n         xgb_right = ifelse(win == xgb_class, 1, 0)) %&gt;%\n  # By Week, Calculate Predictive Accuracy\n  group_by(week) %&gt;%\n  summarise(log_week_right = mean(log_right),\n         rf_week_right = mean(rf_right),\n         sv_week_right = mean(sv_right),\n         xgb_week_right = mean(xgb_right)) %&gt;%\n  # Pivot to Color by Model Type\n  pivot_longer(cols = starts_with(\"log_week_right\"):starts_with(\"xgb_week_right\"),\n               names_to = \"Model\",\n               values_to = \"Accuracy\") %&gt;%\n  mutate(Model = recode(Model,\n                        log_week_right = \"Logistic Regression\",\n                        rf_week_right = \"Random Forest\",\n                        sv_week_right = \"SVM\",\n                        xgb_week_right = \"XGBoost\")) %&gt;%\n  ggplot(aes(x = week, y = Accuracy, color = Model)) +\n  geom_line() +\n  geom_point() +\n  scale_x_continuous(breaks = 2:18) +\n  scale_y_continuous(breaks = seq(0.6, 1, by = 0.05),\n                     labels = scales::percent) + \n  scale_color_manual(\n    values = c(\"Logistic Regression\" = \"#e31837\", \n               \"Random Forest\" = \"#003594\", \n               \"SVM\" = \"#041e42\", \n               \"XGBoost\" = \"#ffb81c\")\n  ) +\n  labs(title = \"Week 1 is Excluded Due to Lack of In-Season Data\",\n       x = \"Week\",\n       y = \"Average Predictive Accuracy\",\n       color = \"Model\") +\n  blog_theme() + \n  theme(\n    plot.title = element_text(face = \"bold\"), \n    legend.title = element_text(face = \"bold\")  \n  )\n\n\n\n\n\nAverage In-Sample Predictive Accuracy by Model Over NFL Weeks\n\n\n\n\nLike a lot of things in the world of data science, when you plot the data expecting answers, you actually just get a lot more questions. While these report in-sample results (in contrast to cross-validated out-of-sample accuracy metrics… so take these accuracy numbers with a grain of salt), I still would have expected an upward trend over the NFL season, but nope! And there’s other interesting things as well… like how three of the models have a crazy dip in predictive performance in week 10. Don’t really know what that’s about. Well, even if the plot doesn’t support my diagnosis and prescription all that well, I’m convinced that pursuing a modeling strategy that incorporates prior information and domain knowledge would probably result in less “Saints over Cowboys” and “Vikings over 49ers” predictions.\n\n\nSetting Up My Workflow\nLastly, I want to document how I’m going to go about creating predictions every week. After all, I’ve collected data and trained some models, but there is no magic button I can press that will just sequentially update everything every week throughout the remainder of the NFL season. The following code chunk walks through my “workflow” so to speak.\n\n# Establish Global Week Parameters So I Don't Have to Update Every Data Set Individually\nlast_week &lt;- 1\nthis_week &lt;- 2\n\n# Load and Clean Updated Predictor Data\nqbr_2024 &lt;- load_espn_qbr(\n  seasons = 2024,\n  summary_type = c(\"week\")) %&gt;%\n  filter(season_type == \"Regular\") %&gt;%\n  select(c(team_abb, season, game_week, qbr_total, pts_added)) %&gt;%\n  group_by(season, team_abb) %&gt;%\n  mutate(\n    moving_qbr_mean = cumsum(qbr_total) / game_week,\n    moving_pts_added = cumsum(pts_added / game_week),\n    # Rename Washington for Merging\n    team_abb = ifelse(team_abb == \"WSH\", \"WAS\", team_abb)) %&gt;%\n  filter(game_week == last_week) %&gt;%\n  mutate(game_week = this_week)\n\noffensive_2024 &lt;- load_player_stats(\n  seasons = 2024,\n  stat_type = \"offense\") %&gt;%\n  filter(season_type == \"REG\") %&gt;%\n  group_by(season, recent_team, week) %&gt;%\n  summarise(\n    passing_epa = sum(passing_epa, na.rm = TRUE),\n    rushing_epa = sum(rushing_epa, na.rm = TRUE),\n    receiving_epa = sum(receiving_epa, na.rm = TRUE)\n  ) %&gt;%\n  ungroup() %&gt;%\n  group_by(season, recent_team) %&gt;%\n  mutate(\n    moving_passing_epa = cumsum(passing_epa) / week,\n    moving_rushing_epa = cumsum(rushing_epa) / week,\n    moving_receiving_epa = cumsum(receiving_epa) / week) %&gt;%\n  select(season, recent_team, week, passing_epa, rushing_epa, receiving_epa, moving_passing_epa, \n         moving_rushing_epa, moving_receiving_epa) %&gt;%\n  filter(week == last_week) %&gt;%\n  mutate(week = this_week) %&gt;%\n  mutate(recent_team = ifelse(recent_team == \"LA\", \"LAR\", recent_team))\n\ndefensive_2024 &lt;- load_player_stats(\n  seasons = 2024,\n  stat_type = \"defense\") %&gt;%\n  filter(season_type == \"REG\") %&gt;%\n  group_by(season, team, week) %&gt;%\n  summarise(\n    tackles = sum(def_tackles, na.rm = TRUE),\n    forced_fumbles = sum(def_fumbles_forced, na.rm = TRUE),\n    sacks = sum(def_sacks, na.rm = TRUE),\n    ints = sum(def_interceptions, na.rm = TRUE),\n    pass_broken = sum( def_pass_defended, na.rm = TRUE)\n  ) %&gt;%\n  ungroup() %&gt;%\n  group_by(season, team) %&gt;%\n  mutate(\n    moving_tackles = cumsum(tackles) / week,\n    moving_forced_fumbles = cumsum(forced_fumbles) / week,\n    moving_sacks = cumsum(sacks) / week,\n    moving_ints = cumsum(ints) / week,\n    moving_pass_broken = cumsum(pass_broken) / week) %&gt;%\n  select(season, team, week, tackles, forced_fumbles, sacks, ints, pass_broken, moving_tackles, \n         moving_forced_fumbles, moving_sacks, moving_ints, moving_pass_broken) %&gt;%\n  filter(week == last_week) %&gt;%\n  mutate(week = this_week) %&gt;%\n  mutate(team = ifelse(team == \"LA\", \"LAR\", team))\n\nseason_2024 &lt;- load_schedules(seasons = 2024)\n\nseason_2024 &lt;- clean_homeaway(season_2024) %&gt;%\n  filter(game_type == \"REG\") %&gt;%\n  mutate(home = ifelse(location == \"home\", 1, 0),\n         win = ifelse(team_score &gt; opponent_score, 1, 0)) %&gt;%\n  select(game_id, season, week, team, opponent, home, win) %&gt;%\n  filter(week == this_week) %&gt;%\n  mutate(team = ifelse(team == \"LA\", \"LAR\", team),\n         opponent = ifelse(opponent == \"LA\", \"LAR\", opponent))\n\nmerged_2024 &lt;- inner_join(season_2024, qbr_2024, \n                          by = c(\"team\" = \"team_abb\", \"week\" = \"game_week\", \"season\")) %&gt;%\n  inner_join(offensive_2024, by = c(\"team\" = \"recent_team\", \"week\", \"season\")) %&gt;%\n  inner_join(defensive_2024, by = c(\"team\", \"week\", \"season\")) %&gt;%\n  group_by(game_id) %&gt;%\n  mutate(opp_qbr = lead(moving_qbr_mean),\n         opp_qbr = ifelse(is.na(opp_qbr), \n                          lag(moving_qbr_mean), opp_qbr),\n         opp_pass_epa = lead(moving_passing_epa),\n         opp_pass_epa = ifelse(is.na(opp_pass_epa), \n                               lag(moving_passing_epa), opp_pass_epa),\n         opp_rushing_epa = lead(moving_rushing_epa),\n         opp_rushing_epa = ifelse(is.na(opp_rushing_epa), \n                                  lag(moving_rushing_epa), opp_rushing_epa),\n         opp_receiving_epa = lead(moving_receiving_epa),\n         opp_receiving_epa = ifelse(is.na(opp_receiving_epa), \n                                    lag(moving_receiving_epa), opp_receiving_epa),\n         opp_tackles = lead(moving_tackles),\n         opp_tackles = ifelse(is.na(opp_tackles), \n                              lag(moving_tackles), opp_tackles),\n         opp_forced_fumbles = lead(moving_forced_fumbles),\n         opp_forced_fumbles = ifelse(is.na(opp_forced_fumbles), \n                                     lag(moving_forced_fumbles), opp_forced_fumbles),\n         opp_sacks = lead(moving_sacks),\n         opp_sacks = ifelse(is.na(opp_sacks), \n                            lag(moving_sacks), opp_sacks),\n         opp_ints = lead(moving_ints),\n         opp_ints = ifelse(is.na(opp_ints), \n                           lag(moving_ints), opp_ints),\n         opp_pass_broken = lead(moving_pass_broken),\n         opp_pass_broken = ifelse(is.na(opp_pass_broken), \n                                  lag(moving_pass_broken), opp_pass_broken)\n         ) %&gt;%\n  mutate(\n    qbr_diff = moving_qbr_mean - opp_qbr,\n    pass_epa_diff = moving_passing_epa - opp_pass_epa,\n    rushing_epa_diff = moving_rushing_epa - opp_rushing_epa,\n    receiving_epa_diff = moving_receiving_epa - opp_receiving_epa,\n    tackles_diff = moving_tackles - opp_tackles,\n    forced_fumbles_diff = moving_forced_fumbles - opp_forced_fumbles,\n    sacks_diff = moving_sacks - opp_sacks,\n    ints_diff = moving_ints - opp_ints,\n    pass_broken_diff = moving_pass_broken - opp_pass_broken\n  ) %&gt;%\n  mutate(win = factor(win, levels = c(0, 1), labels = c(\"Lose\", \"Win\"))) %&gt;%\n  filter(!is.na(qbr_diff))\n\n# Load Trained Models\nlog_fit &lt;- readRDS(\"data-and-analysis/log_fit_model.rds\")\nrf_fit &lt;- readRDS(\"data-and-analysis/rf_fit_model.rds\")\nsv_fit &lt;- readRDS(\"data-and-analysis/sv_fit_model.rds\")\nxgb_fit &lt;- readRDS(\"data-and-analysis/xgb_fit_model.rds\")\n\n# Create a Data Frame with Model Predictions\nlog_preds &lt;- predict(log_fit, merged_2024, type = \"prob\")[,2]\nrf_preds &lt;- predict(rf_fit, merged_2024, type = \"prob\")[,2]\nsv_preds &lt;- predict(sv_fit, merged_2024, type = \"prob\")[,2]\nxgb_preds &lt;- predict(xgb_fit, merged_2024, type = \"prob\")[,2]\n\npredictions &lt;- tibble(\n  Team = merged_2024$team,\n  Game_ID = merged_2024$game_id,\n  week = merged_2024$week,\n  Logistic_Regression = paste0(round(log_preds * 100, 2), \"%\"),\n  Random_Forest = paste0(round(rf_preds * 100, 2), \"%\"),\n  SVM = paste0(round(sv_preds * 100, 2), \"%\"),\n  XGBoost = paste0(round(xgb_preds * 100, 2), \"%\")\n)\n\n# Create a Back-Up Spreadsheet Before Updating\nglobal_preds &lt;- read_excel(\"data-and-analysis/nfl_2024_global_preds.xlsx\")\nwrite_xlsx(global_preds, \"data-and-analysis/nfl_2024_global_preds_backup.xlsx\")\n\n# Add Model Predictions for This Week to Season-Level Spreadsheet\nupdated_preds &lt;- inner_join(predictions, global_preds, by = c(\n  \"Team\", \"Game_ID\", \"week\", \"Logistic_Regression\", \"Random_Forest\", \"SVM\", \"XGBoost\"\n))\nwrite_xlsx(updated_preds, \"data-and-analysis/nfl_2024_global_preds.xlsx\")\n\nNow that I’ve created this, I hope that clicking “Run” now serves as the magical button that I just have to click and I get new predictions every week. We will see how this goes, as I’m sure there’s some bug/dependency I’m missing."
  },
  {
    "objectID": "blog/2024/causal-inference-simulation/index.html",
    "href": "blog/2024/causal-inference-simulation/index.html",
    "title": "Causal Inference for Casuals",
    "section": "",
    "text": "Code\n# Load Libraries\npacman::p_load(\n  \"dplyr\", # Data Manipulation\n  \"ggdag\", # Visualizing DAGs\n  \"ggplot2\", # Data Visualization\n  \"scales\", # Plotting Percentage Values\n  install = FALSE\n)\n\n# Define a Custom Theme - Taken From Andrew Heiss's Blogs\nblog_theme &lt;- function() {\n  theme_bw() +  # Start with theme_bw\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      plot.title = element_text(face = \"bold\"),\n      axis.title = element_text(face = \"bold\"),\n      strip.text = element_text(face = \"bold\"),\n      strip.background = element_rect(fill = \"grey80\", color = NA),\n      legend.title = element_text(face = \"bold\")\n    )\n}"
  },
  {
    "objectID": "blog/2024/causal-inference-simulation/index.html#mediators",
    "href": "blog/2024/causal-inference-simulation/index.html#mediators",
    "title": "Causal Inference for Casuals",
    "section": "Mediators",
    "text": "Mediators\nThe first type of variable that we want to avoid controlling for is the mediator. A mediator is any variable that is, in part caused by \\(X\\) which then, in part, causes a change in \\(Y\\).\n\n\nCode\n# Set Up the DAG\nsimple_dag &lt;- dagify(\n  Y ~ Z + X + M,\n  X ~ Z,\n  M ~ X,\n  exposure = \"X\",\n  outcome = \"Y\",\n  coords = list(x = c(Y = 7, X = 4, Z = 5.5, M = 5.5),\n                y = c(Y = 2, X = 2, Z = 3, M = 1.5)),\n  labels = c(Y = \"Y\", X = \"X\", Z = \"Z\", M = \"M\")\n)\n\n# Convert DAG to a Tidy Object for Plotting\nsimple_dag_tidy &lt;- simple_dag %&gt;% \n  tidy_dagitty() %&gt;%\n  mutate(status = case_when(\n    name == \"X\" ~ \"exposure\",\n    name == \"Y\" ~ \"outcome\",\n    name == \"M\" ~ \"mediator\",\n    TRUE ~ \"other\"\n  ))\n\nstatus_colors &lt;- c(exposure = \"#6f1a1a\", outcome = \"#384d35\", mediator = \"#7d8d67\")\n\n# Create Plot\nggplot(simple_dag_tidy, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point(aes(color = status)) +\n  geom_label(aes(label = label, fill = status),\n             color = \"white\", fontface = \"bold\", nudge_y = 0.15) +\n  scale_color_manual(values = status_colors, na.value = \"grey20\") +\n  scale_fill_manual(values = status_colors, na.value = \"grey20\") +\n  guides(color = \"none\", fill = \"none\") +\n  theme_dag()\n\n\n\n\n\nDAG Incluing a Mediator\n\n\n\n\nControlling for a mediator should be avoided because, if we control for it, our estimated effect of \\(X\\) on \\(Y\\) will be a deflated version of the causal effect of interest because controlling for the mediator effectively removes the part of the causal effect of \\(X\\) on \\(Y\\) that is mediated through \\(M\\). We can see this clearly with the following simulated regressions.\n\nset.seed(1234)\n\n# Simulate Data Where X-Y Relationship is Confounded\n# Number of Observations\nn &lt;- 3000\n\n# Simulate the Confounder as a Continuous Variable\nZ &lt;- rnorm(n, mean = 50, sd = 10)\n\n# Simulate the Treatment as a Binary Variable That is Impacted by Z\n# Using the Inverse Logit Function\nprob_X &lt;- 1 / (1 + exp(-(-2.5 + (0.05*Z))))\nX &lt;- rbinom(n, size = 1, prob = prob_X)\n\n# Simulate a Mediator That is Impacted by X\nM &lt;- (10*X) + rnorm(n, mean = 0, sd = 10) \n\n# Simulate the Outcome with a Base Number to Start with (50), the Impact of X,\n# the Impact of Z, the Impact of M Mediated by X, and Random Noise. The Estimated Total Effect of X Should Be 30\nY &lt;- 50 + ((20*X) + M) + (5*Z) + rnorm(n, mean = 0, sd = 10) \n\n# Combine This Data\nsim_data &lt;- tibble(\n  n = 1:n,\n  Y = Y,\n  Z = Z,\n  X = X,\n  M = M,\n)\n\n# Run a Biased Model Where the Mediator is Included \nsummary(lm(Y ~ X + Z + M, data = sim_data))\n\n\nCall:\nlm(formula = Y ~ X + Z + M, data = sim_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.299  -6.454  -0.020   6.449  33.509 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 50.08945    0.91749   54.59   &lt;2e-16 ***\nX           20.17165    0.41190   48.97   &lt;2e-16 ***\nZ            5.00013    0.01851  270.16   &lt;2e-16 ***\nM            0.98048    0.01810   54.18   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.805 on 2996 degrees of freedom\nMultiple R-squared:  0.9708,    Adjusted R-squared:  0.9708 \nF-statistic: 3.325e+04 on 3 and 2996 DF,  p-value: &lt; 2.2e-16\n\n# Run an Unbiased Model Where the Mediator is Excluded\nsummary(lm(Y ~ X + Z, data = sim_data))\n\n\nCall:\nlm(formula = Y ~ X + Z, data = sim_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-52.431  -9.296   0.177   9.374  45.548 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 48.52958    1.29011   37.62   &lt;2e-16 ***\nX           30.06503    0.51941   57.88   &lt;2e-16 ***\nZ            5.03174    0.02602  193.35   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.79 on 2997 degrees of freedom\nMultiple R-squared:  0.9423,    Adjusted R-squared:  0.9422 \nF-statistic: 2.446e+04 on 2 and 2997 DF,  p-value: &lt; 2.2e-16\n\n\nNote that, in this blog post, \\(X\\) = 20 is the sort of unbiased causal effect we have been looking for so far. However, I’ve modified that a bit here because our mediator contributes to the causal effect of \\(X\\) on \\(Y\\). In reality, our causal effect of \\(X\\) should be the direct effect of \\(X\\) on \\(Y\\) (20) but it should also include the total effect that is mediated through \\(M\\) (10). In the first model, we see our direct effect is correct, but we are not attempting to estimate the direct effect. The direct effect is accurate because we are controlling for our mediator and our mediator is not confounded. That’s right… if you want to work with a mediator to estimate direct and indirect effects (this practice is referred to as mediation analysis) of \\(X\\) and \\(M\\) on \\(Y\\), you’ve doubled the challenge of dealing with confounding because, not only do you have to make sure that \\(X\\) \\(\\rightarrow\\) \\(Y\\) is unconfounded, you also have to make sure that \\(M\\) \\(\\rightarrow\\) \\(Y\\) is also unconfounded. This is a major reason behind the skepticism for mediation analyses. Regardless, as you can see in the second model where \\(M\\) is excluded, we get the correct total effect of \\(X\\) on \\(Y\\), which includes the effect of \\(X\\) on \\(Y\\) and the effect of \\(M\\) on \\(Y\\) that is caused by \\(X\\)."
  },
  {
    "objectID": "blog/2024/causal-inference-simulation/index.html#colliders",
    "href": "blog/2024/causal-inference-simulation/index.html#colliders",
    "title": "Causal Inference for Casuals",
    "section": "Colliders",
    "text": "Colliders\nColliders are interesting because they are very sneaky. I like to think of them as inverse confounders. You need to control for a confounder (\\(X\\) \\(\\leftarrow\\) \\(Z\\) \\(\\rightarrow\\) \\(Y\\)) but you should not control for a collider (\\(X\\) \\(\\rightarrow\\) \\(C\\) \\(\\leftarrow\\) \\(Y\\)). The crucial difference here is the causal ordering of the third variable as it relates to the treatment and outcome. Whereas a confounder is a common cause of \\(X\\) and \\(Y\\), a collider is a common consequence of \\(X\\) and \\(Y\\). In the following DAG, note that I am not positing that \\(X\\) impacts \\(Y\\). This is intentional.\n\n\nCode\n# Set Up the DAG\nsimple_dag &lt;- dagify(\n  Y ~ Z,\n  X ~ Z,\n  C ~ X + Y,\n  exposure = \"X\",\n  outcome = \"Y\",\n  coords = list(x = c(Y = 7, X = 4, Z = 5.5, C = 5.5),\n                y = c(Y = 2, X = 2, Z = 3, C = 1.5)),\n  labels = c(Y = \"Y\", X = \"X\", Z = \"Z\", C = \"C\")\n)\n\n# Convert DAG to a Tidy Object for Plotting\nsimple_dag_tidy &lt;- simple_dag %&gt;% \n  tidy_dagitty() %&gt;%\n  mutate(status = case_when(\n    name == \"X\" ~ \"exposure\",\n    name == \"Y\" ~ \"outcome\",\n    name == \"C\" ~ \"collider\",\n    TRUE ~ \"other\"\n  ))\n\nstatus_colors &lt;- c(exposure = \"#6f1a1a\", outcome = \"#384d35\", collider = \"#ec9b00\")\n\n# Create Plot\nggplot(simple_dag_tidy, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point(aes(color = status)) +\n  geom_label(aes(label = label, fill = status),\n             color = \"white\", fontface = \"bold\", nudge_y = 0.15) +\n  scale_color_manual(values = status_colors, na.value = \"grey20\") +\n  scale_fill_manual(values = status_colors, na.value = \"grey20\") +\n  guides(color = \"none\", fill = \"none\") +\n  theme_dag()\n\n\n\n\n\nDAG Incluing a Collider\n\n\n\n\nWhy does controlling for a common consequence of treatment and outcome lead to problems? Because it can suggest a spurious association between \\(X\\) and \\(Y\\). In other words, controlling for a collider can create an association between \\(X\\) and \\(Y\\) that does not reflect a causal relationship. This is why I like to think of colliders as inverse confounders. By default, confounders create spurious associations between \\(X\\) and \\(Y\\), and we combat this and remove the spurious association to make causal inferences by controlling for the confounder. In contrast, we can introduce spurious associations as well by controlling for certain variables; common causes.\nBut this is still vague… What is a hypothetical example of this? To keep thing’s grounded, I’ll borrow an example from Rohrer 2018. On a particularly lonely day, an individual may be reminiscing on their former romantic partners. They start thinking about the traits of their former romantic partners, and they begin to notice that their more attractive partners were less intelligent than their comparatively less attractive partners. (In DAG terms, this individual is considering whether Attractiveness \\(\\rightarrow\\) Intelligence). Should this person conclude that, among all potential partners, the more attractive they are, the less intelligent they will be? No! Because this observation is conditioned on a collider. And that collider is the fact they have dated the subjects. Most people are attracted to physically attractive and intelligent people, so whether or not this individual has dated someone is likely influenced by their attractiveness and intelligence (Attractiveness \\(\\rightarrow\\) Dated \\(\\leftarrow\\) Intelligence). We can clearly see that whether or not this individual has dated someone is a collider. By just looking at their former partners, they are introducing a spurious association between physical attraction and intelligence.\nOkay but, among this subset of people, the individual still finds a negative association between physical attraction and intelligence. Maybe it’s not fair to generalize this relationship to the broader dating pool, but how does that explain this individual’s former partners? We can explain this with a couple of reasons. First, it is rare that this individual could find someone else who is both highly attractive and highly intelligent. Even if they could, that person would likely already be taken. Second, this individual is probably unlikely to date someone who is also low in both physical attraction and intelligence. As a result, their former partners varied in both intelligence and physical attraction. Sometimes, a partner was more attractive, but not equally intelligent. At other times, a partner was more intelligent, but not as physically attractive. If this person would not have developed their conclusion based on their prior dating experiences (this is equivalent to controlling for people they have previously dated), they likely would not have found that this negative relationship holds up. In a very roundabout way, we have also described a type of selection bias here. By conditioning on a collider, we are limiting our analysis to a subset of cases that are impacted by treatment and outcome. This serves to bias our analysis and create associations that some might interpret as causal while no such association actually exists. This issue does not become a problem if you simply don’t control for a collider.\nLet’s put this into practice using simulation. In contrast to the mediator example, I am not going to simulate a relationship between \\(X\\) and \\(Y\\). If our model estimates such a relationship, there is a problem.\n\nset.seed(1234)\n\n# Simulate Data with No X-Y Relationship\n# Number of Observations\nn &lt;- 3000\n\n# Simulate the Confounder as a Continuous Variable\nZ &lt;- rnorm(n, mean = 50, sd = 10)\n\n# Simulate the Treatment as a Binary Variable That is Impacted by Z\n# Using the Inverse Logit Function\nprob_X &lt;- 1 / (1 + exp(-(-2.5 + (0.05*Z))))\nX &lt;- rbinom(n, size = 1, prob = prob_X)\n\n# Simulate the Outcome with a Base Number to Start with (50), \n# the Impact of Z, and Random Noise. \nY &lt;- 50 + (5*Z) + rnorm(n, mean = 0, sd = 10) \n\n# Simulate a Collider\nC &lt;- (10*X) + (10*Y) + rnorm(n, mean = 0, sd = 10) \n\n# Combine This Data\nsim_data &lt;- tibble(\n  n = 1:n,\n  Y = Y,\n  Z = Z,\n  X = X,\n  C = C,\n)\n\n# Run a Biased Model Including the Collider\nsummary(lm(Y ~ X + Z + C, data = sim_data))\n\n\nCall:\nlm(formula = Y ~ X + Z + C, data = sim_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3454 -0.6454  0.0037  0.6414  3.3643 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.3664761  0.1262877   2.902  0.00374 ** \nX           -0.9889660  0.0368645 -26.827  &lt; 2e-16 ***\nZ            0.0393890  0.0092391   4.263 2.08e-05 ***\nC            0.0992183  0.0001799 551.507  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9776 on 2996 degrees of freedom\nMultiple R-squared:  0.9996,    Adjusted R-squared:  0.9996 \nF-statistic: 2.735e+06 on 3 and 2996 DF,  p-value: &lt; 2.2e-16\n\n# Run an Unbiased Model Excluding the Collider\nsummary(lm(Y ~ X + Z, data = sim_data))\n\n\nCall:\nlm(formula = Y ~ X + Z, data = sim_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-31.885  -6.900  -0.085   6.837  36.475 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 48.40907    0.92564  52.298   &lt;2e-16 ***\nX            0.09033    0.37268   0.242    0.808    \nZ            5.03224    0.01867 269.502   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.897 on 2997 degrees of freedom\nMultiple R-squared:  0.9626,    Adjusted R-squared:  0.9626 \nF-statistic: 3.855e+04 on 2 and 2997 DF,  p-value: &lt; 2.2e-16\n\n\nAs you can see, when I controlled for the collider, a truly non-existent relationship between \\(X\\) and \\(Y\\) shows up. In contrast, when \\(C\\) is omitted, we don’t observe an effect of \\(X\\) on \\(Y\\) which, in this case, is true! Beware colliders, folks, and think carefully before you control for a variable."
  },
  {
    "objectID": "blog/2024/causal-inference-simulation/index.html#ancestors-of-outcome-and-treatment",
    "href": "blog/2024/causal-inference-simulation/index.html#ancestors-of-outcome-and-treatment",
    "title": "Causal Inference for Casuals",
    "section": "Ancestors of Outcome and Treatment",
    "text": "Ancestors of Outcome and Treatment\nLastly, we get to the ancestors. These types of variables are interesting because they are very common and you could identify a lot of them, but you don’t have to control for any of them. And, if you do control for them, it might not be a huge deal. Why is this the case? After all, there seemed to be pretty major consequences when controlling for a mediator/collider?\nAs you can see in the DAG below, we have two types of ancestors. An ancestor of \\(X\\) and an ancestor of \\(Y\\). Each of these impact their respective node, but are otherwise unconnected to other nodes in the DAG. Because neither is related to both \\(X\\) and \\(Y\\), controlling/not controlling for these should not impact the causal interpretation of your estimate… with a couple of caveats.\n\n\nCode\n# Set Up the DAG\nsimple_dag &lt;- dagify(\n  Y ~ Z + X + AY,\n  X ~ Z + AX,\n  exposure = \"X\",\n  outcome = \"Y\",\n  coords = list(x = c(Y = 7, X = 4, Z = 5.5, AX = 3.25, AY = 7.75),\n                y = c(Y = 2, X = 2, Z = 3, AX = 2.5, AY = 2.5)),\n  labels = c(Y = \"Y\", X = \"X\", Z = \"Z\", AX = \"AX\", AY = \"AY\")\n)\n\n# Convert DAG to a Tidy Object for Plotting\nsimple_dag_tidy &lt;- simple_dag %&gt;% \n  tidy_dagitty() %&gt;%\n  mutate(status = case_when(\n    name == \"X\" ~ \"exposure\",\n    name == \"Y\" ~ \"outcome\",\n    name == \"AX\" ~ \"ax\",\n    name == \"AY\" ~ \"ay\",\n    TRUE ~ \"other\"\n  ))\n\nstatus_colors &lt;- c(exposure = \"#6f1a1a\", outcome = \"#384d35\", ax = \"#a32b2b\", ay = \"#597657\")\n\n# Create Plot\nggplot(simple_dag_tidy, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point(aes(color = status)) +\n  geom_label(aes(label = label, fill = status),\n             color = \"white\", fontface = \"bold\", nudge_y = 0.1) +\n  scale_color_manual(values = status_colors, na.value = \"grey20\") +\n  scale_fill_manual(values = status_colors, na.value = \"grey20\") +\n  guides(color = \"none\", fill = \"none\") +\n  theme_dag()\n\n\n\n\n\nDAG Incluing Ancestors of Treatment and Outcome\n\n\n\n\nTo demonstrate, I am simply simulating a variable that causes a change in \\(X\\) (\\(AX\\)) and a variable that causes a change in \\(Y\\) (\\(AY\\))… and that’s all that these variables do. Then, I run a regression controlling for neither, and a respective regression controlling for each. Let’s see what we get.\n\nset.seed(1234)\n\n# Simulate Data Where X-Y Relationship is Confounded\n# Number of Observations\nn &lt;- 3000\n\n# Simulate the Confounder as a Continuous Variable\nZ &lt;- rnorm(n, mean = 50, sd = 10)\n\n# Simulate Ancestors of Treatment/Outcome\nAX &lt;- rnorm(n, mean = 10, sd = 10)\nAY &lt;- rnorm(n, mean = 10, sd = 10)\n\n# Simulate the Treatment as a Binary Variable That is Impacted by Z and AX\n# Using the Inverse Logit Function\nprob_X &lt;- 1 / (1 + exp(-(-2.5 + (0.05*Z) + (0.1*AX))))\nX &lt;- rbinom(n, size = 1, prob = prob_X)\n\n# Simulate the Outcome with a Base Number to Start with (50), the Impact of X,\n# the Impact of Z, the Impact of AY, and Random Noise. The Estimated Effect of X Should Be 20\nY &lt;- 50 + (20*X) + (5*Z) + (2*AY) + rnorm(n, mean = 0, sd = 10) \n\n# Combine This Data\nsim_data &lt;- tibble(\n  n = 1:n,\n  Y = Y,\n  Z = Z,\n  X = X,\n  AX = AX,\n  AY = AY\n)\n\n# Run a Regression Omitting Both the Ancestor of Treatment and Ancestor of Outcome\nsummary(lm(Y ~ X + Z, data = sim_data))\n\n\nCall:\nlm(formula = Y ~ X + Z, data = sim_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-76.469 -14.940   0.352  14.707  83.582 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 70.43861    2.08184   33.84   &lt;2e-16 ***\nX           18.82075    0.89826   20.95   &lt;2e-16 ***\nZ            5.01715    0.04171  120.28   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 22.21 on 2997 degrees of freedom\nMultiple R-squared:  0.849, Adjusted R-squared:  0.8489 \nF-statistic:  8426 on 2 and 2997 DF,  p-value: &lt; 2.2e-16\n\n# Run a Regression Only Including the Ancestor of Treatment\nsummary(lm(Y ~ X + Z + AX, data = sim_data))\n\n\nCall:\nlm(formula = Y ~ X + Z + AX, data = sim_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-75.912 -15.042   0.284  14.718  83.053 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 70.87527    2.11189  33.560   &lt;2e-16 ***\nX           19.26880    0.96962  19.873   &lt;2e-16 ***\nZ            5.01320    0.04183 119.834   &lt;2e-16 ***\nAX          -0.05481    0.04468  -1.227     0.22    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 22.21 on 2996 degrees of freedom\nMultiple R-squared:  0.8491,    Adjusted R-squared:  0.8489 \nF-statistic:  5619 on 3 and 2996 DF,  p-value: &lt; 2.2e-16\n\n# Run a Regression Only Including the Ancestor of Outcome\nsummary(lm(Y ~ X + Z + AY, data = sim_data))\n\n\nCall:\nlm(formula = Y ~ X + Z + AY, data = sim_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-39.969  -6.847   0.187   6.650  36.293 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 49.81184    0.95232   52.31   &lt;2e-16 ***\nX           19.75974    0.40282   49.05   &lt;2e-16 ***\nZ            5.00853    0.01870  267.80   &lt;2e-16 ***\nAY           1.99510    0.01828  109.15   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.957 on 2996 degrees of freedom\nMultiple R-squared:  0.9697,    Adjusted R-squared:  0.9696 \nF-statistic: 3.192e+04 on 3 and 2996 DF,  p-value: &lt; 2.2e-16\n\n\nWhen neither ancestor is included, we get pretty close to the actual treatment size. Similarly, when either ancestor is included, the estimated effect doesn’t change much and is pretty unbiased. So, what changed? Check out the standard errors! Without controlling for either ancestor, the standard error for the \\(X\\) estimate is 0.9. However, when we control for \\(AX\\), that standard error gets a bit bigger (0.97). In contrast, when we control for \\(AY\\), the standard error shrinks all the way down to 0.4. While controlling for either did not impact our causal effect estimate, including these variables did have an impact on the precision of our estimate. In other words, they respectively increased/decreased statistical uncertainty regarding whether these effect are statistically different from 0. In all cases, the estimated effect of \\(X\\) was still statistically significant, but that is likely due to how simple my simulated data are.\nSo what are those two caveats that I briefly mentioned a second ago. First, don’t always think of \\(AX\\) being bad. As a general rule, you probably should not control for \\(AX\\), but you should think about what your ancestors of treatment are. If, as I simulated, your ancestor of treatment only impacts \\(Y\\) through \\(X\\) and is totally unrelated to any other node in your DAG, you probably have an instrument which is a whole other can of worms that I won’t open here but basically, the TLDR is that you can use this instrument as a form of randomization… Long story but this is where instrumental variables designs are relevant. Although, another caveat is warranted here because identifying actual instruments is very, very hard in practice.\nThe second caveat is one that confuses me. Notice how I made the effect size of \\(AY\\) so small compared to all of the other effect sizes we have been working with thus far? Well, I did that because making \\(AY\\) large (as ancestors of outcome will often be in practice) really screwed up results and made the estimate for \\(X\\) very biased (like, down in the 13-16 range). I don’t know why this is the case. Theoretically, \\(AY\\) has nothing to do with \\(X\\), so including it should not impact the estimate for \\(X\\). Nonetheless, it did and I do not fully understand why. I don’t think I’ve discovered something novel… in reality, something is going on in the math behind the simulation that I’m sure I’m not understanding. Regardless, controlling for ancestors of outcome often results in increase to statistical precision, so it’s not like its the end of the world to have to control for \\(AY\\)."
  },
  {
    "objectID": "blog/2024/dynamic-causal-inference/index.html",
    "href": "blog/2024/dynamic-causal-inference/index.html",
    "title": "An Introduction to Dynamic Causal Inference",
    "section": "",
    "text": "Code\n# Load Libraries\npacman::p_load(\n  \"dplyr\", # Data Manipulation\n  \"ggplot2\", # Data Visualization\n  \"ggtext\", # Labels\n  \"dagitty\", # Creating DAGs\n  \"ggdag\", # Plotting DAGs\n  install = FALSE\n)\n\n# Define a Custom Theme - Taken From Andrew Heiss's Blogs\nblog_theme &lt;- function() {\n  theme_bw() +  # Start with theme_bw\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      plot.title = element_text(face = \"bold\"),\n      axis.title = element_text(face = \"bold\"),\n      strip.text = element_text(face = \"bold\"),\n      strip.background = element_rect(fill = \"grey80\", color = NA),\n      legend.title = element_text(face = \"bold\")\n    )\n}"
  },
  {
    "objectID": "blog/2024/dynamic-causal-inference/index.html#footnotes",
    "href": "blog/2024/dynamic-causal-inference/index.html#footnotes",
    "title": "An Introduction to Dynamic Causal Inference",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote: if you are already familiar with the core concepts of causal inference, feel free to skip down to the section where I start discussing time and why it messes things up.↩︎\nWell… kind of. There are actually a lot of other assumptions that you would need to check for first, like positivity, SUTVA, no measurement error, etc. (which will not be covered here in sufficient detail if you are not familiar with these topics). If you’re not super familiar with these topics, I highly recommend checking out Chatton and Rohrer 2024.↩︎\nI do not condone this attitude… never treat your regressions or research designs this way.↩︎\nHere are some good starting materials for DAGs, simulation, and sensitivity analyses: Rohrer 2018, Blair et al. 2023, Cinelli and Hazlett 2020.↩︎"
  },
  {
    "objectID": "blog/2024/simulating-panel-data/index.html",
    "href": "blog/2024/simulating-panel-data/index.html",
    "title": "Simulating Complex Panel Data to Validate Model Performance in Estimating Dynamic Treatment Effects",
    "section": "",
    "text": "Code\n# Load Libraries\npacman::p_load(\n  \"dplyr\", # Data Manipulation\n  \"ggplot2\", # Data Visualization\n  \"ggdag\", # Visualizing DAGs\n  \"dagitty\", # More DAGs Stuff\n  \"tidyr\", # Re-Shaping Data\n  \"purrr\", # map() Function\n  \"ipw\", # IPW\n  install = FALSE\n)\n\n# Define a Custom Theme - Taken From Andrew Heiss's Blogs\nblog_theme &lt;- function() {\n  theme_bw() +  # Start with theme_bw\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      plot.title = element_text(face = \"bold\"),\n      axis.title = element_text(face = \"bold\"),\n      strip.text = element_text(face = \"bold\"),\n      strip.background = element_rect(fill = \"grey80\", color = NA),\n      legend.title = element_text(face = \"bold\")\n    )\n}\n\n\n\nIntro\nOver the past year, I’ve spent a lot of time reviewing the literature covering the intersection between causal inference and panel (longitudinal) data. This intersection may sound niche, but I’ve almost always worked with panel data for academic, personal, and professional projects, and I’ve found that most of the introductory causal inference educational material teaches causal inference in the cross-sectional setting.\nThis practice makes sense because the moment you introduce time into a cross-sectional data set, making causal inferences becomes more complicated. However, I’ve found that in many fields (including my own), this necessary attention to detail is rarely ever acknowledged (if it is even widely understood). Researchers are well-trained in understanding how panel data creates problems for their standard errors and hypothesis testing. Yet, this extension to causal inference and point estimation still seems to fly under the radar quite a bit.\nSo, what’s the point of this blog? Well, its purpose is twofold. First, I want to show you how to simulate complex panel data that might resemble the data that you could end up working with. If you’re curious about why simulation matters, I highly recommend checking out my previous blog post first. Panel data isn’t something so simple to simulate if (and this is almost always the case) variables from a previous time impact variables in the future. Second, I want to show how conventional methodologies used with panel data (like regression adjustment) and others perform when we have complex panel data and want to estimate dynamic (lagged) effects. I am really trying to drive home the point that a lot of the conventional tools we use are very inappropriate and end up estimating very wrong results!\nWhy is this the case? Check out the following section! (And, if you want, check out my first blog post that also covers this topic as well.)\n\n\nThe Problems with Panel Data\nI’ll keep this part fairly sort because I do reference this point in this blog post, but panel data is problematic for both simulation and estimation.\nFor simulation, panel data is tricky because we have to build in time-varying effects into the simulation. In the simple cross-sectional setting, simulation is pretty easy. For example, consider the following code:\n\nset.seed(12345)\n\n# Establish the Sample Size\nn &lt;- 2000\n\ncross_sectional &lt;- tibble(\n  # Create a Cross-Sectional ID\n  id = 1:n,\n  # Create a Confounder\n  Z = rnorm(n, 0, 1),\n  # Create a Treatment\n  X = (0.5 * Z) + rnorm(n, 0, 1),\n  # Create an Outcome\n  Y = (0.5 * Z) + (0.3 * X) + rnorm(n, 0, 1)\n)\n\nThis is easy enough because the cross-sectional just represents a snapshot. But if we added time as an element, it get’s trickier. We have to specify how prior \\(Z\\) values impact future \\(Z\\), \\(X\\), and \\(Y\\) values. We have to specify how prior \\(X\\) values impact future \\(X\\), \\(Z\\), and \\(Y\\) values. We have to specify how prior \\(Y\\) values impact future \\(Y\\), \\(Z\\), and \\(X\\) values. Keep in mind, that sounds like a lot and we are only working with one confounder in this circumstance.\nStill, assuming just one confounder, you would need to specify one lagged effect for each variable if you’re panel data set included only two time periods. Basically, with two time periods (and assuming all prior values impact all current values), you’d need to specify the following for your three variables:\n\\[\nZ_t = Z_{t-1} + X_{t-1} + Y_{t-1} + \\mu\n\\]\n\\[\nX_t = Z_t + Z_{t-1} + X_{t-1} + Y_{t-1} + \\mu\n\\]\n\\[\nY_t = X_t + Z_t + Z_{t-1} + X_{t-1} + Y_{t-1} + \\mu\n\\]\nWhat if you had three time periods (\\(T\\) = 3)? Well, if you kept simulating using this same approach, you’d have to manually specify this same formula and model \\(X_{t-1}\\), \\(Y_{t-1}\\), \\(Z_{t-1}\\) as a function of \\(X_{t-2}\\), \\(Y_{t-2}\\), \\(Z_{t-2}\\). This is already a headache! As \\(T\\) increases, so does the amount of manual code. Fortunately, there is an easy fix for this that we’ll cover later. Now onto the second problem of panel data.\nPanel data really challenges how our conventional process of “controlling” for things works. Let’s create a couple of DAGs to illustrate why this is the case.\n\n\nCode\ndgp1_dag &lt;- dagitty('dag {\n  \"Z[TIC]\" [pos=\"2.5,2\"]\n  \"X[t-1]\" [pos=\"1,1\"]\n  \"Y[t-1]\" [pos=\"2,1.25\"]\n  \"X[t]\" [pos=\"3,1\"]\n  \"Y[t]\" [pos=\"4,1.25\"]\n  \"Z[TIC]\" -&gt; \"X[t-1]\"\n  \"Z[TIC]\" -&gt; \"Y[t-1]\"\n  \"Z[TIC]\" -&gt; \"X[t]\"\n  \"Z[TIC]\" -&gt; \"Y[t]\"\n  \"X[t-1]\" -&gt; \"Y[t-1]\"\n  \"Y[t-1]\" -&gt; \"X[t]\"\n  \"X[t]\" -&gt; \"Y[t]\"\n  \"X[t-1]\" -&gt; \"X[t]\"\n  \"Y[t-1]\" -&gt; \"Y[t]\"\n}') %&gt;%\n  tidy_dagitty()\n\nggplot(dgp1_dag, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point(color = \"grey80\", size = 16) +\n  geom_dag_text(color = \"black\", size = 5, parse = TRUE) +\n  theme_dag()\n\n\n\n\n\nDAG of Treatment-Outcome Feedback and a Time-Invariant Confounder\n\n\n\n\nThis DAG represents a data generating process (DGP) where treatment (\\(X\\)) impacts the outcome (\\(Y\\)) which subsequently impacts future treatments which impacts future outcomes, etc. In other words, this represents a treatment-outcome feedback loop. In addition, a time-invariant confounder (\\(Z_{TIC}\\)) impacts each treatment and outcome value. If I wanted to estimate the current and lagged effect of \\(X\\) on \\(Y\\) with this DGP, why can’t I just run lm(Y ~ X + lag(X) + Z, data = data)? After all, I am controlling for the confounder, so what’s the problem?\nThe issue is that panel data creates a “damned if you do, damned if you don’t” situation. If you look at this DAG closely, you might notice that something sneaky has happened. There is more than one confounder. Remember what a confounder is: any variable that causally impacts both the treatment and outcome of interest. If we want to estimate the effect of \\(X_{t}\\) on \\(Y_{t}\\), we need to control for \\(Z_{TIC}\\) yes, but pay attention to the \\(Y_{t-1}\\) node. It impacts both \\(X_{t}\\) and \\(Y_{t}\\). In other words, it confounds the relationship between treatment and outcome and, if we fail to control for it, our estimate if biased and confounded.\nSo, just control for it, right? What’s wrong with running lm(Y ~ X + lag(X) + Z + lag(Y), data = data)? Indeed, if you run this, you’d be able to estimate the effect of \\(X_{t}\\) on \\(Y_{t}\\), but you’d no longer be able to estimate the *lagged effect of \\(X\\). Why is that? By controlling for \\(Y{_t-1}\\), you are blocking the portion of the effect of \\(X{_t-1}\\) on \\(Y_{t}\\) that runs through \\(Y{_t-1}\\). To help visualize this, imagine drawing a big red “X” over the \\(Y{_t-1}\\) node. By controlling for it, it’s effect is blocked, which is necessary for making a causal inference about the \\(X_{t} \\rightarrow Y_{t}\\) relationship but simultaneously makes an unbiased causal inference about the \\(X_{t-1} \\rightarrow Y_{t}\\) relationship impossible. Damned if you do, damned if you don’t indeed.\nBut what if you don’t suspect a treatment-outcome feedback? Are you safe then? Nope! Because, in the prior DGP, the confounder was time-invariant. It doesn’t change over time. But a lot of confounders are time-varying and do change over time. The DAG below presents a DGP where a time-varying confounder is present.\n\n\nCode\ndgp3_dag &lt;- dagitty('dag {\n  \"X[t-1]\" [pos=\"1,3\"]\n  \"Y[t-1]\" [pos=\"3,3\"]\n  \"X[t]\" [pos=\"1,1\"]\n  \"Y[t]\" [pos=\"3,1\"]\n  \"Z[t-1]\" [pos=\"2,4\"]\n  \"Z[t]\" [pos=\"2,2\"]\n  \"Z[t-1]\" -&gt; \"X[t-1]\"\n  \"Z[t-1]\" -&gt; \"Y[t-1]\"\n  \"Z[t-1]\" -&gt; \"Z[t]\"\n  \"X[t-1]\" -&gt; \"Y[t-1]\"\n  \"X[t-1]\" -&gt; \"Z[t]\"\n  \"X[t-1]\" -&gt; \"X[t]\"\n  \"Y[t-1]\" -&gt; \"Y[t]\"\n  \"Z[t]\" -&gt; \"X[t]\"\n  \"Z[t]\" -&gt; \"Y[t]\"\n  \"X[t]\" -&gt; \"Y[t]\"\n}') %&gt;%\n  tidy_dagitty()\n\nggplot(dgp3_dag, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point(color = \"grey80\", size = 16) +\n  geom_dag_text(color = \"black\", size = 5, parse = TRUE) +\n  theme_dag()\n\n\n\n\n\nDAG of Time-Varying Confounding\n\n\n\n\nAs you can see, while we don’t have a treatment-outcome feedback loop, we do have a treatment-confounder feedback loop. And, as you could probably guess, this also creates a “damned if you do, damned if you don’t” situation. If I control for \\(Z_{t}\\), I de-bias the \\(X_{t} \\rightarrow Y_{t}\\) relationship but bias the \\(X_{t-1} \\rightarrow Y_{t}\\) relationship. If I decided to omit controlling for \\(Z_{t}\\), then the problem would be vice versa. No conventional adjustment method will solve this problem.\nControlling for a variable in a regression equation (regression adjustment), matching, fixed effects, including lagged variables as covariates, you name it. None of these resolve this issue. And that’s very problematic because time-varying treatments and confounders are probably the norm and not the exception. So, what are you to do with a panel data set where you are interested in the estimation of lagged treatment effects? Before diving into that, let’s resolve the first problem with panel data by demonstrating how you can simulate such complex data.\n\n\nSimulating Panel Data Generating Processes\nFirst, let’s figure out how to simulate a DGP like the one represented in the first DAG where we have treatment-outcome feedback and one time-invariant confounder. Also, massive shout-out to Andrew Heiss and his blog post where I learned about this approach. This first DGP will only include two time periods.\n\nset.seed(12345)\nn &lt;- 2000\n\ndgp1_wide &lt;- tibble(id = 1:n,\n                    # Create a Time Invariant Confounder\n                    Z = rnorm(n, 0, 1)) %&gt;% \n  # Create Lagged and Current Treatment and Outcome Values\n  mutate(X1 = (0.5 * Z) + rnorm(n, 0, 1),\n         Y1 = (0.3 * Z) + (0.4 * X1) + rnorm(n, 0, 1),\n         X2 = (0.5 * Z) + (0.4 * Y1) + (0.4 * X1) + rnorm(n, 0, 1),\n         Y2 = (0.1 * Z) + (0.4 * X2) + (0.4 * Y1) + rnorm(n, 0, 1))\n\nhead(dgp1_wide)\n\n# A tibble: 6 × 6\n     id      Z     X1     Y1      X2     Y2\n  &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1     1  0.586 -0.315 -0.993 -0.579  -2.06 \n2     2  0.709  1.43   0.601  0.142  -1.84 \n3     3 -0.109 -0.631 -0.106 -1.26    0.850\n4     4 -0.453  0.872 -0.456  0.714  -2.28 \n5     5  0.606  1.71  -0.139 -0.0182  0.629\n6     6 -1.82  -0.872 -0.758 -2.80   -3.54 \n\n\nThis is good, but we don’t have a time variable. Instead, we just have cross-sectional variables whose temporal components are separated into different columns. We can fix this by pivoting the data longer.\n\ndgp1_long &lt;- dgp1_wide %&gt;%\n  # Pivot This Wider to Create a Time ID Column\n  pivot_longer(cols = c(X1, Y1, X2, Y2)) %&gt;% \n  separate(name, into = c(\"variable\", \"time\"), sep = 1) %&gt;% \n  pivot_wider(names_from = \"variable\", values_from = \"value\") %&gt;% \n  # Create Lagged Treatment and Outcome Columns\n  group_by(id) %&gt;% \n  mutate(across(c(X, Y), list(lag = lag))) %&gt;% \n  ungroup()\n\nhead(dgp1_long)\n\n# A tibble: 6 × 7\n     id      Z time       X      Y  X_lag  Y_lag\n  &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     1  0.586 1     -0.315 -0.993 NA     NA    \n2     1  0.586 2     -0.579 -2.06  -0.315 -0.993\n3     2  0.709 1      1.43   0.601 NA     NA    \n4     2  0.709 2      0.142 -1.84   1.43   0.601\n5     3 -0.109 1     -0.631 -0.106 NA     NA    \n6     3 -0.109 2     -1.26   0.850 -0.631 -0.106\n\n\nAnd that checks out! But this is only for two time periods. As mentioned earlier, this approach will not work for more time periods, because we would have to specify the lagged effects for each variable for each time period. The following solution shows a way to automate this process while maintaining the same DGP.\n\n# Create a First Year Data Set \ndgp2_first_year &lt;- expand_grid(id = 1:n, time = 1) %&gt;% \n  mutate(Z = rnorm(n, 0, 1),  \n         X = (0.5 * Z) + rnorm(n, 0, 1),\n         Y = (0.3 * Z) + (0.4 * X) + rnorm(n, 0, 1))\n\n# Add Another 9 Empty Time Periods\ndgp2_panel_empty &lt;- dgp2_first_year %&gt;% \n  bind_rows(expand_grid(id = 1:n, time = 2:10)) %&gt;% \n  arrange(id, time)\n\n# Add Noise with a Custom dgp() Function\ndgp2 &lt;- function(df) {\n  for (i in 2:nrow(df)) {\n    df$X[i] &lt;- (0.5 * df$Z[i]) + (0.4 * df$Y[i - 1]) + (0.4 * df$X[i - 1]) + rnorm(1, 0, 1)\n    df$Y[i] &lt;- (0.3 * df$Z[i]) + (0.4 * df$X[i]) + (0.4 * df$Y[i - 1]) + rnorm(1, 0, 1)\n  }\n  df\n}\n\n# Apply the dgp2() Function to the Empty Data Set\ndgp2_long &lt;- dgp2_panel_empty %&gt;% \n  group_by(id) %&gt;% \n  # Make Z Constant Across Time\n  mutate(Z = Z[1]) %&gt;%  # Propagate Z across all rows\n  # Nest the Data Into a Single Cell in Each Row\n  nest() %&gt;% \n  # Run dgp() on the Nested Cell\n  mutate(dgp = map(data, dgp2)) %&gt;% \n  select(-data) %&gt;% \n  # Unnest the Nested dgp()-ed Cells\n  unnest(dgp) %&gt;% \n  # Add Lags\n  mutate(across(c(X, Y), list(lag = lag))) %&gt;% \n  ungroup()\n\nhead(dgp2_long)\n\n# A tibble: 6 × 7\n     id  time     Z      X     Y  X_lag  Y_lag\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     1     1 0.161  0.850 0.773 NA     NA    \n2     1     2 0.161 -0.268 0.783  0.850  0.773\n3     1     3 0.161 -1.02  0.105 -0.268  0.783\n4     1     4 0.161  1.04  1.66  -1.02   0.105\n5     1     5 0.161  0.395 2.09   1.04   1.66 \n6     1     6 0.161  1.59  3.12   0.395  2.09 \n\n\nAnd that worked! Okay, but what if you’d instead be interested in simulating something like the other DGP we discussed without treatment-outcome feedback, but instead featuring a treatment-confounder feedback loop? I’ll also simulate a simpler two-period data set first and then expand it to 10 time periods. First, the two-period simulated data:\n\ndgp3_wide &lt;- tibble(id = 1:n) %&gt;% \n  # Create Lagged and Current Treatment and Outcome Values\n  mutate(Z1 = rnorm(n, 0, 1),\n         X1 = (0.5 * Z1) + rnorm(n, 0, 1),\n         Y1 = (0.3 * Z1) + (0.4 * X1) + rnorm(n, 0, 1),\n         Z2 = (0.6 * Z1) + (0.4 * X1) + rnorm(n, 0, 1),\n         X2 = (0.5 * Z2) + (0.4 * X1) + rnorm(n, 0, 1),\n         Y2 = (0.3 * Z2) + (0.4 * X2) + (0.4 * Y1) + rnorm(n, 0, 1))\n\ndgp3_long &lt;- dgp3_wide %&gt;%\n  # Pivot This Wider to Create a Time ID Column\n  pivot_longer(cols = c(X1, Y1, Z1, X2, Y2, Z2)) %&gt;% \n  separate(name, into = c(\"variable\", \"time\"), sep = 1) %&gt;% \n  pivot_wider(names_from = \"variable\", values_from = \"value\") %&gt;% \n  # Create Lagged Treatment and Outcome Columns\n  group_by(id) %&gt;% \n  mutate(across(c(X, Y, Z), list(lag = lag))) %&gt;% \n  ungroup()\n\nhead(dgp3_long)\n\n# A tibble: 6 × 8\n     id time       X       Y      Z  X_lag   Y_lag  Z_lag\n  &lt;int&gt; &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1     1 1     -0.110 -0.571  -0.276 NA     NA      NA    \n2     1 2     -0.956 -1.60   -0.677 -0.110 -0.571  -0.276\n3     2 1      2.37   0.0209  0.869 NA     NA      NA    \n4     2 2      2.80   2.01    1.93   2.37   0.0209  0.869\n5     3 1     -2.80  -1.99   -1.70  NA     NA      NA    \n6     3 2     -0.386 -1.28   -1.15  -2.80  -1.99   -1.70 \n\n\nAgain, this is simple enough and the only real change we had to make was specifying a dynamic time-varying effect for the confounder. Let’s see what the code looks like with 10 time-periods:\n\n# Create the Data in the First Time Period\ndgp4_first_year &lt;- expand_grid(id = 1:n, time = 1) %&gt;% \n  mutate(Z = rnorm(n, 0, 1), \n         X = (0.5 * Z) + rnorm(n, 0, 1),\n         Y = (0.3 * Z) + (0.4 * X) + rnorm(n, 0, 1))\n\n# Add Another 9 Empty Time Periods\ndgp4_panel_empty &lt;- dgp4_first_year %&gt;% \n  bind_rows(expand_grid(id = 1:n, time = 2:10)) %&gt;% \n  arrange(id, time)\n\n# Add Noise with a Custom dgp() Function\ndgp4 &lt;- function(df) {\n  for (i in 2:nrow(df)) {\n    df$Z[i] &lt;- (0.6 * df$Z[i - 1]) + (0.4 * df$X[i - 1]) + rnorm(1, 0, 1)\n    df$X[i] &lt;- (0.5 * df$Z[i]) + (0.4 * df$Y[i - 1]) + (0.4 * df$X[i - 1]) + rnorm(1, 0, 1)\n    df$Y[i] &lt;- (0.3 * df$Z[i]) + (0.4 * df$X[i]) + (0.4 * df$Y[i - 1]) + rnorm(1, 0, 1)\n  }\n  df\n}\n\n# Apply the dgp() Function to the Empty Data Set\ndgp4_long &lt;- dgp4_panel_empty %&gt;% \n  group_by(id) %&gt;% \n  # Nest the Data Into a Single Cell in Each Row\n  nest() %&gt;% \n  # Run dgp() on the Nested Cell\n  mutate(dgp = map(data, dgp4)) %&gt;% \n  select(-data) %&gt;% \n  # Unnest the Nested dgp()-ed Cells\n  unnest(dgp) %&gt;% \n  # Add Lags\n  mutate(across(c(X, Y, Z), list(lag = lag))) %&gt;% \n  ungroup()\n\nhead(dgp4_long)\n\n# A tibble: 6 × 8\n     id  time      Z      X      Y  X_lag  Y_lag  Z_lag\n  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     1     1 -0.228  0.237 -0.641 NA     NA     NA    \n2     1     2  0.319 -1.66   0.660  0.237 -0.641 -0.228\n3     1     3  0.180 -0.434  2.53  -1.66   0.660  0.319\n4     1     4 -1.66  -0.234  0.575 -0.434  2.53   0.180\n5     1     5 -0.451  0.845 -0.401 -0.234  0.575 -1.66 \n6     1     6  1.12  -0.388  0.707  0.845 -0.401 -0.451\n\n\nAgain, this is largely the same code as the code used to generate DGP 2, the only difference here is that we have to simulate the dynamic nature of the confounder in our dgp4() function. So, now that we’ve got this part of the puzzle figured out and we have four data sets where we know what the actual effects are, let’s test some models and see which generally perform the best at estimating the true effect.\n\n\nEstimating and Testing Panel Data Models\nFirst, we’ll start with the OG - standard regression adjustment, where we model a relationship between the outcome and treatment and adjust for confounding by including the control variables as covariates in the regression equation. Below, I estimate regressions for each DGP with some variation within-DGP. First, I estimate models that are confounded (they do not adjust for \\(Z\\)) in contrast to models that adjust for \\(Z\\). I do this to show that, even when you control for a confounder, that is not enough to get an unbiased estimate with time-varying treatments and confounders. Second, I estimate some models with and without a lagged effect specified. If the complications of panel data create a “damned if you do, damned if you don’t” situation, then perhaps the models only seeking to model the immediate effect of \\(X\\) on \\(Y\\) will perform better.\n\ndgp1_reg_confounded &lt;- lm(Y ~ X, data = dgp1_long)\ndgp1_reg_adjusted &lt;- lm(Y ~ X + Z, data = dgp1_long)\ndgp1_reg_confounded_lag &lt;- lm(Y ~ X + X_lag, data = dgp1_long)\ndgp1_reg_adjusted_lag &lt;- lm(Y ~ X + X_lag + Z, data = dgp1_long)\n\ndgp2_reg_confounded &lt;- lm(Y ~ X, data = dgp2_long)\ndgp2_reg_adjusted &lt;- lm(Y ~ X + Z, data = dgp2_long)\ndgp2_reg_confounded_lag &lt;- lm(Y ~ X + X_lag, data = dgp2_long)\ndgp2_reg_adjusted_lag &lt;- lm(Y ~ X + X_lag + Z, data = dgp2_long)\n\ndgp3_reg_confounded &lt;- lm(Y ~ X, data = dgp3_long)\ndgp3_reg_adjusted &lt;- lm(Y ~ X + Z, data = dgp3_long)\ndgp3_reg_confounded_lag &lt;- lm(Y ~ X + X_lag, data = dgp3_long)\ndgp3_reg_adjusted_lag &lt;- lm(Y ~ X + X_lag + Z, data = dgp3_long)\n\ndgp4_reg_confounded &lt;- lm(Y ~ X, data = dgp4_long)\ndgp4_reg_adjusted &lt;- lm(Y ~ X + Z, data = dgp4_long)\ndgp4_reg_confounded_lag &lt;- lm(Y ~ X + X_lag, data = dgp4_long)\ndgp4_reg_adjusted_lag &lt;- lm(Y ~ X + X_lag + Z, data = dgp4_long)\n\nNext, I specify a series of autoregressive distributed lag (ADL) models with both confounded and adjusted estimates. Basically, this model includes the use of a lagged dependent variable (LDV) as a covariate in the regression model. We probably should not put too much stock in the ADL results, especially for the estimation of lagged effects. Recall the DAGs created to model the DGPs discussed in this blog. \\(Y_{t-1}\\) (the lagged dependent variable) is on the causal pathway that connects \\(X_{t-1}\\) to \\(Y_{t}\\). By including a LDV in the model, we are removing part of the lagged effect of \\(X\\). So, in theory, these should not perform well at estimating lagged effects.\n\ndgp1_adl_confounded &lt;- lm(Y ~ X + X_lag + Y_lag, data = dgp1_long)\ndgp1_adl_adjusted &lt;- lm(Y ~ X + X_lag + Y_lag + Z, data = dgp1_long)\n\ndgp2_adl_confounded &lt;- lm(Y ~ X + X_lag + Y_lag, data = dgp2_long)\ndgp2_adl_adjusted &lt;- lm(Y ~ X + X_lag + Y_lag + Z, data = dgp2_long)\n\ndgp3_adl_confounded &lt;- lm(Y ~ X + X_lag + Y_lag, data = dgp3_long)\ndgp3_adl_adjusted &lt;- lm(Y ~ X + X_lag + Y_lag + Z, data = dgp3_long)\n\ndgp4_adl_confounded &lt;- lm(Y ~ X + X_lag + Y_lag, data = dgp4_long)\ndgp4_adl_adjusted &lt;- lm(Y ~ X + X_lag + Y_lag + Z, data = dgp4_long)\n\nLastly, I attempt to recover the current and lagged treatment effects by using something called (and take a deep breath because it’s a long name) a “marginal structural model with inverse probability weights”. We could (and will) simplify this by using acronyms like MSM with IPW or, more succinctly, IPW. This is for sure the more complicated process, but it should yield the best results. While this blog is not devoted towards explaining MSMs with IPW (that is the next blog I’m working on), I will briefly walk through the estimation of MSMs with IPW. Even if you don’t fully understand the theory of these models, their performance should represent their value in the panel data setting very well.\nFirst, we have to estimate the inverse probability weights… What are these? Basically, they are weights that we construct that represent the inverse of the estimated probability that a given unit at a given time will receive their treatment status. What is the point of that complicated sounding (but less complicated in practice) process? Well, the idea is that we use these weights to create a pseudo-population that, if all confounding factors are accounted for, represents the distribution of treatment if treatment was randomly allocated. We create this pseudo-population by weighting our data using the inverse probability weights.\nIn an observational setting, the probability of any given treatment value will not be identical for treated and non-treated units because units self-select or are selected into their treatment values non-randomly. You can see this in the top plot. However, when weighting units by the inverse probability of treatment, we can see what the distribution of treatment would have looked like if treatment was assigned at random (or, in other words, was assigned at random). This is great for making causal inferences because it mimics the randomization properties of a randomized controlled trial.\nBut how does any of this get around the “damned if you do, damned if you don’t” situation. You might be thinking that this is just another way to adjust for confounding. And while IPW does adjust for confounding (just as regression adjustment, matching, etc. also would), inverse probability weights can also be constructed so that we don’t have the “damned if you do, damned if you don’t” problem. Summarizing this way too simply, IPW is valuable because inverse probability weights are generated at each time point and are modeled based on each unit’s treatment history. Rather than brute-force adjustment of a confounder, IPW provides more nuance and tact and specifically accounts for temporal dynamics in it’s confounding adjustment strategy. There’s a lot more to this than what I just described, but that’s all I have room for in this blog post. Now, on to estimation.\nFirst, I’ll start by constructing the inverse probability weights using the {ipw} package. Before I can create weights, I have to remove NA observations since the {ipw} package does not like a data set with missingness.\n\n# First, Create Data Frames Without NAs for the {ipw} Package\ndgp1_wo_na &lt;- dgp1_long %&gt;%\n  filter(!is.na(X_lag)) %&gt;% \n  mutate(time = as.numeric(time))\n\ndgp2_wo_na &lt;- dgp2_long %&gt;%\n  filter(!is.na(X_lag)) %&gt;% \n  mutate(time = as.numeric(time))\n\ndgp3_wo_na &lt;- dgp3_long %&gt;%\n  filter(!is.na(X_lag)) %&gt;% \n  mutate(time = as.numeric(time))\n\ndgp4_wo_na &lt;- dgp4_long %&gt;%\n  filter(!is.na(X_lag)) %&gt;% \n  mutate(time = as.numeric(time))\n\n# Create Weights for DGP 1\npanel_weights_dgp1 &lt;- ipwtm(\n  exposure = X,\n  family = \"gaussian\", \n  numerator = ~ X_lag + Z, \n  denominator = ~ X_lag + Y_lag + Z, \n  id = id,\n  timevar = time,\n  type = \"all\",\n  corstr = \"ar1\",\n  data = as.data.frame(dgp1_wo_na))\n\n# Store Weights for DGP 1\npanel_weights_dgp1 &lt;- dgp1_wo_na %&gt;% \n  mutate(ipw = panel_weights_dgp1$ipw.weights)\n\nAnd, just like that, inverse probability weights have been generated for DGP 1. However, there’s more nuance to IPW than just that. A common practice in IPW is the process known as truncation. The basic idea is that, when we estimate treatment probabilities and take the inverse, we can end up with a select few crazy large or small weights that throw the results off. A common practice in this case is the process of truncation where we select an artifical cut-off (1% and 5% most commonly) and remove the most extreme IPWs from the sample. I also create IPWs with 1% and 5% truncation.\n\n# Create Weights for DGP 1 - 5% Truncation\npanel_weights_dgp1_95 &lt;- ipwtm(\n  exposure = X,\n  family = \"gaussian\", \n  numerator = ~ X_lag + Z, \n  denominator = ~ X_lag + Y_lag + Z, \n  id = id,\n  timevar = time,\n  type = \"all\",\n  corstr = \"ar1\",\n  trunc = 0.05,\n  data = as.data.frame(dgp1_wo_na))\n\n# Store Weights for DGP 1 - 5% Truncation\npanel_weights_dgp1_95 &lt;- dgp1_wo_na %&gt;% \n  mutate(ipw = panel_weights_dgp1_95$weights.trunc)\n\n# Create Weights for DGP 1 - 1% Truncation\npanel_weights_dgp1_99 &lt;- ipwtm(\n  exposure = X,\n  family = \"gaussian\", \n  numerator = ~ X_lag + Z, \n  denominator = ~ X_lag + Y_lag + Z, \n  id = id,\n  timevar = time,\n  type = \"all\",\n  corstr = \"ar1\",\n  trunc = 0.01,\n  data = as.data.frame(dgp1_wo_na))\n\n# Store Weights for DGP 1 - 1% Truncation\npanel_weights_dgp1_99 &lt;- dgp1_wo_na %&gt;% \n  mutate(ipw = panel_weights_dgp1_99$weights.trunc)\n\nAlright and now, I’ll do all of this all over again to get IPWs for DGPs 2-4. This code is hidden under a code fold because it’s loooong, but feel free to expand it if you want.\n\n\nCode\n# Create Weights for DGP 2\npanel_weights_dgp2 &lt;- ipwtm(\n  exposure = X,\n  family = \"gaussian\", \n  numerator = ~ X_lag + Z, \n  denominator = ~ X_lag + Y_lag + Z, \n  id = id,\n  timevar = time,\n  type = \"all\",\n  corstr = \"ar1\",\n  data = as.data.frame(dgp2_wo_na))\n\n# Store Weights for DGP 2\npanel_weights_dgp2 &lt;- dgp2_wo_na %&gt;% \n  mutate(ipw = panel_weights_dgp2$ipw.weights)\n\n# Create Weights for DGP 2 - 5% Truncation\npanel_weights_dgp2_95 &lt;- ipwtm(\n  exposure = X,\n  family = \"gaussian\", \n  numerator = ~ X_lag + Z, \n  denominator = ~ X_lag + Y_lag + Z, \n  id = id,\n  timevar = time,\n  type = \"all\",\n  corstr = \"ar1\",\n  trunc = 0.05,\n  data = as.data.frame(dgp2_wo_na))\n\n# Store Weights for DGP 2 - 5% Truncation\npanel_weights_dgp2_95 &lt;- dgp2_wo_na %&gt;% \n  mutate(ipw = panel_weights_dgp2_95$weights.trunc)\n\n# Create Weights for DGP 2 - 1% Truncation\npanel_weights_dgp2_99 &lt;- ipwtm(\n  exposure = X,\n  family = \"gaussian\", \n  numerator = ~ X_lag + Z, \n  denominator = ~ X_lag + Y_lag + Z, \n  id = id,\n  timevar = time,\n  type = \"all\",\n  corstr = \"ar1\",\n  trunc = 0.01,\n  data = as.data.frame(dgp2_wo_na))\n\n# Store Weights for DGP 2 - 1% Truncation\npanel_weights_dgp2_99 &lt;- dgp2_wo_na %&gt;% \n  mutate(ipw = panel_weights_dgp2_99$weights.trunc)\n\n# Create Weights for DGP 3\npanel_weights_dgp3 &lt;- ipwtm(\n  exposure = X,\n  family = \"gaussian\", \n  numerator = ~ X_lag + Z, \n  denominator = ~ X_lag + Y_lag + Z, \n  id = id,\n  timevar = time,\n  type = \"all\",\n  corstr = \"ar1\",\n  data = as.data.frame(dgp3_wo_na))\n\n# Store Weights for DGP 3\npanel_weights_dgp3 &lt;- dgp3_wo_na %&gt;% \n  mutate(ipw = panel_weights_dgp3$ipw.weights)\n\n# Create Weights for DGP 3 - 5% Truncation\npanel_weights_dgp3_95 &lt;- ipwtm(\n  exposure = X,\n  family = \"gaussian\", \n  numerator = ~ X_lag + Z, \n  denominator = ~ X_lag + Y_lag + Z, \n  id = id,\n  timevar = time,\n  type = \"all\",\n  corstr = \"ar1\",\n  trunc = 0.05,\n  data = as.data.frame(dgp3_wo_na))\n\n# Store Weights for DGP 3 - 5% Truncation\npanel_weights_dgp3_95 &lt;- dgp3_wo_na %&gt;% \n  mutate(ipw = panel_weights_dgp3_95$weights.trunc)\n\n# Create Weights for DGP 3 - 1% Truncation\npanel_weights_dgp3_99 &lt;- ipwtm(\n  exposure = X,\n  family = \"gaussian\", \n  numerator = ~ X_lag + Z, \n  denominator = ~ X_lag + Y_lag + Z, \n  id = id,\n  timevar = time,\n  type = \"all\",\n  corstr = \"ar1\",\n  trunc = 0.01,\n  data = as.data.frame(dgp3_wo_na))\n\n# Store Weights for DGP 3 - 1% Truncation\npanel_weights_dgp3_99 &lt;- dgp3_wo_na %&gt;% \n  mutate(ipw = panel_weights_dgp3_99$weights.trunc)\n\n# Create Weights for DGP 4\npanel_weights_dgp4 &lt;- ipwtm(\n  exposure = X,\n  family = \"gaussian\", \n  numerator = ~ X_lag + Z, \n  denominator = ~ X_lag + Y_lag + Z, \n  id = id,\n  timevar = time,\n  type = \"all\",\n  corstr = \"ar1\",\n  data = as.data.frame(dgp4_wo_na))\n\n# Store Weights for DGP 4\npanel_weights_dgp4 &lt;- dgp4_wo_na %&gt;% \n  mutate(ipw = panel_weights_dgp4$ipw.weights)\n\n# Create Weights for DGP 4 - 5% Truncation\npanel_weights_dgp4_95 &lt;- ipwtm(\n  exposure = X,\n  family = \"gaussian\", \n  numerator = ~ X_lag + Z, \n  denominator = ~ X_lag + Y_lag + Z, \n  id = id,\n  timevar = time,\n  type = \"all\",\n  corstr = \"ar1\",\n  trunc = 0.05,\n  data = as.data.frame(dgp4_wo_na))\n\n# Store Weights for DGP 4 - 5% Truncation\npanel_weights_dgp4_95 &lt;- dgp4_wo_na %&gt;% \n  mutate(ipw = panel_weights_dgp4_95$weights.trunc)\n\n# Create Weights for DGP 4 - 1% Truncation\npanel_weights_dgp4_99 &lt;- ipwtm(\n  exposure = X,\n  family = \"gaussian\", \n  numerator = ~ X_lag + Z, \n  denominator = ~ X_lag + Y_lag + Z, \n  id = id,\n  timevar = time,\n  type = \"all\",\n  corstr = \"ar1\",\n  trunc = 0.01,\n  data = as.data.frame(dgp4_wo_na))\n\n# Store Weights for DGP 4 - 1% Truncation\npanel_weights_dgp4_99 &lt;- dgp4_wo_na %&gt;% \n  mutate(ipw = panel_weights_dgp4_99$weights.trunc)\n\n\nNow that we have all of the IPWs, we actually have to apply them to a marginal structural model (MSM). Don’t worry, this part is easy. It’s literally as simple as just running a regression (like normal) but including a weights argument and supplying that argument with the IPWs:\n\ndgp1_ipw_notrunc &lt;- lm(Y ~ X + X_lag + Z, data = panel_weights_dgp1, weights = ipw)\ndgp1_ipw_95trunc &lt;- lm(Y ~ X + X_lag + Z, data = panel_weights_dgp1_95, weights = ipw)\ndgp1_ipw_99trunc &lt;- lm(Y ~ X + X_lag + Z, data = panel_weights_dgp1_99, weights = ipw)\n\ndgp2_ipw_notrunc &lt;- lm(Y ~ X + X_lag + Z, data = panel_weights_dgp2, weights = ipw)\ndgp2_ipw_95trunc &lt;- lm(Y ~ X + X_lag + Z, data = panel_weights_dgp2_95, weights = ipw)\ndgp2_ipw_99trunc &lt;- lm(Y ~ X + X_lag + Z, data = panel_weights_dgp2_99, weights = ipw)\n\ndgp3_ipw_notrunc &lt;- lm(Y ~ X + X_lag + Z, data = panel_weights_dgp3, weights = ipw)\ndgp3_ipw_95trunc &lt;- lm(Y ~ X + X_lag + Z, data = panel_weights_dgp3_95, weights = ipw)\ndgp3_ipw_99trunc &lt;- lm(Y ~ X + X_lag + Z, data = panel_weights_dgp3_99, weights = ipw)\n\ndgp4_ipw_notrunc &lt;- lm(Y ~ X + X_lag + Z, data = panel_weights_dgp4, weights = ipw)\ndgp4_ipw_95trunc &lt;- lm(Y ~ X + X_lag + Z, data = panel_weights_dgp4_95, weights = ipw)\ndgp4_ipw_99trunc &lt;- lm(Y ~ X + X_lag + Z, data = panel_weights_dgp4_99, weights = ipw)\n\nOkay, we are almost there. We are almost to the point where we can compare how well each of these dozens of models performed. Before I visually represent this, I am going to store all of these results in a single data frame:\n\n# Store Results in a Data Frame\nresults_list &lt;- list()\n\n# Create a Function to Store Results\nstore_results &lt;- function(dgp, method, model, trunc, confounded) {\n  # Store X Estimate\n  x_est &lt;- coef(model)[2] \n  # Store Lagged X Estimate\n  x_lag_est &lt;- if (\"X_lag\" %in% names(coef(model))) coef(model)[\"X_lag\"] else NA\n  # Store and Separate Confidence Intervals for Estimates\n  ci &lt;- confint(model)\n  ci_lower &lt;- ci[2, 1]  \n  ci_upper &lt;- ci[2, 2]\n  \n  # Store Confidence Intervals for X Lag Estimate\n  # Set Default to NA Since Some Models Don't Estimate Lagged Effects\n  ci_x_lag_lower &lt;- NA\n  ci_x_lag_upper &lt;- NA\n  if (!is.na(x_lag_est)) {\n    ci_x_lag &lt;- confint(model)[\"X_lag\", ] \n    ci_x_lag_lower &lt;- ci_x_lag[1]  \n    ci_x_lag_upper &lt;- ci_x_lag[2]\n  }\n  \n  # Create a New Data Frame to Store Results\n  new_result &lt;- data.frame(\n    dgp = as.factor(dgp),\n    method = method,\n    x_estimate = x_est,\n    # Define the True Effect of X\n    true_x = 0.4,\n    x_lag_estimate = x_lag_est,\n    # Define the True Effects of X Lagged (This Is Different for DGPs 1&3 vs. DGPs 2&4)\n    true_x_lag = ifelse(dgp %in% c(1, 3), 0.16, 0.096),\n    ci_lower = ci_lower,\n    ci_upper = ci_upper,\n    ci_lower_x_lag = ci_x_lag_lower,  \n    ci_upper_x_lag = ci_x_lag_upper,  \n    confounded = as.factor(confounded),\n    truncation = trunc\n  )\n  \n  # Update Results List with Each New Model Run\n  results_list &lt;&lt;- append(results_list, list(new_result))\n}\n\n# Go Through Each Model and Store Result\nstore_results(1, \"Regression Adjustment\", dgp1_reg_adjusted, NA, 0)\nstore_results(2, \"Regression Adjustment\", dgp2_reg_adjusted, NA, 0)\nstore_results(3, \"Regression Adjustment\", dgp3_reg_adjusted, NA, 0)\nstore_results(4, \"Regression Adjustment\", dgp4_reg_adjusted, NA, 0)\n\nstore_results(1, \"Regression Adjustment\", dgp1_reg_confounded, NA, 1)\nstore_results(2, \"Regression Adjustment\", dgp2_reg_confounded, NA, 1)\nstore_results(3, \"Regression Adjustment\", dgp3_reg_confounded, NA, 1)\nstore_results(4, \"Regression Adjustment\", dgp4_reg_confounded, NA, 1)\n\nstore_results(1, \"Regression Adjustment\", dgp1_reg_adjusted_lag, NA, 0)\nstore_results(2, \"Regression Adjustment\", dgp2_reg_adjusted_lag, NA, 0)\nstore_results(3, \"Regression Adjustment\", dgp3_reg_adjusted_lag, NA, 0)\nstore_results(4, \"Regression Adjustment\", dgp4_reg_adjusted_lag, NA, 0)\n\nstore_results(1, \"Regression Adjustment\", dgp1_reg_confounded_lag, NA, 1)\nstore_results(2, \"Regression Adjustment\", dgp2_reg_confounded_lag, NA, 1)\nstore_results(3, \"Regression Adjustment\", dgp3_reg_confounded_lag, NA, 1)\nstore_results(4, \"Regression Adjustment\", dgp4_reg_confounded_lag, NA, 1)\n\nstore_results(1, \"ADL\", dgp1_adl_adjusted, NA, 0)\nstore_results(2, \"ADL\", dgp2_adl_adjusted, NA, 0)\nstore_results(3, \"ADL\", dgp3_adl_adjusted, NA, 0)\nstore_results(4, \"ADL\", dgp4_adl_adjusted, NA, 0)\n\nstore_results(1, \"ADL\", dgp1_adl_confounded, NA, 1)\nstore_results(2, \"ADL\", dgp2_adl_confounded, NA, 1)\nstore_results(3, \"ADL\", dgp3_adl_confounded, NA, 1)\nstore_results(4, \"ADL\", dgp4_adl_confounded, NA, 1)\n\nstore_results(1, \"IPW\", dgp1_ipw_notrunc, NA, 0)\nstore_results(2, \"IPW\", dgp2_ipw_notrunc, NA, 0)\nstore_results(3, \"IPW\", dgp3_ipw_notrunc, NA, 0)\nstore_results(4, \"IPW\", dgp4_ipw_notrunc, NA, 0)\n\nstore_results(1, \"IPW\", dgp1_ipw_95trunc, 0.05, 0)\nstore_results(2, \"IPW\", dgp2_ipw_95trunc, 0.05, 0)\nstore_results(3, \"IPW\", dgp3_ipw_95trunc, 0.05, 0)\nstore_results(4, \"IPW\", dgp4_ipw_95trunc, 0.05, 0)\n\nstore_results(1, \"IPW\", dgp1_ipw_99trunc, 0.01, 0)\nstore_results(2, \"IPW\", dgp2_ipw_99trunc, 0.01, 0)\nstore_results(3, \"IPW\", dgp3_ipw_99trunc, 0.01, 0)\nstore_results(4, \"IPW\", dgp4_ipw_99trunc, 0.01, 0)\n\n# Combine All Results\nall_results &lt;- do.call(rbind, results_list)\n\nAnd now, we can visualize how well these models performed. Note that I am creating two plots - one for each DGP with the first of these plots showing the performance of models for DGPs 1-2 with treatment-outcome feedback and 1 time-invariant confounder and another for DGPs 3-4 with treatment-confounder feedback.\n\n# Visualize Performance of Each Model\n# First Do Some Data Pre-Processing\nall_results_long &lt;- all_results %&gt;%\n  # Pivot This Data Set So That Estimate Type is a Column\n  pivot_longer(cols = c(x_estimate, x_lag_estimate), \n               names_to = \"estimate_type\", \n               values_to = \"estimate_value\") %&gt;%\n  mutate(\n    # If the Estimate Type is 'X_Lag_Estimate' and is NA, Exclude It\n    estimate_type = ifelse(is.na(estimate_value) & estimate_type == \"x_lag_estimate\", NA, estimate_type),\n    # Create a String of Text That IDs What the Model Is\n    model_type = paste0(\n      \"DGP \", dgp,\n      ifelse(!is.na(truncation), \n             paste0(\": \",\n                    ifelse(truncation == 0.05, \"5% Truncated\", \n                           ifelse(truncation == 0.01, \"1% Truncated\", \"No Truncation\"))), \n             \"\")\n    )\n  ) %&gt;%\n  # Remove Rows Where Estimate_Type is NA\n  drop_na(estimate_type)\n\n# Plot Results for DGP 1 and 2\nall_results_long %&gt;%\n  filter(dgp %in% c(1, 2)) %&gt;%\n  # Rename Regression Adjustment for Legend Plotting\n  mutate(method = ifelse(method == \"Regression Adjustment\", \"RA\", method),\n         # Re-Order for Plotting\n         method = factor(method, levels = c(\"RA\", \"ADL\", \"IPW\")),\n         # Re-Name for Plotting\n         confounded = ifelse(confounded == 1, \"Yes\", \"No\")) %&gt;%\n  # Order by Method and DGP So That Estimate from the Same DGP Are Plotted Together\n  arrange(method, dgp) %&gt;%\n  mutate(id = row_number()) %&gt;%\n  ggplot(aes(x = estimate_value, y = id, color = method, shape = factor(confounded))) +\n  # Plot True Effect Size Vertical Lines\n  geom_vline(data = all_results_long %&gt;% filter(estimate_type == \"x_estimate\"),\n             aes(xintercept = 0.4), linetype = \"dashed\", color = \"#a91e11\", size = 0.75) +\n  geom_vline(data = all_results_long %&gt;% filter(estimate_type == \"x_lag_estimate\"),\n             aes(xintercept = 0.16), linetype = \"dashed\", color = \"#a91e11\", size = 0.75) +\n  # Insert geom_point() After So That It Is Pushed Forward In Front of the Vertical Lines\n  geom_point(size = 2.5) +\n  geom_errorbar(aes(xmin = ifelse(estimate_type == \"x_estimate\", ci_lower, ci_lower_x_lag), \n                    xmax = ifelse(estimate_type == \"x_estimate\", ci_upper, ci_upper_x_lag)), \n                width = 0.75, size = 0.75) +\n  # Facet by X_Estimate or X_Lag_Estimate\n  facet_wrap(~estimate_type, scales = \"free_x\", labeller = labeller(\n    estimate_type = c(\n      x_estimate = \"Estimate of X at Time t\", \n      x_lag_estimate = \"Lagged Estimate of X at Time t-1\" \n    )\n  )) +\n  # Add a Label That Defines the Model Type\n  geom_label(aes(label = model_type), vjust = -0.55, hjust = 0.5, size = 3, \n             fill = \"white\", fontface = \"bold\", label.size = 0.25, \n             label.padding = unit(0.2, \"lines\"), show.legend = FALSE) + \n  labs(\n    title = \"\",\n    subtitle = \"Vertical dashed lines reflect the true effect size\",\n    x = \"\",\n    y = \"\"\n  ) +\n  blog_theme() +\n  theme(\n    legend.position = \"bottom\",\n    axis.text.y = element_blank(),  \n    axis.ticks.y = element_blank(),  \n    legend.text = element_text(face = \"bold\"),\n    plot.subtitle = element_text(face = \"bold\"),\n  ) +\n  scale_color_manual(\n    name = \"Adjustment Method\",\n    values = c(\n      \"RA\" = \"#003f5a\", \n      \"ADL\" = \"#fea02f\",                   \n      \"IPW\" = \"#007a7a\"                   \n    ),\n    guide = guide_legend(\n      title.position = \"top\",  \n      title.hjust = 0.5, \n      label.position = \"bottom\",  \n      override.aes = list(linetype = 0)\n    )\n  ) + \nscale_shape_manual(\n  name = \"Is Estimate Confounded?\",\n    values = c(\n      \"No\" = 16, \n      \"Yes\" = 17   \n    ),\n  guide = guide_legend(\n    title.position = \"top\",  \n    title.hjust = 0.5,\n    label.position = \"bottom\",   \n    override.aes = list(linetype = 0)\n  )) +  \n  # Reverse Y Scale So That ID Is Plotted in Descending Order\n  scale_y_reverse() \n\n\n\n\n\n\n\n\nOkay, neat! First, let’s break down what this plot is showing us and then we’ll substantively interpret it. The left-hand panel shows us \\(X_{t} \\rightarrow Y_{t}\\) effect estimates while the right-hand panel shows us \\(X_{t-1} \\rightarrow Y_{t}\\) effect estimates. Point estimates are colored by estimation technique. Dark blue reflects regression adjustment, yellow is ADL, and teal is IPW. Dots are estimates adjusted for confounding while triangles are estimates where the estimate is known to be confounded. On that note as well, please don’t pay too much attention to the confidence intervals! This exercise was more about point estimation and not about hypothesis testing so my standard errors are for sure wrong. Nonetheless, we actually know the true effect size, so just pay attention to the points/triangles. Lastly, the dashed vertical line is the true effect size.\nWe know that the true effect size for \\(X\\) at time \\(t\\) is 0.4 because I directly specified that when simulating data. How did I come up with 0.16 for the lagged effect size for the effect of \\(X_{t-1}\\)}? To do this, I need to identify all the paths that \\(X_{t-1}\\) impacts \\(Y_{t}\\). For DGPs 1 and 2, we have two paths:\n\\[\nX_{t-1} \\rightarrow Y_{t-1} \\rightarrow Y_{t}\n\\]\n\\[\nX_{t-1} \\rightarrow X_{t} \\rightarrow Y_{t}\n\\] We can flesh this out by attaching our known effect sizes at each point:\n\\[\nX_{t-1} (0.4) \\rightarrow Y_{t-1} (0.4) \\rightarrow Y_{t}\n\\]\n\\[\nX_{t-1} (0.4) \\rightarrow X_{t} (0.4) \\rightarrow Y_{t}\n\\] For each of these paths, we take the product of the effect sizes. In other words: \\((0.4 * 0.4) + (0.4 * 0.4)\\) which equals 0.32. So why is the effect size 0.16? I’m not sure! I’m trusting the published research of Thoemmes and Ong (2016), whose research is what DGPs 1-2 are based on. If we are interested in the effect of \\(X_{t-1}\\) through one path then 0.16 makes sense to me. (For example, if we cared about the effect that goes through \\(X_{t-1} (0.4) \\rightarrow X_{t} (0.4) \\rightarrow Y_{t}\\) only). If this was the case, then \\(0.4 * 0.4 = 0.16\\). That makes sense to me, but I don’t understand why the total estimate would not include the effect of \\(X_{t-1}\\) that goes through \\(X_{t-1} (0.4) \\rightarrow Y_{t-1} (0.4) \\rightarrow Y_{t}\\). Well, that’s something for future Brian to figure out, but let’s trust the experts and not the novice who is trying to figure this out!\nSo, what do we find? For the estimation of current effects, regression adjustment (by far the most popular strategy with both panel and cross-sectional data) performs very poorly! In fact, with more data (DGP 2), the estimates trend towards getting worse! Whether or not the inclusion of a lagged \\(X\\) value as a covariate makes the estimate better or worse is a bit unclear. For DGP 1, estimates without the lagged \\(X\\) are less biased but, for DGP 2, estimates without the lagged \\(X\\) are much more biased.\nThe ADL and IPW results perform pretty well for the estimation of the current effect of \\(X\\). As expected, however, the ADL does really bad for lagged effect estimation. Which is exactly what we expected since including a LDV as a covariate blocks a huge part of the lagged treatment effect. Lastly, IPW does pretty good here, although it is interesting that the inclusion of more data (DGP 2) seems to make the estimates more biased. Not sure how to explain that… and I won’t try given that I myself am still a bit unsure on what the total lagged effect is in this circumstance. Next, let’s evaluate model performance for DGPs 3 and 4:\n\n# Plot Results for DGP 3 and 4\nall_results_long %&gt;%\n  filter(dgp %in% c(3, 4)) %&gt;%\n  # Rename Regression Adjustment for Legend Plotting\n  mutate(method = ifelse(method == \"Regression Adjustment\", \"RA\", method),\n         # Re-Order for Plotting\n         method = factor(method, levels = c(\"RA\", \"ADL\", \"IPW\")),\n         # Re-Name for Plotting\n         confounded = ifelse(confounded == 1, \"Yes\", \"No\")) %&gt;%\n  # Order by Method and DGP So That Estimate from the Same DGP Are Plotted Together\n  arrange(method, dgp) %&gt;%\n  mutate(id = row_number()) %&gt;%\n  ggplot(aes(x = estimate_value, y = id, color = method, shape = factor(confounded))) +\n  # Plot True Effect Size Vertical Lines\n  geom_vline(data = all_results_long %&gt;% filter(estimate_type == \"x_estimate\"),\n             aes(xintercept = 0.4), linetype = \"dashed\", color = \"#a91e11\", size = 0.75) +\n  geom_vline(data = all_results_long %&gt;% filter(estimate_type == \"x_lag_estimate\"),\n             aes(xintercept = 0.256), linetype = \"dashed\", color = \"#a91e11\", size = 0.75) +\n  # Insert geom_point() After So That It Is Pushed Forward In Front of the Vertical Lines\n  geom_point(size = 2.5) +\n  geom_errorbar(aes(xmin = ifelse(estimate_type == \"x_estimate\", ci_lower, ci_lower_x_lag), \n                    xmax = ifelse(estimate_type == \"x_estimate\", ci_upper, ci_upper_x_lag)), \n                width = 0.75, size = 0.75) +\n  # Facet by X_Estimate or X_Lag_Estimate\n  facet_wrap(~estimate_type, scales = \"free_x\", labeller = labeller(\n    estimate_type = c(\n      x_estimate = \"Estimate of X at Time t\", \n      x_lag_estimate = \"Lagged Estimate of X at Time t-1\" \n    )\n  )) +\n  # Add a Label That Defines the Model Type\n  geom_label(aes(label = model_type), vjust = -0.55, hjust = 0.5, size = 3, \n             fill = \"white\", fontface = \"bold\", label.size = 0.25, \n             label.padding = unit(0.2, \"lines\"), show.legend = FALSE) + \n  labs(\n    title = \"\",\n    subtitle = \"Vertical dashed lines reflect the true effect size\",\n    x = \"\",\n    y = \"\"\n  ) +\n  blog_theme() +\n  theme(\n    legend.position = \"bottom\",\n    axis.text.y = element_blank(),  \n    axis.ticks.y = element_blank(),  \n    legend.text = element_text(face = \"bold\"),\n    plot.subtitle = element_text(face = \"bold\"),\n  ) +\n  scale_color_manual(\n    name = \"Adjustment Method\",\n    values = c(\n      \"RA\" = \"#003f5a\", \n      \"ADL\" = \"#fea02f\",                   \n      \"IPW\" = \"#007a7a\"                   \n    ),\n    guide = guide_legend(\n      title.position = \"top\",  \n      title.hjust = 0.5, \n      label.position = \"bottom\",  \n      override.aes = list(linetype = 0)\n    )\n  ) + \n  scale_shape_manual(\n    name = \"Is Estimate Confounded?\",\n    values = c(\n      \"No\" = 16, \n      \"Yes\" = 17   \n    ),\n    guide = guide_legend(\n      title.position = \"top\",  \n      title.hjust = 0.5,\n      label.position = \"bottom\",   \n      override.aes = list(linetype = 0)\n    )) +  \n  # Reverse Y Scale So That ID Is Plotted in Descending Order\n  scale_y_reverse()  \n\n\n\n\n\n\n\n\nHere, we see a perhaps better case for IPW than more traditional models. The RA estimates generally perform very poorly for estimating the current effect of \\(X\\). In contrast, as expected, both the ADL and IPW estimates perform well, but can the same be said for the estimation of the lagged effect?\nBefore evaluating, let me explain why the lagged treatment effect is 0.256. Again, to be able to identify the total lagged effect, we need to figure out all of the paths that link \\(X_{t-1}\\) to \\(Y_{t}\\). If you reference the DAG earlier in this blog, you should be able to identify three paths:\n\\[\nX_{t-1} (0.4) \\rightarrow Z_{t} (0.2) \\rightarrow Y_{t}\n\\]\n\\[\nX_{t-1} (0.4) \\rightarrow X_{t} (0.4) \\rightarrow Y_{t}\n\\]\n\\[\nX_{t-1} (0.4) \\rightarrow Z_{t} (0.1) \\rightarrow X_{t} (0.4) \\rightarrow Y_{t}\n\\] Within each path, we take the product, so:\n\\[\nX_{t-1} (0.4) * Z_{t} (0.2) = 0.08\n\\] \\[\nX_{t-1} (0.4) * X_{t} (0.4) = 0.16\n\\] \\[\nX_{t-1} (0.4) * Z_{t} (0.1) * X_{t} (0.4) = 0.016\n\\] If we do \\(0.08 + 0.16 + 0.016\\), we get 0.256, which is where I’m getting the lagged effect of \\(X\\) from. With that explained, how do the models perform with estimating the lagged effect? Surprisingly, RA isn’t terrible and it’s actually slightly better when it’s confounded. This actually makes sense for the lagged effect. Remember, part of the effect of the lagged effect goes through \\(Z_{t}\\). If we control for that confounder, we remove part of the lagged effect. If we don’t control for this confounder, we do not block that part of the lagged effect. Although, we obviously bias the current effect in the process. As expected, the ADL does not do well here. Lastly, IPW looks really solid here, for both the current and lagged effect.\n\n\nConclusion\nOkay, so that was a lot, but I really wanted to drive two key points home. First, panel data is complicated and when we use models to estimate dynamic effects, knowing how to simulate complex panel data is a very valuable tool to know. Second, regression adjustment probably won’t cut it! If you know or suspect time-varying confounding, treatment-outcome feedback loops, etc., you also know beforehand that your statistical estimates could be very, very wrong. Marginal structural models with inverse probability weights can be one solution to the problem posed by complex panel data (although it is not the only solution). However, MSMs with IPW are pretty popular and are probably increasing in popularity. As a result, I plan on making a blog post just dedicated to this methodology, exploring all of the ins-and-outs and simulating data a bit more similar to real-life panel data (i.e. larger \\(T\\), a binary treatment, a lot of diverse confounders, etc.) Thanks for reading!"
  },
  {
    "objectID": "blog/index.html#section",
    "href": "blog/index.html#section",
    "title": "Blog",
    "section": "2025",
    "text": "2025\n\n\n\n\n\n\n\n\n\n\nDo NFL Referees Favor the Kansas City Chiefs?\n\n\n\n\n\n\nnfl\n\n\nvisualization\n\n\n\nThe back-to-back NFL champions are poised for a third consecutive Super Bowl win (three-peat), which has never been accomplished before. Is such a feat possible due to unfair officiating? Let’s look at the data!\n\n\n\n\n\nJanuary 31, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-1",
    "href": "blog/index.html#section-1",
    "title": "Blog",
    "section": "2024",
    "text": "2024\n\n\n\n\n\n\n\n\n\n\nSimulating Complex Panel Data to Validate Model Performance in Estimating Dynamic Treatment Effects\n\n\n\n\n\n\ncausal inference\n\n\nsimulation\n\n\ndags\n\n\nipw\n\n\nmsm\n\n\npanel data\n\n\n\nMaking causal inferences is hard, and making causal inferences is harder with complex panel data. In this blog post, learn how to test the validity of your panel data models using simulation!\n\n\n\n\n\nDecember 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCausal Inference for Casuals\n\n\n\n\n\n\ncausal inference\n\n\nsimulation\n\n\ndags\n\n\nregression\n\n\n\nLearn about the core concepts of causal inference and the motivations for causal analysis with the help of simulation. Beginners to causal inference welcome!\n\n\n\n\n\nOctober 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting the Outcome of NFL Games in the 2024-2025 Season\n\n\n\n\n\n\nmachine learning\n\n\nnfl\n\n\n\nFollow my attempt to predict the winners and losers of each game in the 2024-2025 NFL season.\n\n\n\n\n\nSeptember 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAn Introduction to Dynamic Causal Inference\n\n\n\n\n\n\ncausal inference\n\n\npanel data\n\n\ndags\n\n\n\nLearn the basics to making causal inferences with panel/longitudinal data.\n\n\n\n\n\nJuly 8, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Research",
    "section": "",
    "text": "My personal research covers the intersection of causal inference, quasi-experimental design, and policy evaluation and a variety of topics that I find interesting, including (but not limited to): peace and conflict, democracy and elections, political and economic development, and the NFL (professional American football)."
  },
  {
    "objectID": "research/index.html#dissertation",
    "href": "research/index.html#dissertation",
    "title": "Research",
    "section": "Dissertation",
    "text": "Dissertation\n\n“Rethinking the Study of Conflict and Peace: Making Causal Inferences in Quantitative Conflict and Peace Research” \n\nManuscript \nCode (Chapter 2) \nCode (Chapter 3)"
  }
]