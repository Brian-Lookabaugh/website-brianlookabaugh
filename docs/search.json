[
  {
    "objectID": "resume/index.html",
    "href": "resume/index.html",
    "title": "Brian Lookabaugh",
    "section": "",
    "text": "Download"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Brian Lookabaugh",
    "section": "",
    "text": "Greetings and welcome to my personal website! My name is Brian Lookabaugh and I am a Research Analyst at Fors Marsh. In my position, I leverage my skills in data analysis and statistical modeling to solve a variety of business problems. These include descriptive tasks (creating visualizations to communicate trends and patterns), predictive problems (utilizing machine learning to predict outcomes and forecast), and causal questions (drawing on my background in causal inference to answer questions such as “did X have an impact on Y?”).\n\nOutside of my professional role, I routinely refine and expand my methodological toolkit by researching new methods and applying my skill set to other recreational interests of mine. You can find some of these passion projects under the “Blog” section of this site. If you have any questions about my research, please feel free to reach out to me!"
  },
  {
    "objectID": "blog/2024/dynamic-causal-inference/index.html",
    "href": "blog/2024/dynamic-causal-inference/index.html",
    "title": "An Introduction to Dynamic Causal Inference",
    "section": "",
    "text": "Code\n# Load Libraries\npacman::p_load(\n  \"dplyr\", # Data Manipulation\n  \"ggplot2\", # Data Visualization\n  \"ggtext\", # Labels\n  \"dagitty\", # Creating DAGs\n  \"ggdag\", # Plotting DAGs\n  install = FALSE\n)\n\n# Define a Custom Theme - Taken From Andrew Heiss's Blogs\nblog_theme &lt;- function() {\n  theme_bw() +  # Start with theme_bw\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      plot.title = element_text(face = \"bold\"),\n      axis.title = element_text(face = \"bold\"),\n      strip.text = element_text(face = \"bold\"),\n      strip.background = element_rect(fill = \"grey80\", color = NA),\n      legend.title = element_text(face = \"bold\")\n    )\n}"
  },
  {
    "objectID": "blog/2024/dynamic-causal-inference/index.html#footnotes",
    "href": "blog/2024/dynamic-causal-inference/index.html#footnotes",
    "title": "An Introduction to Dynamic Causal Inference",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote: if you are already familiar with the core concepts of causal inference, feel free to skip down to the section where I start discussing time and why it messes things up.↩︎\nWell… kind of. There are actually a lot of other assumptions that you would need to check for first, like positivity, SUTVA, no measurement error, etc. (which will not be covered here in sufficient detail if you are not familiar with these topics). If you’re not super familiar with these topics, I highly recommend checking out Chatton and Rohrer 2024.↩︎\nI do not condone this attitude… never treat your regressions or research designs this way.↩︎\nHere are some good starting materials for DAGs, simulation, and sensitivity analyses: Rohrer 2018, Blair et al. 2023, Cinelli and Hazlett 2020.↩︎"
  },
  {
    "objectID": "blog/index.html#section",
    "href": "blog/index.html#section",
    "title": "Blog",
    "section": "2024",
    "text": "2024\n\n\n\n\n\n\n\n\n\n\nPredicting the Outcome of NFL Games in the 2024-2025 Season\n\n\n\n\n\n\nmachine learning\n\n\nnfl\n\n\n\nFollow my attempt to predict the winners and losers of each game in the 2024-2025 NFL season.\n\n\n\n\n\nSeptember 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAn Introduction to Dynamic Causal Inference\n\n\n\n\n\n\ncausal inference\n\n\npanel data\n\n\ndags\n\n\n\nLearn the basics to making causal inferences with panel/longitudinal data.\n\n\n\n\n\nJuly 8, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Research",
    "section": "",
    "text": "My personal research covers the intersection of causal inference, quasi-experimental design, and policy evaluation and a variety of topics that I find interesting, including (but not limited to): peace and conflict, democracy and elections, political and economic development, and the NFL (professional American football)."
  },
  {
    "objectID": "research/index.html#dissertation",
    "href": "research/index.html#dissertation",
    "title": "Research",
    "section": "Dissertation",
    "text": "Dissertation\n\n“Rethinking the Study of Conflict and Peace: Making Causal Inferences in Quantitative Conflict and Peace Research” \n\nManuscript \nCode (Chapter 2) \nCode (Chapter 3)"
  },
  {
    "objectID": "blog/2024/nfl-workflow/index.html",
    "href": "blog/2024/nfl-workflow/index.html",
    "title": "Predicting the Outcome of NFL Games in the 2024-2025 Season",
    "section": "",
    "text": "Code\n# Load Libraries\npacman::p_load(\n  \"dplyr\", # Data Manipulation\n  \"nflverse\", # NFL Verse Environment\n  \"gt\", # Nice Tables\n  \"tidyr\", # Reshaping Data\n  \"stringr\", # Working with Strings\n  \"caret\", # Machine Learning\n  \"scales\", # Percent Formatting\n  \"readxl\", # Reading Excel Files\n  \"writexl\", # Writing Excel Files\n  install = FALSE\n)\n\n# Define a Custom Theme - Taken From Andrew Heiss's Blogs\nblog_theme &lt;- function() {\n  theme_bw() +  # Start with theme_bw\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      plot.title = element_text(face = \"bold\"),\n      axis.title = element_text(face = \"bold\"),\n      strip.text = element_text(face = \"bold\"),\n      strip.background = element_rect(fill = \"grey80\", color = NA),\n      legend.title = element_text(face = \"bold\")\n    )\n}\n\n\nI am a massive fan of NFL football. I look forward to the inaugural start of the regular season every September and it feels all too soon when the season ends when the Super Bowl is played in February. As much as I love the game-play, the sports shows spinning their narratives, and the social aspect of NFL Sundays, I have been looking for excuses to get my hands on NFL data and having fun with an additional aspect of the game.\nRecently, in pursuit of this goal, I went to the Playoff Predictors website, where you can go game-by-game and pick who you think will win each game. It’s a fun exercise that I look forward to every year when the NFL schedule is released and it gives me a picture of what I intuitively think the standings might look like at the conclusion of the upcoming season. Once I got these standings, I played around with the {nflreadr} and {gt} packages to present my predicted standings in a more aesthetically pleasing way.\n\n\nCode\n# Create My Vibes Tribble - Adding Extra Spacing\nvibes &lt;- tribble(\n  ~east, ~record_1, ~space_1, ~north, ~record_2, ~space_2, ~south, ~record_3, ~space_3, ~west, ~record_4, ~conf,\n  \"BUF\", \"10-7\", \" \", \"BAL\", \"13-4\", \" \", \"IND\", \"13-4\", \" \", \"KC\", \"12-5\", \"AFC\",\n  \"NYJ\", \"9-8\", \" \", \"CIN\", \"10-7\", \" \", \"HOU\", \"11-6\", \" \", \"LAC\", \"11-6\", \"AFC\",\n  \"MIA\", \"7-10\", \" \", \"PIT\", \"8-9\", \" \",\"TEN\", \"9-8\", \" \", \"DEN\", \"7-10\", \"AFC\",\n  \"NE\", \"3-14\", \" \", \"CLE\", \"8-9\", \" \", \"JAX\", \"9-8\", \" \", \"LV\", \"6-11\", \"AFC\",\n  \"PHI\", \"9-8\", \" \", \"GB\", \"12-5\", \" \", \"ATL\", \"9-8\", \" \", \"LAR\", \"11-6\", \"NFC\",\n  \"WSH\", \"8-9\", \" \", \"DET\", \"11-6\", \" \", \"TB\", \"7-10\", \" \", \"SF\", \"10-7\", \"NFC\",\n  \"DAL\", \"7-10\", \" \", \"CHI\", \"9-8\", \" \", \"CAR\", \"5-12\", \" \", \"ARZ\", \"9-8\", \"NFC\",\n  \"NYG\", \"5-12\", \" \", \"MIN\", \"6-11\", \" \", \"NO\", \"4-13\", \" \", \"SEA\", \"4-13\", \"NFC\"\n)\n\nvibes %&gt;%\n  # Group By Conference\n  gt(groupname_col = \"conf\") %&gt;%\n  # Create Columns Labels\n  cols_label(\n    east = \"\",\n    record_1 = \"East\",\n    space_1 = \"\",\n    north = \"\",\n    record_2 = \"North\",\n    space_2 = \"\",\n    south = \"\",\n    record_3 = \"South\",\n    space_3 = \"\",\n    west = \"\",\n    record_4 = \"West\"\n  ) %&gt;%\n  # Align Column Title Text\n  tab_style(style = cell_text(align = \"center\"), locations = cells_column_labels()) %&gt;%\n  # Align Body Text\n  tab_style(style = cell_text(align = \"center\"), locations = cells_body()) %&gt;%\n  # Distinguish Division Rows\n  tab_style(\n    style = list(\n      cell_fill(color = \"#bcc0be\")),\n    locations = cells_body(rows = which(vibes$east %in% c(\"AFC\", \"NFC\")))) %&gt;%\n  # Add Team Logos\n  nflplotR::gt_nfl_logos(columns = c(\"east\", \"north\", \"south\", \"west\"))\n\n\n\n\n\n\n\n\n\nEast\n\n\nNorth\n\n\nSouth\n\n\nWest\n\n\n\n\nAFC\n\n\n\n10-7\n\n\n13-4\n\n\n13-4\n\n\n12-5\n\n\n\n9-8\n\n\n10-7\n\n\n11-6\n\n\n11-6\n\n\n\n7-10\n\n\n8-9\n\n\n9-8\n\n\n7-10\n\n\n\n3-14\n\n\n8-9\n\n\n9-8\n\n\n6-11\n\n\nNFC\n\n\n\n9-8\n\n\n12-5\n\n\n9-8\n\n\n11-6\n\n\n\n8-9\n\n\n11-6\n\n\n7-10\n\n\n10-7\n\n\n\n7-10\n\n\n9-8\n\n\n5-12\n\n\n9-8\n\n\n\n5-12\n\n\n6-11\n\n\n4-13\n\n\n4-13\n\n\n\n\n\n\n\nI like to refer to these as my “vibes-based” predictions because that’s really all they are. However, as a trained social scientist, I am well aware that “vibes” are not wholly informative, well-defined, nor do they contain a great deal of explanatory or predictive power. So, I thought, why not get my hands on more NFL data and try to work up a machine learning based approach? And that is what this blog is for.\n\nData/Feature Collection\nPrior to any fancy modeling, I need to collect some data to predict who wins each game. I want to start off with a major caveat here. I am doing this for fun and educational purposes. Undoubtedly, the predictors I have selected are not reflective of the most advanced analytics nor are they comprehensive. I chose the “lowest hanging fruit” for ease of access. This is probably going to hurt the predictive power of the models (models predict better with more predictive data), but again, humor me!\nOverall, I am using the following variables as predictors: whether a team is playing at home, QBR (quarterback rating), passing EPA (expected points added), rushing EPA, receiving EPA, forced fumbles, sacks, interceptions, and passes broken up. Because each prediction is at the game-level, I am using a differenced variable for computational ease (i.e., rather than include the home team’s passing EPA and the away team’s passing EPA in the same model, I just create a difference between the two and use this difference as a predictor for each team). Regardless of which method is used, the predictive performance remained the same after testing.\nThis selection leaves a lot to be desired. What about more advanced metrics like ELO? What about schematic data (like what type of offense the home team runs v. what type of defense the away team runs, etc.)? What about circumstantial data like whether a key player is out? These are all great things to add that will need to be included in the future! If you’re curious about the data collection syntax, check out the code fold below!\n\n\nCode\n# Load and Clean the QBR Data\nqbr &lt;- load_espn_qbr(\n  # Select the 2006-2023 Seasons as Training Data\n  seasons = 2006:2023,\n  # Aggregate at the Week-Level\n  summary_type = c(\"week\")) %&gt;%\n  # Exclude Playoff Games\n  filter(season_type == \"Regular\") %&gt;%\n  # Select Relevant Columns\n  select(c(team_abb, season, game_week, qbr_total, pts_added)) %&gt;%\n  # Create Cumulative Averages\n  group_by(season, team_abb) %&gt;%\n  mutate(\n    moving_qbr_mean = cumsum(qbr_total) / game_week,\n    moving_pts_added = cumsum(pts_added / game_week),\n    # Rename Washington for Merging\n    team_abb = ifelse(team_abb == \"WSH\", \"WAS\", team_abb))\n\n# Load and Clean Offensive Stats Data\noffensive &lt;- load_player_stats(\n  # Select the 2006-2023 Seasons as Training Data\n  seasons = 2006:2023,\n  # Filter to Offense\n  stat_type = \"offense\") %&gt;%\n  # Exclude the Playoffs\n  filter(season_type == \"REG\") %&gt;%\n  # Create Team-Level Stats\n  group_by(season, recent_team, week) %&gt;%\n  summarise(\n    passing_epa = sum(passing_epa, na.rm = TRUE),\n    rushing_epa = sum(rushing_epa, na.rm = TRUE),\n    receiving_epa = sum(receiving_epa, na.rm = TRUE)\n  ) %&gt;%\n  ungroup() %&gt;%\n  # Create Cumulative Averages\n  group_by(season, recent_team) %&gt;%\n  mutate(\n    moving_passing_epa = cumsum(passing_epa) / week,\n    moving_rushing_epa = cumsum(rushing_epa) / week,\n    moving_receiving_epa = cumsum(receiving_epa) / week) %&gt;%\n  # Keep Relevant Columns\n  select(season, recent_team, week, passing_epa, rushing_epa, receiving_epa, \n         moving_passing_epa, moving_rushing_epa, moving_receiving_epa) %&gt;%\n  # Convert Team Abbreviations to a More Standard Form for Merging\n  mutate(recent_team = ifelse(recent_team == \"LA\", \"LAR\", recent_team))\n\n# Load and Clean Defensive Stats Data\ndefensive &lt;- load_player_stats(\n  # Select the 2006-2023 Seasons as Training Data\n  seasons = 2006:2023,\n  # Filter to Defense\n  stat_type = \"defense\") %&gt;%\n  # Exclude Playoff Games\n  filter(season_type == \"REG\") %&gt;%\n  # Create Team-Level Stats\n  group_by(season, team, week) %&gt;%\n  summarise(\n    tackles = sum(def_tackles, na.rm = TRUE),\n    forced_fumbles = sum(def_fumbles_forced, na.rm = TRUE),\n    sacks = sum(def_sacks, na.rm = TRUE),\n    ints = sum(def_interceptions, na.rm = TRUE),\n    pass_broken = sum( def_pass_defended, na.rm = TRUE)\n  ) %&gt;%\n  ungroup() %&gt;%\n  # Create Cumulative Averages\n  group_by(season, team) %&gt;%\n  mutate(\n    moving_tackles = cumsum(tackles) / week,\n    moving_forced_fumbles = cumsum(forced_fumbles) / week,\n    moving_sacks = cumsum(sacks) / week,\n    moving_ints = cumsum(ints) / week,\n    moving_pass_broken = cumsum(pass_broken) / week) %&gt;%\n  # Keep Relevant Columns\n  select(season, team, week, tackles, forced_fumbles, sacks, ints, pass_broken, moving_tackles, \n         moving_forced_fumbles, moving_sacks, moving_ints, moving_pass_broken) %&gt;%\n  # Convert Team Abbreviations to a More Standard Form for Merging\n  mutate(team = ifelse(team == \"LA\", \"LAR\", team))\n\n# Load and Clean Schedules Data\nseasons &lt;- load_schedules(seasons = 2006:2023)\n\n# Convert the Data From Dyadic to Monadic\nseasons &lt;- clean_homeaway(seasons) %&gt;%\n  # Exclude Playoff Games\n  filter(game_type == \"REG\") %&gt;%\n  # Create a Home Team Variable\n  mutate(home = ifelse(location == \"home\", 1, 0),\n         # Create a Win Variable\n         win = ifelse(team_score &gt; opponent_score, 1, 0)) %&gt;%\n  # Keep Relevant Columns\n  select(game_id, season, week, team, opponent, home, win) %&gt;%\n   # Convert Team Abbreviations to a More Standard Form for Merging\n  mutate(team = ifelse(team == \"LA\", \"LAR\", team),\n         opponent = ifelse(opponent == \"LA\", \"LAR\", opponent))\n\n# Merge This Data\nmerged &lt;- inner_join(seasons, qbr, by = c(\"season\", \"team\" = \"team_abb\", \"week\" = \"game_week\")) %&gt;%\n  inner_join(offensive, by = c(\"season\", \"team\" = \"recent_team\", \"week\")) %&gt;%\n  inner_join(defensive, by = c(\"season\", \"team\", \"week\"))\n\nmerged &lt;- merged %&gt;%\n  group_by(game_id) %&gt;%\n  # Create Opponent Columns\n  # This Work Because Each Team Opponent Is In a Paired Set of Rows\n  # The Opponent Is Always the Second Observation\n  # Basically, This Just Reverses Cumulative Stats For Each Team Under a Different Name\n  mutate(opp_qbr = lead(moving_qbr_mean),\n         opp_qbr = ifelse(is.na(opp_qbr), lag(moving_qbr_mean), opp_qbr),\n         opp_pass_epa = lead(moving_passing_epa),\n         opp_pass_epa = ifelse(is.na(opp_pass_epa), \n                               lag(moving_passing_epa), opp_pass_epa),\n         opp_rushing_epa = lead(moving_rushing_epa),\n         opp_rushing_epa = ifelse(is.na(opp_rushing_epa), \n                                  lag(moving_rushing_epa), opp_rushing_epa),\n         opp_receiving_epa = lead(moving_receiving_epa),\n         opp_receiving_epa = ifelse(is.na(opp_receiving_epa), \n                                    lag(moving_receiving_epa), opp_receiving_epa),\n         opp_tackles = lead(moving_tackles),\n         opp_tackles = ifelse(is.na(opp_tackles), \n                              lag(moving_tackles), opp_tackles),\n         opp_forced_fumbles = lead(moving_forced_fumbles),\n         opp_forced_fumbles = ifelse(is.na(opp_forced_fumbles), \n                                     lag(moving_forced_fumbles), opp_forced_fumbles),\n         opp_sacks = lead(moving_sacks),\n         opp_sacks = ifelse(is.na(opp_sacks), \n                            lag(moving_sacks), opp_sacks),\n         opp_ints = lead(moving_ints),\n         opp_ints = ifelse(is.na(opp_ints), \n                           lag(moving_ints), opp_ints),\n         opp_pass_broken = lead(moving_pass_broken),\n         opp_pass_broken = ifelse(is.na(opp_pass_broken), \n                                  lag(moving_pass_broken), opp_pass_broken)\n         ) %&gt;%\n  # Create Differenced Columns\n  mutate(\n    qbr_diff = moving_qbr_mean - opp_qbr,\n    pass_epa_diff = moving_passing_epa - opp_pass_epa,\n    rushing_epa_diff = moving_rushing_epa - opp_rushing_epa,\n    receiving_epa_diff = moving_receiving_epa - opp_receiving_epa,\n    tackles_diff = moving_tackles - opp_tackles,\n    forced_fumbles_diff = moving_forced_fumbles - opp_forced_fumbles,\n    sacks_diff = moving_sacks - opp_sacks,\n    ints_diff = moving_ints - opp_ints,\n    pass_broken_diff = moving_pass_broken - opp_pass_broken\n  ) %&gt;%\n  # Make the Outcome Column Suitable for Classification\n  mutate(win = factor(win, levels = c(0, 1), labels = c(\"Lose\", \"Win\"))) %&gt;%\n  # Drop NAs Because They Will Create Problems\n  drop_na()\n\n\n\n\nMachine Learning Algorithms Limitations\nOkay, now onto the actual machine learning algorithms that will be used. Again, nothing super fancy here. In the interest of keeping things simple at first, I chose to just explore how predictive accuracy fluctuates between four popular ML algorithms (logistic regression… which makes me cringe to refer to it as “ML”, random forest, support vector machine (SVM), and XGBoost). For those curious, I did engage in hyper-parameter tuning, but, no amount of tuning really improved the model results that much, and I felt that, in the interest of simplicity and computational time, it would be best to just include four basic ML algorithms for now.\n\n# For Reproducibility\nset.seed(1234)\n\n# Establish a Cross-Validation Method\ncv_method &lt;- trainControl(method = \"cv\",\n                          number = 10,\n                          classProbs = TRUE,\n                          summaryFunction = twoClassSummary)\n\n# Fit Models\n# Logistic Regression\nlog_fit &lt;- train(win ~ home + qbr_diff + pass_epa_diff + rushing_epa_diff + receiving_epa_diff +\n                       forced_fumbles_diff + sacks_diff + ints_diff + pass_broken_diff, \n                  data = merged,\n                  method = \"glm\",\n                  family = \"binomial\",\n                  trControl = cv_method,\n                  metric = \"ROC\")\n\n# Save Model Results So I Don't Have to Re-Train Every Time\nsaveRDS(log_fit, \"data-and-analysis/log_fit_model.rds\")\n\n# Random Forest\nrf_fit &lt;- train(win ~ home + qbr_diff + pass_epa_diff + rushing_epa_diff + receiving_epa_diff +\n                       forced_fumbles_diff + sacks_diff + ints_diff + pass_broken_diff, \n                  data = merged,\n                  method = \"rf\",\n                  trControl = cv_method,\n                  metric = \"ROC\")\n\nsaveRDS(rf_fit, \"data-and-analysis/rf_fit_model.rds\")\n\n# Support Vector Machine\nsv_fit &lt;- train(win ~ home + qbr_diff + pass_epa_diff + rushing_epa_diff + receiving_epa_diff +\n                       forced_fumbles_diff + sacks_diff + ints_diff + pass_broken_diff, \n                data = merged,\n                method = \"svmLinear\",\n                trControl = cv_method,\n                metric = \"ROC\")\n\nsaveRDS(sv_fit, \"data-and-analysis/sv_fit_model.rds\")\n\n# XGBoost\nxgb_fit &lt;- train(win ~ qbr_diff + pass_epa_diff + rushing_epa_diff + receiving_epa_diff +\n                        forced_fumbles_diff + sacks_diff + ints_diff + pass_broken_diff, \n                 data = merged,\n                 method = \"xgbTree\",\n                 trControl = cv_method,\n                 metric = \"ROC\")\n\nsaveRDS(xgb_fit, \"data-and-analysis/xgb_fit_model.rds\")\n\n# Store the Predictive Accuracy Results in a Table\nresults &lt;- tibble(\n  Model = c(\"Logistic Regression\", \"Random Forest\", \"SVM\", \"XGBoost\"),\n  # Store ROC Metrics\n  ROC = c(\n    # which.max() Doesn't Do Anything Here, But It Would If I Had Tons of Different\n    # Models for Each Model Type. It Would Select the Model with the Highest Predictive\n    # Power. Not Helpful Here Since I Am Only Running One Model of Each Type, But It's\n    # A Useful Reference That I Want to Keep for the Future\n    log_fit$results[which.max(log_fit$results$ROC), \"ROC\"], \n    rf_fit$results[which.max(rf_fit$results$ROC), \"ROC\"], \n    sv_fit$results[which.max(sv_fit$results$ROC), \"ROC\"], \n    xgb_fit$results[which.max(xgb_fit$results$ROC), \"ROC\"]\n  ),\n  # Store Accurate Predictions Percentage\n  Accuracy = c(\n    (log_fit$results$Spec[which.max(log_fit$results$ROC)] + \n       log_fit$results$Sens[which.max(log_fit$results$ROC)]) / 2,\n    (rf_fit$results$Spec[which.max(rf_fit$results$ROC)] + \n       rf_fit$results$Sens[which.max(rf_fit$results$ROC)]) / 2,\n    (sv_fit$results$Spec[which.max(sv_fit$results$ROC)] + \n       sv_fit$results$Sens[which.max(sv_fit$results$ROC)]) / 2,\n    (xgb_fit$results$Spec[which.max(xgb_fit$results$ROC)] + \n       xgb_fit$results$Sens[which.max(xgb_fit$results$ROC)]) / 2\n  )\n)\n\n\n\nModel Evaluation\nSo, how did these models fair? Eh… not great, as you can see below\n\nresults    \n\n# A tibble: 4 × 3\n  Model                 ROC Accuracy\n  &lt;chr&gt;               &lt;dbl&gt;    &lt;dbl&gt;\n1 Logistic Regression 0.798    0.718\n2 Random Forest       0.785    0.715\n3 SVM                 0.798    0.717\n4 XGBoost             0.792    0.720\n\n\n70-ish% isn’t terrible. It’s better than a coin flip. But really, how impressive is that? Just off of vibes, anyone who sort of knows the NFL will probably get 70% of game predictions right. Honestly, you might even do better if you just follow Vegas and predict the winner based on who is the betting favorite to win. That’s not very satisfying is it? A truly impressive ML algorithm should be able to predict not only when a favorite wins but also when a favorite does not win. These very crude models don’t appear to have that predictive complexity. Why is that the case? I can think of three reasons.\nFirst, as already stated, better predictors would go a long way. The good news is that this is probably the easiest fix. I just need to put the time in to research and collect the data.\nSecond, there may have been more complex hyper-parameter tuning I could have engaged with. Given that I come from a causal inference background, machine learning is not my specialty, and I do not have a wealth of information lodged in my head about all the tuning options for each ML algorithm. However, I’m sure that predictive gains could be there with some hyper-parameter tuning.\nLastly, I think that a different modeling approach could go a long way. And, to demonstrate my reasoning, let’s look at how my trained models are predicting the outcomes of the upcoming Week 2 games.\n\n\nCode\n# To Do This, I Need to Load In 2024 \"Test\" Data That the Model Was Not Trained On\n# This Is Just a Repeat of the Prior Data Cleaning Process for the Training Data\n# So I Don't Annotate Code Here\nqbr_2024 &lt;- load_espn_qbr(\n  seasons = 2024,\n  summary_type = c(\"week\")) %&gt;%\n  filter(season_type == \"Regular\") %&gt;%\n  select(c(team_abb, season, game_week, qbr_total, pts_added)) %&gt;%\n  group_by(season, team_abb) %&gt;%\n  mutate(\n    moving_qbr_mean = cumsum(qbr_total) / game_week,\n    moving_pts_added = cumsum(pts_added / game_week),\n    # Rename Washington for Merging\n    team_abb = ifelse(team_abb == \"WSH\", \"WAS\", team_abb)) %&gt;%\n  # Keep Last Week's Data\n  filter(game_week == 1) %&gt;%\n  # Convert Lagged Game Week to Current Since We're Using Last Week's Predictors\n  mutate(game_week = 2)\n\noffensive_2024 &lt;- load_player_stats(\n  seasons = 2024,\n  stat_type = \"offense\") %&gt;%\n  filter(season_type == \"REG\") %&gt;%\n  group_by(season, recent_team, week) %&gt;%\n  summarise(\n    passing_epa = sum(passing_epa, na.rm = TRUE),\n    rushing_epa = sum(rushing_epa, na.rm = TRUE),\n    receiving_epa = sum(receiving_epa, na.rm = TRUE)\n  ) %&gt;%\n  ungroup() %&gt;%\n  group_by(season, recent_team) %&gt;%\n  mutate(\n    moving_passing_epa = cumsum(passing_epa) / week,\n    moving_rushing_epa = cumsum(rushing_epa) / week,\n    moving_receiving_epa = cumsum(receiving_epa) / week) %&gt;%\n  select(season, recent_team, week, passing_epa, rushing_epa, receiving_epa, moving_passing_epa,\n         moving_rushing_epa, moving_receiving_epa) %&gt;%\n  filter(week == 1) %&gt;%\n  mutate(week = 2) %&gt;%\n  mutate(recent_team = ifelse(recent_team == \"LA\", \"LAR\", recent_team))\n\ndefensive_2024 &lt;- load_player_stats(\n  seasons = 2024,\n  stat_type = \"defense\") %&gt;%\n  filter(season_type == \"REG\") %&gt;%\n  group_by(season, team, week) %&gt;%\n  summarise(\n    tackles = sum(def_tackles, na.rm = TRUE),\n    forced_fumbles = sum(def_fumbles_forced, na.rm = TRUE),\n    sacks = sum(def_sacks, na.rm = TRUE),\n    ints = sum(def_interceptions, na.rm = TRUE),\n    pass_broken = sum( def_pass_defended, na.rm = TRUE)\n  ) %&gt;%\n  ungroup() %&gt;%\n  group_by(season, team) %&gt;%\n  mutate(\n    moving_tackles = cumsum(tackles) / week,\n    moving_forced_fumbles = cumsum(forced_fumbles) / week,\n    moving_sacks = cumsum(sacks) / week,\n    moving_ints = cumsum(ints) / week,\n    moving_pass_broken = cumsum(pass_broken) / week) %&gt;%\n  select(season, team, week, tackles, forced_fumbles, sacks, ints, pass_broken, moving_tackles, \n         moving_forced_fumbles, moving_sacks, moving_ints, moving_pass_broken) %&gt;%\n  filter(week == 1) %&gt;%\n  mutate(week = 2) %&gt;%\n  mutate(team = ifelse(team == \"LA\", \"LAR\", team))\n\nseason_2024 &lt;- load_schedules(seasons = 2024)\n\n# Convert the Data From Dyadic to Monadic\nseason_2024 &lt;- clean_homeaway(season_2024) %&gt;%\n  filter(game_type == \"REG\") %&gt;%\n  mutate(home = ifelse(location == \"home\", 1, 0),\n         win = ifelse(team_score &gt; opponent_score, 1, 0)) %&gt;%\n  select(game_id, season, week, team, opponent, home, win) %&gt;%\n  filter(week == 2) %&gt;%\n  mutate(team = ifelse(team == \"LA\", \"LAR\", team),\n         opponent = ifelse(opponent == \"LA\", \"LAR\", opponent))\n\nmerged_2024 &lt;- inner_join(season_2024, qbr_2024, \n                          by = c(\"team\" = \"team_abb\", \"week\" = \"game_week\", \"season\")) %&gt;%\n  inner_join(offensive_2024, by = c(\"team\" = \"recent_team\", \"week\", \"season\")) %&gt;%\n  inner_join(defensive_2024, by = c(\"team\", \"week\", \"season\")) %&gt;%\n  group_by(game_id) %&gt;%\n  mutate(opp_qbr = lead(moving_qbr_mean),\n         opp_qbr = ifelse(is.na(opp_qbr), \n                          lag(moving_qbr_mean), opp_qbr),\n         opp_pass_epa = lead(moving_passing_epa),\n         opp_pass_epa = ifelse(is.na(opp_pass_epa), \n                               lag(moving_passing_epa), opp_pass_epa),\n         opp_rushing_epa = lead(moving_rushing_epa),\n         opp_rushing_epa = ifelse(is.na(opp_rushing_epa), \n                                  lag(moving_rushing_epa), opp_rushing_epa),\n         opp_receiving_epa = lead(moving_receiving_epa),\n         opp_receiving_epa = ifelse(is.na(opp_receiving_epa), \n                                    lag(moving_receiving_epa), opp_receiving_epa),\n         opp_tackles = lead(moving_tackles),\n         opp_tackles = ifelse(is.na(opp_tackles), \n                              lag(moving_tackles), opp_tackles),\n         opp_forced_fumbles = lead(moving_forced_fumbles),\n         opp_forced_fumbles = ifelse(is.na(opp_forced_fumbles), \n                                     lag(moving_forced_fumbles), opp_forced_fumbles),\n         opp_sacks = lead(moving_sacks),\n         opp_sacks = ifelse(is.na(opp_sacks), \n                            lag(moving_sacks), opp_sacks),\n         opp_ints = lead(moving_ints),\n         opp_ints = ifelse(is.na(opp_ints), \n                           lag(moving_ints), opp_ints),\n         opp_pass_broken = lead(moving_pass_broken),\n         opp_pass_broken = ifelse(is.na(opp_pass_broken), \n                                  lag(moving_pass_broken), opp_pass_broken)\n         ) %&gt;%\n  mutate(\n    qbr_diff = moving_qbr_mean - opp_qbr,\n    pass_epa_diff = moving_passing_epa - opp_pass_epa,\n    rushing_epa_diff = moving_rushing_epa - opp_rushing_epa,\n    receiving_epa_diff = moving_receiving_epa - opp_receiving_epa,\n    tackles_diff = moving_tackles - opp_tackles,\n    forced_fumbles_diff = moving_forced_fumbles - opp_forced_fumbles,\n    sacks_diff = moving_sacks - opp_sacks,\n    ints_diff = moving_ints - opp_ints,\n    pass_broken_diff = moving_pass_broken - opp_pass_broken\n  ) %&gt;%\n  mutate(win = factor(win, levels = c(0, 1), labels = c(\"Lose\", \"Win\"))) %&gt;%\n  # Remove Games with Missing Feature Data\n  filter(!is.na(opp_qbr))\n\n\n\nlog_preds &lt;- predict(log_fit, merged_2024, type = \"prob\")[,2]\nrf_preds &lt;- predict(rf_fit, merged_2024, type = \"prob\")[,2]\nsv_preds &lt;- predict(sv_fit, merged_2024, type = \"prob\")[,2]\nxgb_preds &lt;- predict(xgb_fit, merged_2024, type = \"prob\")[,2]\n\n# Combine Predictions Into a Data Frame\npredictions &lt;- tibble(\n  Team = merged_2024$team,\n  Game_ID = merged_2024$game_id,\n  Logistic_Regression = paste0(round(log_preds * 100, 2), \"%\"),\n  Random_Forest = paste0(round(rf_preds * 100, 2), \"%\"),\n  SVM = paste0(round(sv_preds * 100, 2), \"%\"),\n  XGBoost = paste0(round(xgb_preds * 100, 2), \"%\")\n)\n\n# Load in NFL Logo Data to Make Cool Tables with {gt}\nteam_logos &lt;- nflfastR::teams_colors_logos %&gt;% \n  select(team_abbr, team_logo_espn)\n\nweek2_preds &lt;- predictions %&gt;%\n  left_join(team_logos, by = c(\"Team\" = \"team_abbr\")) %&gt;%\n  select(Team, team_logo_espn, Game_ID, Logistic_Regression, Random_Forest, SVM, XGBoost)\n\n# I Want to Create a Table That Has Teams Who Are Playing Each Other In the Same Row\n# To Do This, I'll Need to Reshape the Data\nreshaped_week2 &lt;- week2_preds %&gt;%\n    group_by(Game_ID) %&gt;%\n    summarise(\n        Team_1 = first(Team),\n        Team_1_Logo = first(team_logo_espn),\n        Team_1_Logistic_Regression = first(Logistic_Regression),\n        Team_1_Random_Forest = first(Random_Forest),\n        Team_1_SVM = first(SVM),\n        Team_1_XGBoost = first(XGBoost),\n        Team_2 = last(Team),\n        Team_2_Logo = last(team_logo_espn),\n        Team_2_Logistic_Regression = last(Logistic_Regression),\n        Team_2_Random_Forest = last(Random_Forest),\n        Team_2_SVM = last(SVM),\n        Team_2_XGBoost = last(XGBoost)\n    )\n\n# Now, I Can Use {gt} To Make a Nice Table\nreshaped_week2 %&gt;%\n  # Start a {gt} Table\n    gt() %&gt;%\n  # Modify Logo Settings\n    text_transform(\n        locations = cells_body(vars(Team_1_Logo, Team_2_Logo)),\n        fn = function(x) {\n            web_image(url = x, height = 40)  # Adjust the height as needed\n        }\n    ) %&gt;%\n  # Remove Columns From the Table\n    cols_hide(\n        columns = c(Game_ID, Team_1, Team_2)\n    ) %&gt;%\n  # Label the Columns\n    cols_label(\n        Team_1_Logo = \"Home\",\n        Team_1_Logistic_Regression = \"Logit\",\n        Team_1_Random_Forest = \"Random Forest\",\n        Team_1_SVM = \"SVM\",\n        Team_1_XGBoost = \"XGBoost\",\n        Team_2_Logo = \"Away\",\n        Team_2_Logistic_Regression = \"Logit\",\n        Team_2_Random_Forest = \"Random Forest\",\n        Team_2_SVM = \"SVM\",\n        Team_2_XGBoost = \"XGBoost\"\n    ) %&gt;%\n  # Create a Title for the Table\n    tab_header(\n        title = \"Predicted Win Probability by Game and Model\"\n    ) %&gt;%\n  # Column Formatting\n    tab_style(\n        style = list(\n            cell_text(weight = \"bold\")\n        ),\n        locations = cells_column_labels(everything())  \n    ) %&gt;%\n    cols_align(\n        align = \"center\",\n        columns = everything()\n    ) %&gt;%\n  # Adjust Column Widths \n    cols_width(\n        Team_1_Logo ~ px(100),  # Adjust as needed\n        Team_1_Logistic_Regression ~ px(85),  # Adjust as needed\n        Team_1_Random_Forest ~ px(85),  # Adjust as needed\n        Team_1_SVM ~ px(85),  # Adjust as needed\n        Team_1_XGBoost ~ px(85),  # Adjust as needed\n        Team_2_Logo ~ px(100),  # Adjust as needed\n        Team_2_Logistic_Regression ~ px(85),  # Adjust as needed\n        Team_2_Random_Forest ~ px(85),  # Adjust as needed\n        Team_2_SVM ~ px(85),  # Adjust as needed\n        Team_2_XGBoost ~ px(85)  # Adjust as needed\n    )\n\nWarning: Since gt v0.3.0, `columns = vars(...)` has been deprecated.\n• Please use `columns = c(...)` instead.\n\n\n\n\n\n\n\n\nPredicted Win Probability by Game and Model\n\n\nHome\nLogit\nRandom Forest\nSVM\nXGBoost\nAway\nLogit\nRandom Forest\nSVM\nXGBoost\n\n\n\n\n\n98.21%\n91.6%\n97.95%\n93.03%\n\n1.66%\n6%\n1.92%\n4.92%\n\n\n\n15.42%\n12.8%\n15.35%\n7.54%\n\n83.61%\n85.2%\n83.75%\n85.47%\n\n\n\n93.46%\n91.8%\n93.25%\n83.63%\n\n6.11%\n8%\n6.34%\n14.28%\n\n\n\n89.19%\n80.2%\n88.54%\n83.76%\n\n10.13%\n19.6%\n10.79%\n11.25%\n\n\n\n90.95%\n89.8%\n91.02%\n79.28%\n\n8.47%\n11.4%\n8.44%\n14.85%\n\n\n\n79.87%\n71%\n76.22%\n64.98%\n\n18.99%\n31.2%\n22.57%\n30.99%\n\n\n\n2.47%\n4.2%\n2.68%\n2.32%\n\n97.35%\n94.4%\n97.14%\n97.02%\n\n\n\n45.8%\n46%\n47.71%\n31.15%\n\n52.4%\n45.4%\n50.59%\n62.23%\n\n\n\n96.01%\n69.8%\n96.07%\n85.33%\n\n3.72%\n26.4%\n3.68%\n13.92%\n\n\n\n21.73%\n19.6%\n20.61%\n14.24%\n\n77.01%\n83%\n78.26%\n81.43%\n\n\n\n92.83%\n87.6%\n92.69%\n78.2%\n\n6.7%\n14%\n6.87%\n17.81%\n\n\n\n10.32%\n24.6%\n10.73%\n13.51%\n\n88.98%\n73.2%\n88.6%\n82.98%\n\n\n\n8.04%\n9.6%\n8.97%\n6.5%\n\n91.41%\n86.4%\n90.46%\n91.34%\n\n\n\n38.9%\n49.2%\n40.41%\n51.87%\n\n59.36%\n48.6%\n57.94%\n54.9%\n\n\n\n92.36%\n88.2%\n91.65%\n81.26%\n\n7.15%\n15.4%\n7.85%\n17.28%\n\n\n\n17.74%\n16.4%\n17.38%\n20.76%\n\n81.18%\n83.6%\n81.63%\n77.53%\n\n\n\n\n\n\n\nAs you can see, there are some wacky predictions for week 2 game outcomes. The Saints are massive favorites over the Cowboys? The Vikings are massive favorites to the 49ers? What?! Well, the answer is not very surprising. In predicting week 2 games, we use all data from prior weeks in the season. In week 2, this means we only have one week of data to draw from. That means that, if a team does exceptionally well in week 1, this great performance is going to impact predictions for week 2. Both the Saints and Vikings had great offensive and defensive performances in week 1, which explains why this model is so bullish on these teams. It stands to reason that such model predictions would probably not show up later on in the season.\nThis gets to my third point on my model performance. When a model is solely impacted by the data, and the available data is not incredibly informative, we are going to get predictions that are pretty counter-intuitive. Basically, please do not put any money down on the Saints or Vikings outright winning this week! I think something to explore in the future would be Bayesian methods to incorporate prior information (i.e. the Cowboys perform well in the early regular season, the 49ers are really good, etc.) that can stabilize the existing limited data with prior knowledge. As the causal inference folks are quick to say… data are dumb, especially when such limited data. Especially early in the season, Bayesian methods may prove really helpful in preventing predictions that are generated from an outlier or two.\nOut of curiosity, I wanted to check how well the model was able to predict the outcome of games by each week in the season. The expectation would be that the model becomes more accurate as the season goes on (we get more information). Below is a plot of the average percent of games whose outcomes are correctly predicted each week from the 2006-2023 seasons.\n\n\nCode\nmerged$log_preds &lt;- predict(log_fit, merged, type = \"prob\")[,2]\nmerged$rf_preds &lt;- predict(rf_fit, merged, type = \"prob\")[,2]\nmerged$rf_preds2 &lt;- ifelse(merged$rf_preds &gt;= 0.5, 1, 0)\nmerged$sv_preds &lt;- predict(sv_fit, merged, type = \"prob\")[,2]\nmerged$xgb_preds &lt;- predict(xgb_fit, merged, type = \"prob\")[,2]\n\nmerged %&gt;%\n  filter(week != 1) %&gt;%\n  mutate(log_class = ifelse(log_preds &gt;= 0.5, \"Win\", \"Lose\"),\n         rf_class = ifelse(rf_preds &gt;= 0.5, \"Win\", \"Lose\"),\n         sv_class = ifelse(sv_preds &gt;= 0.5, \"Win\", \"Lose\"),\n         xgb_class = ifelse(xgb_preds &gt;= 0.5, \"Win\", \"Lose\")) %&gt;%\n  mutate(log_right = ifelse(win == log_class, 1, 0),\n         rf_right = ifelse(win == rf_class, 1, 0),\n         sv_right = ifelse(win == sv_class, 1, 0),\n         xgb_right = ifelse(win == xgb_class, 1, 0)) %&gt;%\n  # By Week, Calculate Predictive Accuracy\n  group_by(week) %&gt;%\n  summarise(log_week_right = mean(log_right),\n         rf_week_right = mean(rf_right),\n         sv_week_right = mean(sv_right),\n         xgb_week_right = mean(xgb_right)) %&gt;%\n  # Pivot to Color by Model Type\n  pivot_longer(cols = starts_with(\"log_week_right\"):starts_with(\"xgb_week_right\"),\n               names_to = \"Model\",\n               values_to = \"Accuracy\") %&gt;%\n  mutate(Model = recode(Model,\n                        log_week_right = \"Logistic Regression\",\n                        rf_week_right = \"Random Forest\",\n                        sv_week_right = \"SVM\",\n                        xgb_week_right = \"XGBoost\")) %&gt;%\n  ggplot(aes(x = week, y = Accuracy, color = Model)) +\n  geom_line() +\n  geom_point() +\n  scale_x_continuous(breaks = 2:18) +\n  scale_y_continuous(breaks = seq(0.6, 1, by = 0.05),\n                     labels = scales::percent) + \n  scale_color_manual(\n    values = c(\"Logistic Regression\" = \"#e31837\", \n               \"Random Forest\" = \"#003594\", \n               \"SVM\" = \"#041e42\", \n               \"XGBoost\" = \"#ffb81c\")\n  ) +\n  labs(title = \"Week 1 is Excluded Due to Lack of In-Season Data\",\n       x = \"Week\",\n       y = \"Average Predictive Accuracy\",\n       color = \"Model\") +\n  blog_theme() + \n  theme(\n    plot.title = element_text(face = \"bold\"), \n    legend.title = element_text(face = \"bold\")  \n  )\n\n\n\n\n\nAverage In-Sample Predictive Accuracy by Model Over NFL Weeks\n\n\n\n\nLike a lot of things in the world of data science, when you plot the data expecting answers, you actually just get a lot more questions. While these report in-sample results (in contrast to cross-validated out-of-sample accuracy metrics… so take these accuracy numbers with a grain of salt), I still would have expected an upward trend over the NFL season, but nope! And there’s other interesting things as well… like how three of the models have a crazy dip in predictive performance in week 10. Don’t really know what that’s about. Well, even if the plot doesn’t support my diagnosis and prescription all that well, I’m convinced that pursuing a modeling strategy that incorporates prior information and domain knowledge would probably result in less “Saints over Cowboys” and “Vikings over 49ers” predictions.\n\n\nSetting Up My Workflow\nLastly, I want to document how I’m going to go about creating predictions every week. After all, I’ve collected data and trained some models, but there is no magic button I can press that will just sequentially update everything every week throughout the remainder of the NFL season. The following code chunk walks through my “workflow” so to speak.\n\n# Establish Global Week Parameters So I Don't Have to Update Every Data Set Individually\nlast_week &lt;- 1\nthis_week &lt;- 2\n\n# Load and Clean Updated Predictor Data\nqbr_2024 &lt;- load_espn_qbr(\n  seasons = 2024,\n  summary_type = c(\"week\")) %&gt;%\n  filter(season_type == \"Regular\") %&gt;%\n  select(c(team_abb, season, game_week, qbr_total, pts_added)) %&gt;%\n  group_by(season, team_abb) %&gt;%\n  mutate(\n    moving_qbr_mean = cumsum(qbr_total) / game_week,\n    moving_pts_added = cumsum(pts_added / game_week),\n    # Rename Washington for Merging\n    team_abb = ifelse(team_abb == \"WSH\", \"WAS\", team_abb)) %&gt;%\n  filter(game_week == last_week) %&gt;%\n  mutate(game_week = this_week)\n\noffensive_2024 &lt;- load_player_stats(\n  seasons = 2024,\n  stat_type = \"offense\") %&gt;%\n  filter(season_type == \"REG\") %&gt;%\n  group_by(season, recent_team, week) %&gt;%\n  summarise(\n    passing_epa = sum(passing_epa, na.rm = TRUE),\n    rushing_epa = sum(rushing_epa, na.rm = TRUE),\n    receiving_epa = sum(receiving_epa, na.rm = TRUE)\n  ) %&gt;%\n  ungroup() %&gt;%\n  group_by(season, recent_team) %&gt;%\n  mutate(\n    moving_passing_epa = cumsum(passing_epa) / week,\n    moving_rushing_epa = cumsum(rushing_epa) / week,\n    moving_receiving_epa = cumsum(receiving_epa) / week) %&gt;%\n  select(season, recent_team, week, passing_epa, rushing_epa, receiving_epa, moving_passing_epa, \n         moving_rushing_epa, moving_receiving_epa) %&gt;%\n  filter(week == last_week) %&gt;%\n  mutate(week = this_week) %&gt;%\n  mutate(recent_team = ifelse(recent_team == \"LA\", \"LAR\", recent_team))\n\ndefensive_2024 &lt;- load_player_stats(\n  seasons = 2024,\n  stat_type = \"defense\") %&gt;%\n  filter(season_type == \"REG\") %&gt;%\n  group_by(season, team, week) %&gt;%\n  summarise(\n    tackles = sum(def_tackles, na.rm = TRUE),\n    forced_fumbles = sum(def_fumbles_forced, na.rm = TRUE),\n    sacks = sum(def_sacks, na.rm = TRUE),\n    ints = sum(def_interceptions, na.rm = TRUE),\n    pass_broken = sum( def_pass_defended, na.rm = TRUE)\n  ) %&gt;%\n  ungroup() %&gt;%\n  group_by(season, team) %&gt;%\n  mutate(\n    moving_tackles = cumsum(tackles) / week,\n    moving_forced_fumbles = cumsum(forced_fumbles) / week,\n    moving_sacks = cumsum(sacks) / week,\n    moving_ints = cumsum(ints) / week,\n    moving_pass_broken = cumsum(pass_broken) / week) %&gt;%\n  select(season, team, week, tackles, forced_fumbles, sacks, ints, pass_broken, moving_tackles, \n         moving_forced_fumbles, moving_sacks, moving_ints, moving_pass_broken) %&gt;%\n  filter(week == last_week) %&gt;%\n  mutate(week = this_week) %&gt;%\n  mutate(team = ifelse(team == \"LA\", \"LAR\", team))\n\nseason_2024 &lt;- load_schedules(seasons = 2024)\n\nseason_2024 &lt;- clean_homeaway(season_2024) %&gt;%\n  filter(game_type == \"REG\") %&gt;%\n  mutate(home = ifelse(location == \"home\", 1, 0),\n         win = ifelse(team_score &gt; opponent_score, 1, 0)) %&gt;%\n  select(game_id, season, week, team, opponent, home, win) %&gt;%\n  filter(week == this_week) %&gt;%\n  mutate(team = ifelse(team == \"LA\", \"LAR\", team),\n         opponent = ifelse(opponent == \"LA\", \"LAR\", opponent))\n\nmerged_2024 &lt;- inner_join(season_2024, qbr_2024, \n                          by = c(\"team\" = \"team_abb\", \"week\" = \"game_week\", \"season\")) %&gt;%\n  inner_join(offensive_2024, by = c(\"team\" = \"recent_team\", \"week\", \"season\")) %&gt;%\n  inner_join(defensive_2024, by = c(\"team\", \"week\", \"season\")) %&gt;%\n  group_by(game_id) %&gt;%\n  mutate(opp_qbr = lead(moving_qbr_mean),\n         opp_qbr = ifelse(is.na(opp_qbr), \n                          lag(moving_qbr_mean), opp_qbr),\n         opp_pass_epa = lead(moving_passing_epa),\n         opp_pass_epa = ifelse(is.na(opp_pass_epa), \n                               lag(moving_passing_epa), opp_pass_epa),\n         opp_rushing_epa = lead(moving_rushing_epa),\n         opp_rushing_epa = ifelse(is.na(opp_rushing_epa), \n                                  lag(moving_rushing_epa), opp_rushing_epa),\n         opp_receiving_epa = lead(moving_receiving_epa),\n         opp_receiving_epa = ifelse(is.na(opp_receiving_epa), \n                                    lag(moving_receiving_epa), opp_receiving_epa),\n         opp_tackles = lead(moving_tackles),\n         opp_tackles = ifelse(is.na(opp_tackles), \n                              lag(moving_tackles), opp_tackles),\n         opp_forced_fumbles = lead(moving_forced_fumbles),\n         opp_forced_fumbles = ifelse(is.na(opp_forced_fumbles), \n                                     lag(moving_forced_fumbles), opp_forced_fumbles),\n         opp_sacks = lead(moving_sacks),\n         opp_sacks = ifelse(is.na(opp_sacks), \n                            lag(moving_sacks), opp_sacks),\n         opp_ints = lead(moving_ints),\n         opp_ints = ifelse(is.na(opp_ints), \n                           lag(moving_ints), opp_ints),\n         opp_pass_broken = lead(moving_pass_broken),\n         opp_pass_broken = ifelse(is.na(opp_pass_broken), \n                                  lag(moving_pass_broken), opp_pass_broken)\n         ) %&gt;%\n  mutate(\n    qbr_diff = moving_qbr_mean - opp_qbr,\n    pass_epa_diff = moving_passing_epa - opp_pass_epa,\n    rushing_epa_diff = moving_rushing_epa - opp_rushing_epa,\n    receiving_epa_diff = moving_receiving_epa - opp_receiving_epa,\n    tackles_diff = moving_tackles - opp_tackles,\n    forced_fumbles_diff = moving_forced_fumbles - opp_forced_fumbles,\n    sacks_diff = moving_sacks - opp_sacks,\n    ints_diff = moving_ints - opp_ints,\n    pass_broken_diff = moving_pass_broken - opp_pass_broken\n  ) %&gt;%\n  mutate(win = factor(win, levels = c(0, 1), labels = c(\"Lose\", \"Win\"))) %&gt;%\n  filter(!is.na(qbr_diff))\n\n# Load Trained Models\nlog_fit &lt;- readRDS(\"data-and-analysis/log_fit_model.rds\")\nrf_fit &lt;- readRDS(\"data-and-analysis/rf_fit_model.rds\")\nsv_fit &lt;- readRDS(\"data-and-analysis/sv_fit_model.rds\")\nxgb_fit &lt;- readRDS(\"data-and-analysis/xgb_fit_model.rds\")\n\n# Create a Data Frame with Model Predictions\nlog_preds &lt;- predict(log_fit, merged_2024, type = \"prob\")[,2]\nrf_preds &lt;- predict(rf_fit, merged_2024, type = \"prob\")[,2]\nsv_preds &lt;- predict(sv_fit, merged_2024, type = \"prob\")[,2]\nxgb_preds &lt;- predict(xgb_fit, merged_2024, type = \"prob\")[,2]\n\npredictions &lt;- tibble(\n  Team = merged_2024$team,\n  Game_ID = merged_2024$game_id,\n  week = merged_2024$week,\n  Logistic_Regression = paste0(round(log_preds * 100, 2), \"%\"),\n  Random_Forest = paste0(round(rf_preds * 100, 2), \"%\"),\n  SVM = paste0(round(sv_preds * 100, 2), \"%\"),\n  XGBoost = paste0(round(xgb_preds * 100, 2), \"%\")\n)\n\n# Create a Back-Up Spreadsheet Before Updating\nglobal_preds &lt;- read_excel(\"data-and-analysis/nfl_2024_global_preds.xlsx\")\nwrite_xlsx(global_preds, \"data-and-analysis/nfl_2024_global_preds_backup.xlsx\")\n\n# Add Model Predictions for This Week to Season-Level Spreadsheet\nupdated_preds &lt;- inner_join(predictions, global_preds, by = c(\n  \"Team\", \"Game_ID\", \"week\", \"Logistic_Regression\", \"Random_Forest\", \"SVM\", \"XGBoost\"\n))\nwrite_xlsx(updated_preds, \"data-and-analysis/nfl_2024_global_preds.xlsx\")\n\nNow that I’ve created this, I hope that clicking “Run” now serves as the magical button that I just have to click and I get new predictions every week. We will see how this goes, as I’m sure there’s some bug/dependency I’m missing."
  }
]