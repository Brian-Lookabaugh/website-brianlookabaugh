[
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Topics on Causal Inference and Data Science",
    "section": "",
    "text": "R\n\n\nSQL\n\n\ndplyr\n\n\n\n\nA brief introduction/refresher to executing SQL in R and translating between SQL and dplyr syntax.\n\n\n\n\n\n\nFeb 1, 2023\n\n\nBrian Lookabaugh\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/R/SQL and R/index.html",
    "href": "blog/R/SQL and R/index.html",
    "title": "Executing SQL in R",
    "section": "",
    "text": "To begin, we will load the packages that will be utilized in this blog.\n\npacman::p_load(\n  \"dplyr\", ## Data Manipulation in R\n  \"sqldf\", ## Running SQL Queries in R\n  \"dbplyr\", ## Translating dplyr Syntax to SQL Syntax\n  \"DBI\", ## Connecting to a Database\n  \"odbc\", ## Connecting to a Database\n  \"tidyquery\", ## Translating SQL Syntax to dplyr Syntax\n  install = FALSE\n)\n\n\nSetting Up Databases\nIn practice, executing SQL in R requires connection to a pre-existing SQL database. For the purpose of this blog, however, we will just be using a temporary database stored in a local RStudio session. We will store this database as an object call con.\n\ncon <- DBI::dbConnect(RSQLite::SQLite(), \":memory:\")\n\nFor practical reasons, the syntax above will not be sufficient. Each connection will look different, dependent on various circumstances (the type of relational database management system (RDBMS) being used, log-in information, etc.), so the following example is just that; an example using completely made-up information. However, it does serve as a template for real information to be plugged into.\n\ncon <- dbConnect(odbc(),\n                 Driver = ,\n                 Server = ,\n                 Database = ,\n                 UID = ,\n                 PWD = ,\n                 Port = )\n\nReturning to the database we created, it is empty and has no data stored in it. To keep things simple, we are going to load the mtcars data set. We first begin by loading the data into RStudio. The second line of code copies this data set into the local database that we created. Now that we have copied this data into the local database, we can remove the mtcars data set from the local environment.\n\ndata(\"mtcars\")\n\ndbWriteTable(conn = con,\n             name = \"mtcars\",\n             value = mtcars)\n\nrm(mtcars)\n\n\n\nRunning a SQL Query in R\nNow that we have the mtcars data in our database, we can run a SQL query to retrieve information from this data. Using the dbGetQuery command, we can execute SQL syntax to return desired information. Here, we are writing a query to return a table which tells us the average miles per gallon for automatic vehicles grouped by the number of cylinders the vehicle has and ordered by miles per gallon from the highest to lowest values.\n\nquery_1 <- dbGetQuery(con,\n  'SELECT ROUND(AVG(mpg)) as avg_mpg, cyl\n   FROM mtcars\n   WHERE am = 1\n   GROUP BY cyl\n   ORDER BY avg_mpg DESC;'\n)\n\ntibble(query_1)\n\n# A tibble: 3 × 2\n  avg_mpg   cyl\n    <dbl> <dbl>\n1      28     4\n2      21     6\n3      15     8\n\n\nIn contrast, if you wanted to execute a query on a data frame object instead of pulling from a database, you can use sqldf.\n\nquery_2 <- sqldf(\n  'SELECT ROUND(AVG(mpg)) as avg_mpg, cyl\n   FROM mtcars\n   WHERE am = 1\n   GROUP BY cyl\n   ORDER BY avg_mpg DESC;'\n)\n\ntibble(query_2)\n\n# A tibble: 3 × 2\n  avg_mpg   cyl\n    <dbl> <dbl>\n1      28     4\n2      21     6\n3      15     8\n\n\n\n\nRunning a SQL Chunk in RMarkdown/Quarto\nWe can conveniently execute a SQL query in R without relying on a specific command like dbGetQuery. Using RMarkdown or Quarto, we can specify a SQL code chunk. Within the code chunk, you will need to specify the connection (con in our case) and, optionally, the object that the results of the query will be stored in. In the output below, you would begin the code chunk with {sql, connection = con, output.var = \"query_2\"}.\n\nSELECT\n  ROUND(AVG(mpg)) AS avg_mpg,\n  cyl\nFROM mtcars\nWHERE am = 1\nGROUP BY cyl\nORDER BY avg_mpg DESC;\n\nNote that if you are going to be using SQL chunks frequently, it is worth specifying the default connection for SQL chunks as demonstrated below.\n\nknitr::opts_chunk$set(connection = \"con\")\n\n\n\nTranslating dplyr Syntax to SQL Syntax and Vice Versa\nAnother very helpful tool that bridges the gap between SQL and dplyr syntax is the show_query command. Personally, I found this tool incredibly valuable when learning SQL because of my background in R. Essentially, what this tool does is translate dplyr syntax into SQL syntax. In the opposite direction, through the tidyquery package, we also have the capability to the exact opposite and translate SQL syntax into dplyr syntax. Below demonstrates the functionality of these two commands for the same query. First, translating dplyr syntax to SQL syntax:\n\ntbl(con, \"mtcars\") %>%\n  filter(am == 1) %>%\n  group_by(cyl) %>%\n  summarise(avg_mpg = round(mean(mpg))) %>%\n  ungroup() %>%\n  arrange(dplyr::desc(avg_mpg)) %>%\n  show_query()\n\n<SQL>\nSELECT `cyl`, ROUND(AVG(`mpg`), 0) AS `avg_mpg`\nFROM `mtcars`\nWHERE (`am` = 1.0)\nGROUP BY `cyl`\nORDER BY `avg_mpg` DESC\n\n\nNow, we will do the opposite\n\nshow_dplyr(\n  \"SELECT\n    ROUND(AVG(mpg)) AS avg_mpg,\n    cyl\n   FROM mtcars\n   WHERE am = 1\n   GROUP BY cyl\n   ORDER BY avg_mpg DESC;\"\n)\n\nmtcars %>%\n  filter(am == 1) %>%\n  group_by(cyl) %>%\n  summarise(avg_mpg = round(mean(mpg, na.rm = TRUE))) %>%\n  ungroup() %>%\n  arrange(dplyr::desc(avg_mpg))\n\n\nObviously, as one’s knowledge in both SQL and R increases, the further capabilities of executing SQL in R can be explored. My hope is that this serves as a helpful introductory for those seeking to integrate data science tools together."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Brian Lookabaugh",
    "section": "",
    "text": "My current work is focused on making causal inferences from conflict management programs. Cutting out the jargon, I am curious if policies such as peacekeeping operations, peace agreements, mediation, foreign aid, and foreign direct investment actually contribute to resolving civil conflicts and creating peaceful post-conflict environments. Detailed explanations and technical information concerning this research can be located on the projects section of this site or my Github profile. In addition, I occasionally write blog posts where I primarily discuss topics related to causal inference and data science. In particular, I am working on a comprehensive set of blog posts designed to introduce curious social scientists to causal inference and experimental/quasi-experimental design."
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Causal Inference and Data Science Projects",
    "section": "",
    "text": "causal inference\n\n\nquasi-experimental design\n\n\nmatching\n\n\nIPW\n\n\npanel data\n\n\nsensitivity analysis\n\n\n\n\nEmploying a quasi-experimental design, this project estimates the causal impact of UN peacekeeping operation deployments and withdrawals on economic development.\n\n\n\n\n\n\nFeb 4, 2023\n\n\nBrian Lookabaugh\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/UN PKOs and Development/index.html",
    "href": "projects/UN PKOs and Development/index.html",
    "title": "Do UN Peacekeeping Operations Contribute to Economic Development?",
    "section": "",
    "text": "(The entirety of code for this project along with manuscripts, data, and graphics can be found in this GitHub repository)."
  },
  {
    "objectID": "resume/index.html",
    "href": "resume/index.html",
    "title": "Resume",
    "section": "",
    "text": "Download Current Resume"
  }
]