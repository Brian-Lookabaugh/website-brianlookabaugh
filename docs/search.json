[
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Topics on Causal Inference and Data Science",
    "section": "",
    "text": "R\n\n\nSQL\n\n\ndplyr\n\n\n\n\nA brief introduction/refresher to executing SQL in R and translating between SQL and dplyr syntax.\n\n\n\n\n\n\nFeb 1, 2023\n\n\nBrian Lookabaugh\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/R/SQL and R/index.html",
    "href": "blog/R/SQL and R/index.html",
    "title": "Executing SQL in R",
    "section": "",
    "text": "To begin, we will load the packages that will be utilized in this blog.\n\n\nCode\npacman::p_load(\n  \"dplyr\", ## Data Manipulation in R\n  \"sqldf\", ## Running SQL Queries in R\n  \"dbplyr\", ## Translating dplyr Syntax to SQL Syntax\n  \"DBI\", ## Connecting to a Database\n  \"odbc\", ## Connecting to a Database\n  \"tidyquery\", ## Translating SQL Syntax to dplyr Syntax\n  install = FALSE\n)\n\n\n\nSetting Up Databases\nIn practice, executing SQL in R requires connection to a pre-existing SQL database. For the purpose of this blog, however, we will just be using a temporary database stored in a local RStudio session. We will store this database as an object call con.\n\n\nCode\ncon <- DBI::dbConnect(RSQLite::SQLite(), \":memory:\")\n\n\nFor practical reasons, the syntax above will not be sufficient. Each connection will look different, dependent on various circumstances (the type of relational database management system (RDBMS) being used, log-in information, etc.), so the following example is just that; an example using completely made-up information. However, it does serve as a template for real information to be plugged into.\n\n\nCode\ncon <- dbConnect(odbc(),\n                 Driver = ,\n                 Server = ,\n                 Database = ,\n                 UID = ,\n                 PWD = ,\n                 Port = )\n\n\nReturning to the database we created, it is empty and has no data stored in it. To keep things simple, we are going to load the mtcars data set. We first begin by loading the data into RStudio. The second line of code copies this data set into the local database that we created. Now that we have copied this data into the local database, we can remove the mtcars data set from the local environment.\n\n\nCode\ndata(\"mtcars\")\n\ndbWriteTable(conn = con,\n             name = \"mtcars\",\n             value = mtcars)\n\nrm(mtcars)\n\n\n\n\nRunning a SQL Query in R\nNow that we have the mtcars data in our database, we can run a SQL query to retrieve information from this data. Using the dbGetQuery command, we can execute SQL syntax to return desired information. Here, we are writing a query to return a table which tells us the average miles per gallon for automatic vehicles grouped by the number of cylinders the vehicle has and ordered by miles per gallon from the highest to lowest values.\n\n\nCode\nquery_1 <- dbGetQuery(con,\n  'SELECT ROUND(AVG(mpg)) as avg_mpg, cyl\n   FROM mtcars\n   WHERE am = 1\n   GROUP BY cyl\n   ORDER BY avg_mpg DESC;'\n)\n\ntibble(query_1)\n\n\n# A tibble: 3 × 2\n  avg_mpg   cyl\n    <dbl> <dbl>\n1      28     4\n2      21     6\n3      15     8\n\n\nIn contrast, if you wanted to execute a query on a data frame object instead of pulling from a database, you can use sqldf.\n\n\nCode\nquery_2 <- sqldf(\n  'SELECT ROUND(AVG(mpg)) as avg_mpg, cyl\n   FROM mtcars\n   WHERE am = 1\n   GROUP BY cyl\n   ORDER BY avg_mpg DESC;'\n)\n\ntibble(query_2)\n\n\n# A tibble: 3 × 2\n  avg_mpg   cyl\n    <dbl> <dbl>\n1      28     4\n2      21     6\n3      15     8\n\n\n\n\nRunning a SQL Chunk in RMarkdown/Quarto\nWe can conveniently execute a SQL query in R without relying on a specific command like dbGetQuery. Using RMarkdown or Quarto, we can specify a SQL code chunk. Within the code chunk, you will need to specify the connection (con in our case) and, optionally, the object that the results of the query will be stored in. In the output below, you would begin the code chunk with {sql, connection = con, output.var = \"query_2\"}.\n\n\nCode\nSELECT\n  ROUND(AVG(mpg)) AS avg_mpg,\n  cyl\nFROM mtcars\nWHERE am = 1\nGROUP BY cyl\nORDER BY avg_mpg DESC;\n\n\nNote that if you are going to be using SQL chunks frequently, it is worth specifying the default connection for SQL chunks as demonstrated below.\n\n\nCode\nknitr::opts_chunk$set(connection = \"con\")\n\n\n\n\nTranslating dplyr Syntax to SQL Syntax and Vice Versa\nAnother very helpful tool that bridges the gap between SQL and dplyr syntax is the show_query command. Personally, I found this tool incredibly valuable when learning SQL because of my background in R. Essentially, what this tool does is translate dplyr syntax into SQL syntax. In the opposite direction, through the tidyquery package, we also have the capability to the exact opposite and translate SQL syntax into dplyr syntax. Below demonstrates the functionality of these two commands for the same query. First, translating dplyr syntax to SQL syntax:\n\n\nCode\ntbl(con, \"mtcars\") %>%\n  filter(am == 1) %>%\n  group_by(cyl) %>%\n  summarise(avg_mpg = round(mean(mpg))) %>%\n  ungroup() %>%\n  arrange(dplyr::desc(avg_mpg)) %>%\n  show_query()\n\n\n<SQL>\nSELECT `cyl`, ROUND(AVG(`mpg`), 0) AS `avg_mpg`\nFROM `mtcars`\nWHERE (`am` = 1.0)\nGROUP BY `cyl`\nORDER BY `avg_mpg` DESC\n\n\nNow, we will do the opposite\n\n\nCode\nshow_dplyr(\n  \"SELECT\n    ROUND(AVG(mpg)) AS avg_mpg,\n    cyl\n   FROM mtcars\n   WHERE am = 1\n   GROUP BY cyl\n   ORDER BY avg_mpg DESC;\"\n)\n\n\nmtcars %>%\n  filter(am == 1) %>%\n  group_by(cyl) %>%\n  summarise(avg_mpg = round(mean(mpg, na.rm = TRUE))) %>%\n  ungroup() %>%\n  arrange(dplyr::desc(avg_mpg))\n\n\nObviously, as one’s knowledge in both SQL and R increases, the further capabilities of executing SQL in R can be explored. My hope is that this serves as a helpful introductory for those seeking to integrate data science tools together."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Brian Lookabaugh",
    "section": "",
    "text": "My current work is focused on making causal inferences from conflict management programs. Cutting out the jargon, I am curious if policies such as peacekeeping operations, peace agreements, mediation, foreign aid, and foreign direct investment actually contribute to resolving civil conflicts and creating peaceful post-conflict environments. Detailed explanations and technical information concerning this research can be located on the projects section of this site or my Github profile. In addition, I occasionally write blog posts where I primarily discuss topics related to causal inference and data science. In particular, I am working on a comprehensive set of blog posts designed to introduce curious social scientists to causal inference and experimental/quasi-experimental design."
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Causal Inference and Data Science Projects",
    "section": "",
    "text": "causal inference\n\n\nquasi-experimental design\n\n\nmatching\n\n\nIPW\n\n\npanel data\n\n\nsensitivity analysis\n\n\n\n\nEmploying a quasi-experimental design, this project estimates the causal impact of UN peacekeeping operation deployments and withdrawals on economic development.\n\n\n\n\n\n\nFeb 4, 2023\n\n\nBrian Lookabaugh\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/UN PKOs and Development/index.html",
    "href": "projects/UN PKOs and Development/index.html",
    "title": "Do UN Peacekeeping Operations Contribute to Economic Development?",
    "section": "",
    "text": "The United Nation’s (UN) pacifying role in the world has been stained in the public eye with images of Rwanda and Srebrenica, Bosnia coming to mind. Despite this, most studies evaluating the potentially pacifying effects of UN peacekeeping operations (PKOs) have largely demonstrated a positive relationship between UN PKOs and peace. To a lesser extent, researchers have also sought to evaluate the economic consequences of UN PKOs. Given the relationship between economic development and conflict, UN PKOs should also lead to better economic conditions if we expect PKOs to be effective at promoting peace. However, most studies in this area are limited to correlative analyses. Where quasi-experimental methods are employed, they are often lacking, missing a clear confounder identification strategy, misusing quasi-experimental methods, and/or omitting sensitivity analysis to assess the impact of unobserved confounders. This project employs matching and inverse probability weighting (IPW) designed for panel data as implemented by [@Imai et al.:2021] to evaluate the causal impact of UN PKOs on economic development.\nThe remainder of this project is structured as follows. First, I briefly discuss the theoretical relationship between UN PKOs and development to provide the reader with basic familiarity concerning the alleged relationship between treatment (PKO) and outcome (economic development). Second, I walk through the data collection and manipulation process, detailing which variables were selected, why they were selected, and how data was manipulated. Third, I provide context and justification concerning the implementation of the novel panel data matching analysis as proposed by @Imai et al.: 2021. Fourth, I finally get to the fun part… the results. In this section, I walk readers through the results and their substantive importance. Finally, I conclude with a recap of the project, its limitations, and conclusions generated from the findings.\n\nPKOs and Economic Development\n\n\nCollecting the Data\n\n\nThe Limitations of Conventional Matching for Panel Data\n\n\nResults\n\n\nConclusion"
  },
  {
    "objectID": "resume/index.html",
    "href": "resume/index.html",
    "title": "Resume",
    "section": "",
    "text": "Download Current Resume"
  }
]