[
  {
    "objectID": "resume/index.html",
    "href": "resume/index.html",
    "title": "Brian Lookabaugh",
    "section": "",
    "text": "Download"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Brian Lookabaugh",
    "section": "",
    "text": "Greetings and welcome to my personal website! My name is Brian Lookabaugh and I am a Research Analyst at Fors Marsh. In my position, I leverage my skills in data analysis and statistical modeling to solve a variety of business problems. These include descriptive tasks (creating visualizations to communicate trends and patterns), predictive problems (utilizing machine learning to predict outcomes and forecast), and causal questions (drawing on my background in causal inference to answer questions such as “did X have an impact on Y?”).\n\nOutside of my professional role, I routinely refine and expand my methodological toolkit by researching new methods and applying my skill set to other recreational interests of mine. You can find some of these passion projects under the “Blog” section of this site. If you have any questions about my research, please feel free to reach out to me!"
  },
  {
    "objectID": "blog/2024/nfl-workflow/index.html",
    "href": "blog/2024/nfl-workflow/index.html",
    "title": "Predicting the Outcome of NFL Games in the 2024-2025 Season",
    "section": "",
    "text": "Code\n# Load Libraries\npacman::p_load(\n  \"dplyr\", # Data Manipulation\n  \"nflverse\", # NFL Verse Environment\n  \"gt\", # Nice Tables\n  \"tidyr\", # Reshaping Data\n  \"stringr\", # Working with Strings\n  \"caret\", # Machine Learning\n  \"scales\", # Percent Formatting\n  \"readxl\", # Reading Excel Files\n  \"writexl\", # Writing Excel Files\n  install = FALSE\n)\n\n# Define a Custom Theme - Taken From Andrew Heiss's Blogs\nblog_theme &lt;- function() {\n  theme_bw() +  # Start with theme_bw\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      plot.title = element_text(face = \"bold\"),\n      axis.title = element_text(face = \"bold\"),\n      strip.text = element_text(face = \"bold\"),\n      strip.background = element_rect(fill = \"grey80\", color = NA),\n      legend.title = element_text(face = \"bold\")\n    )\n}\n\n\nI am a massive fan of NFL football. I look forward to the inaugural start of the regular season every September and it feels all too soon when the season ends when the Super Bowl is played in February. As much as I love the game-play, the sports shows spinning their narratives, and the social aspect of NFL Sundays, I have been looking for excuses to get my hands on NFL data and having fun with an additional aspect of the game.\nRecently, in pursuit of this goal, I went to the Playoff Predictors website, where you can go game-by-game and pick who you think will win each game. It’s a fun exercise that I look forward to every year when the NFL schedule is released and it gives me a picture of what I intuitively think the standings might look like at the conclusion of the upcoming season. Once I got these standings, I played around with the {nflreadr} and {gt} packages to present my predicted standings in a more aesthetically pleasing way.\n\n\nCode\n# Create My Vibes Tribble - Adding Extra Spacing\nvibes &lt;- tribble(\n  ~east, ~record_1, ~space_1, ~north, ~record_2, ~space_2, ~south, ~record_3, ~space_3, ~west, ~record_4, ~conf,\n  \"BUF\", \"10-7\", \" \", \"BAL\", \"13-4\", \" \", \"IND\", \"13-4\", \" \", \"KC\", \"12-5\", \"AFC\",\n  \"NYJ\", \"9-8\", \" \", \"CIN\", \"10-7\", \" \", \"HOU\", \"11-6\", \" \", \"LAC\", \"11-6\", \"AFC\",\n  \"MIA\", \"7-10\", \" \", \"PIT\", \"8-9\", \" \",\"TEN\", \"9-8\", \" \", \"DEN\", \"7-10\", \"AFC\",\n  \"NE\", \"3-14\", \" \", \"CLE\", \"8-9\", \" \", \"JAX\", \"9-8\", \" \", \"LV\", \"6-11\", \"AFC\",\n  \"PHI\", \"9-8\", \" \", \"GB\", \"12-5\", \" \", \"ATL\", \"9-8\", \" \", \"LAR\", \"11-6\", \"NFC\",\n  \"WSH\", \"8-9\", \" \", \"DET\", \"11-6\", \" \", \"TB\", \"7-10\", \" \", \"SF\", \"10-7\", \"NFC\",\n  \"DAL\", \"7-10\", \" \", \"CHI\", \"9-8\", \" \", \"CAR\", \"5-12\", \" \", \"ARZ\", \"9-8\", \"NFC\",\n  \"NYG\", \"5-12\", \" \", \"MIN\", \"6-11\", \" \", \"NO\", \"4-13\", \" \", \"SEA\", \"4-13\", \"NFC\"\n)\n\nvibes %&gt;%\n  # Group By Conference\n  gt(groupname_col = \"conf\") %&gt;%\n  # Create Columns Labels\n  cols_label(\n    east = \"\",\n    record_1 = \"East\",\n    space_1 = \"\",\n    north = \"\",\n    record_2 = \"North\",\n    space_2 = \"\",\n    south = \"\",\n    record_3 = \"South\",\n    space_3 = \"\",\n    west = \"\",\n    record_4 = \"West\"\n  ) %&gt;%\n  # Align Column Title Text\n  tab_style(style = cell_text(align = \"center\"), locations = cells_column_labels()) %&gt;%\n  # Align Body Text\n  tab_style(style = cell_text(align = \"center\"), locations = cells_body()) %&gt;%\n  # Distinguish Division Rows\n  tab_style(\n    style = list(\n      cell_fill(color = \"#bcc0be\")),\n    locations = cells_body(rows = which(vibes$east %in% c(\"AFC\", \"NFC\")))) %&gt;%\n  # Add Team Logos\n  nflplotR::gt_nfl_logos(columns = c(\"east\", \"north\", \"south\", \"west\"))\n\n\n\n\n\n\n\n\n\nEast\n\n\nNorth\n\n\nSouth\n\n\nWest\n\n\n\n\nAFC\n\n\n\n10-7\n\n\n13-4\n\n\n13-4\n\n\n12-5\n\n\n\n9-8\n\n\n10-7\n\n\n11-6\n\n\n11-6\n\n\n\n7-10\n\n\n8-9\n\n\n9-8\n\n\n7-10\n\n\n\n3-14\n\n\n8-9\n\n\n9-8\n\n\n6-11\n\n\nNFC\n\n\n\n9-8\n\n\n12-5\n\n\n9-8\n\n\n11-6\n\n\n\n8-9\n\n\n11-6\n\n\n7-10\n\n\n10-7\n\n\n\n7-10\n\n\n9-8\n\n\n5-12\n\n\n9-8\n\n\n\n5-12\n\n\n6-11\n\n\n4-13\n\n\n4-13\n\n\n\n\n\n\n\nI like to refer to these as my “vibes-based” predictions because that’s really all they are. However, as a trained social scientist, I am well aware that “vibes” are not wholly informative, well-defined, nor do they contain a great deal of explanatory or predictive power. So, I thought, why not get my hands on more NFL data and try to work up a machine learning based approach? And that is what this blog is for.\n\nData/Feature Collection\nPrior to any fancy modeling, I need to collect some data to predict who wins each game. I want to start off with a major caveat here. I am doing this for fun and educational purposes. Undoubtedly, the predictors I have selected are not reflective of the most advanced analytics nor are they comprehensive. I chose the “lowest hanging fruit” for ease of access. This is probably going to hurt the predictive power of the models (models predict better with more predictive data), but again, humor me!\nOverall, I am using the following variables as predictors: whether a team is playing at home, QBR (quarterback rating), passing EPA (expected points added), rushing EPA, receiving EPA, forced fumbles, sacks, interceptions, and passes broken up. Because each prediction is at the game-level, I am using a differenced variable for computational ease (i.e., rather than include the home team’s passing EPA and the away team’s passing EPA in the same model, I just create a difference between the two and use this difference as a predictor for each team). Regardless of which method is used, the predictive performance remained the same after testing.\nThis selection leaves a lot to be desired. What about more advanced metrics like ELO? What about schematic data (like what type of offense the home team runs v. what type of defense the away team runs, etc.)? What about circumstantial data like whether a key player is out? These are all great things to add that will need to be included in the future! If you’re curious about the data collection syntax, check out the code fold below!\n\n\nCode\n# Load and Clean the QBR Data\nqbr &lt;- load_espn_qbr(\n  # Select the 2006-2023 Seasons as Training Data\n  seasons = 2006:2023,\n  # Aggregate at the Week-Level\n  summary_type = c(\"week\")) %&gt;%\n  # Exclude Playoff Games\n  filter(season_type == \"Regular\") %&gt;%\n  # Select Relevant Columns\n  select(c(team_abb, season, game_week, qbr_total, pts_added)) %&gt;%\n  # Create Cumulative Averages\n  group_by(season, team_abb) %&gt;%\n  mutate(\n    moving_qbr_mean = cumsum(qbr_total) / game_week,\n    moving_pts_added = cumsum(pts_added / game_week),\n    # Rename Washington for Merging\n    team_abb = ifelse(team_abb == \"WSH\", \"WAS\", team_abb))\n\n# Load and Clean Offensive Stats Data\noffensive &lt;- load_player_stats(\n  # Select the 2006-2023 Seasons as Training Data\n  seasons = 2006:2023,\n  # Filter to Offense\n  stat_type = \"offense\") %&gt;%\n  # Exclude the Playoffs\n  filter(season_type == \"REG\") %&gt;%\n  # Create Team-Level Stats\n  group_by(season, recent_team, week) %&gt;%\n  summarise(\n    passing_epa = sum(passing_epa, na.rm = TRUE),\n    rushing_epa = sum(rushing_epa, na.rm = TRUE),\n    receiving_epa = sum(receiving_epa, na.rm = TRUE)\n  ) %&gt;%\n  ungroup() %&gt;%\n  # Create Cumulative Averages\n  group_by(season, recent_team) %&gt;%\n  mutate(\n    moving_passing_epa = cumsum(passing_epa) / week,\n    moving_rushing_epa = cumsum(rushing_epa) / week,\n    moving_receiving_epa = cumsum(receiving_epa) / week) %&gt;%\n  # Keep Relevant Columns\n  select(season, recent_team, week, passing_epa, rushing_epa, receiving_epa, \n         moving_passing_epa, moving_rushing_epa, moving_receiving_epa) %&gt;%\n  # Convert Team Abbreviations to a More Standard Form for Merging\n  mutate(recent_team = ifelse(recent_team == \"LA\", \"LAR\", recent_team))\n\n# Load and Clean Defensive Stats Data\ndefensive &lt;- load_player_stats(\n  # Select the 2006-2023 Seasons as Training Data\n  seasons = 2006:2023,\n  # Filter to Defense\n  stat_type = \"defense\") %&gt;%\n  # Exclude Playoff Games\n  filter(season_type == \"REG\") %&gt;%\n  # Create Team-Level Stats\n  group_by(season, team, week) %&gt;%\n  summarise(\n    tackles = sum(def_tackles, na.rm = TRUE),\n    forced_fumbles = sum(def_fumbles_forced, na.rm = TRUE),\n    sacks = sum(def_sacks, na.rm = TRUE),\n    ints = sum(def_interceptions, na.rm = TRUE),\n    pass_broken = sum( def_pass_defended, na.rm = TRUE)\n  ) %&gt;%\n  ungroup() %&gt;%\n  # Create Cumulative Averages\n  group_by(season, team) %&gt;%\n  mutate(\n    moving_tackles = cumsum(tackles) / week,\n    moving_forced_fumbles = cumsum(forced_fumbles) / week,\n    moving_sacks = cumsum(sacks) / week,\n    moving_ints = cumsum(ints) / week,\n    moving_pass_broken = cumsum(pass_broken) / week) %&gt;%\n  # Keep Relevant Columns\n  select(season, team, week, tackles, forced_fumbles, sacks, ints, pass_broken, moving_tackles, \n         moving_forced_fumbles, moving_sacks, moving_ints, moving_pass_broken) %&gt;%\n  # Convert Team Abbreviations to a More Standard Form for Merging\n  mutate(team = ifelse(team == \"LA\", \"LAR\", team))\n\n# Load and Clean Schedules Data\nseasons &lt;- load_schedules(seasons = 2006:2023)\n\n# Convert the Data From Dyadic to Monadic\nseasons &lt;- clean_homeaway(seasons) %&gt;%\n  # Exclude Playoff Games\n  filter(game_type == \"REG\") %&gt;%\n  # Create a Home Team Variable\n  mutate(home = ifelse(location == \"home\", 1, 0),\n         # Create a Win Variable\n         win = ifelse(team_score &gt; opponent_score, 1, 0)) %&gt;%\n  # Keep Relevant Columns\n  select(game_id, season, week, team, opponent, home, win) %&gt;%\n   # Convert Team Abbreviations to a More Standard Form for Merging\n  mutate(team = ifelse(team == \"LA\", \"LAR\", team),\n         opponent = ifelse(opponent == \"LA\", \"LAR\", opponent))\n\n# Merge This Data\nmerged &lt;- inner_join(seasons, qbr, by = c(\"season\", \"team\" = \"team_abb\", \"week\" = \"game_week\")) %&gt;%\n  inner_join(offensive, by = c(\"season\", \"team\" = \"recent_team\", \"week\")) %&gt;%\n  inner_join(defensive, by = c(\"season\", \"team\", \"week\"))\n\nmerged &lt;- merged %&gt;%\n  group_by(game_id) %&gt;%\n  # Create Opponent Columns\n  # This Work Because Each Team Opponent Is In a Paired Set of Rows\n  # The Opponent Is Always the Second Observation\n  # Basically, This Just Reverses Cumulative Stats For Each Team Under a Different Name\n  mutate(opp_qbr = lead(moving_qbr_mean),\n         opp_qbr = ifelse(is.na(opp_qbr), lag(moving_qbr_mean), opp_qbr),\n         opp_pass_epa = lead(moving_passing_epa),\n         opp_pass_epa = ifelse(is.na(opp_pass_epa), \n                               lag(moving_passing_epa), opp_pass_epa),\n         opp_rushing_epa = lead(moving_rushing_epa),\n         opp_rushing_epa = ifelse(is.na(opp_rushing_epa), \n                                  lag(moving_rushing_epa), opp_rushing_epa),\n         opp_receiving_epa = lead(moving_receiving_epa),\n         opp_receiving_epa = ifelse(is.na(opp_receiving_epa), \n                                    lag(moving_receiving_epa), opp_receiving_epa),\n         opp_tackles = lead(moving_tackles),\n         opp_tackles = ifelse(is.na(opp_tackles), \n                              lag(moving_tackles), opp_tackles),\n         opp_forced_fumbles = lead(moving_forced_fumbles),\n         opp_forced_fumbles = ifelse(is.na(opp_forced_fumbles), \n                                     lag(moving_forced_fumbles), opp_forced_fumbles),\n         opp_sacks = lead(moving_sacks),\n         opp_sacks = ifelse(is.na(opp_sacks), \n                            lag(moving_sacks), opp_sacks),\n         opp_ints = lead(moving_ints),\n         opp_ints = ifelse(is.na(opp_ints), \n                           lag(moving_ints), opp_ints),\n         opp_pass_broken = lead(moving_pass_broken),\n         opp_pass_broken = ifelse(is.na(opp_pass_broken), \n                                  lag(moving_pass_broken), opp_pass_broken)\n         ) %&gt;%\n  # Create Differenced Columns\n  mutate(\n    qbr_diff = moving_qbr_mean - opp_qbr,\n    pass_epa_diff = moving_passing_epa - opp_pass_epa,\n    rushing_epa_diff = moving_rushing_epa - opp_rushing_epa,\n    receiving_epa_diff = moving_receiving_epa - opp_receiving_epa,\n    tackles_diff = moving_tackles - opp_tackles,\n    forced_fumbles_diff = moving_forced_fumbles - opp_forced_fumbles,\n    sacks_diff = moving_sacks - opp_sacks,\n    ints_diff = moving_ints - opp_ints,\n    pass_broken_diff = moving_pass_broken - opp_pass_broken\n  ) %&gt;%\n  # Make the Outcome Column Suitable for Classification\n  mutate(win = factor(win, levels = c(0, 1), labels = c(\"Lose\", \"Win\"))) %&gt;%\n  # Drop NAs Because They Will Create Problems\n  drop_na()\n\n\n\n\nMachine Learning Algorithms Limitations\nOkay, now onto the actual machine learning algorithms that will be used. Again, nothing super fancy here. In the interest of keeping things simple at first, I chose to just explore how predictive accuracy fluctuates between four popular ML algorithms (logistic regression… which makes me cringe to refer to it as “ML”, random forest, support vector machine (SVM), and XGBoost). For those curious, I did engage in hyper-parameter tuning, but, no amount of tuning really improved the model results that much, and I felt that, in the interest of simplicity and computational time, it would be best to just include four basic ML algorithms for now.\n\n# For Reproducibility\nset.seed(1234)\n\n# Establish a Cross-Validation Method\ncv_method &lt;- trainControl(method = \"cv\",\n                          number = 10,\n                          classProbs = TRUE,\n                          summaryFunction = twoClassSummary)\n\n# Fit Models\n# Logistic Regression\nlog_fit &lt;- train(win ~ home + qbr_diff + pass_epa_diff + rushing_epa_diff + receiving_epa_diff +\n                       forced_fumbles_diff + sacks_diff + ints_diff + pass_broken_diff, \n                  data = merged,\n                  method = \"glm\",\n                  family = \"binomial\",\n                  trControl = cv_method,\n                  metric = \"ROC\")\n\n# Save Model Results So I Don't Have to Re-Train Every Time\nsaveRDS(log_fit, \"data-and-analysis/log_fit_model.rds\")\n\n# Random Forest\nrf_fit &lt;- train(win ~ home + qbr_diff + pass_epa_diff + rushing_epa_diff + receiving_epa_diff +\n                       forced_fumbles_diff + sacks_diff + ints_diff + pass_broken_diff, \n                  data = merged,\n                  method = \"rf\",\n                  trControl = cv_method,\n                  metric = \"ROC\")\n\nsaveRDS(rf_fit, \"data-and-analysis/rf_fit_model.rds\")\n\n# Support Vector Machine\nsv_fit &lt;- train(win ~ home + qbr_diff + pass_epa_diff + rushing_epa_diff + receiving_epa_diff +\n                       forced_fumbles_diff + sacks_diff + ints_diff + pass_broken_diff, \n                data = merged,\n                method = \"svmLinear\",\n                trControl = cv_method,\n                metric = \"ROC\")\n\nsaveRDS(sv_fit, \"data-and-analysis/sv_fit_model.rds\")\n\n# XGBoost\nxgb_fit &lt;- train(win ~ qbr_diff + pass_epa_diff + rushing_epa_diff + receiving_epa_diff +\n                        forced_fumbles_diff + sacks_diff + ints_diff + pass_broken_diff, \n                 data = merged,\n                 method = \"xgbTree\",\n                 trControl = cv_method,\n                 metric = \"ROC\")\n\nsaveRDS(xgb_fit, \"data-and-analysis/xgb_fit_model.rds\")\n\n# Store the Predictive Accuracy Results in a Table\nresults &lt;- tibble(\n  Model = c(\"Logistic Regression\", \"Random Forest\", \"SVM\", \"XGBoost\"),\n  # Store ROC Metrics\n  ROC = c(\n    # which.max() Doesn't Do Anything Here, But It Would If I Had Tons of Different\n    # Models for Each Model Type. It Would Select the Model with the Highest Predictive\n    # Power. Not Helpful Here Since I Am Only Running One Model of Each Type, But It's\n    # A Useful Reference That I Want to Keep for the Future\n    log_fit$results[which.max(log_fit$results$ROC), \"ROC\"], \n    rf_fit$results[which.max(rf_fit$results$ROC), \"ROC\"], \n    sv_fit$results[which.max(sv_fit$results$ROC), \"ROC\"], \n    xgb_fit$results[which.max(xgb_fit$results$ROC), \"ROC\"]\n  ),\n  # Store Accurate Predictions Percentage\n  Accuracy = c(\n    (log_fit$results$Spec[which.max(log_fit$results$ROC)] + \n       log_fit$results$Sens[which.max(log_fit$results$ROC)]) / 2,\n    (rf_fit$results$Spec[which.max(rf_fit$results$ROC)] + \n       rf_fit$results$Sens[which.max(rf_fit$results$ROC)]) / 2,\n    (sv_fit$results$Spec[which.max(sv_fit$results$ROC)] + \n       sv_fit$results$Sens[which.max(sv_fit$results$ROC)]) / 2,\n    (xgb_fit$results$Spec[which.max(xgb_fit$results$ROC)] + \n       xgb_fit$results$Sens[which.max(xgb_fit$results$ROC)]) / 2\n  )\n)\n\n\n\nModel Evaluation\nSo, how did these models fair? Eh… not great, as you can see below\n\nresults    \n\n# A tibble: 4 × 3\n  Model                 ROC Accuracy\n  &lt;chr&gt;               &lt;dbl&gt;    &lt;dbl&gt;\n1 Logistic Regression 0.798    0.718\n2 Random Forest       0.785    0.715\n3 SVM                 0.798    0.717\n4 XGBoost             0.792    0.720\n\n\n70-ish% isn’t terrible. It’s better than a coin flip. But really, how impressive is that? Just off of vibes, anyone who sort of knows the NFL will probably get 70% of game predictions right. Honestly, you might even do better if you just follow Vegas and predict the winner based on who is the betting favorite to win. That’s not very satisfying is it? A truly impressive ML algorithm should be able to predict not only when a favorite wins but also when a favorite does not win. These very crude models don’t appear to have that predictive complexity. Why is that the case? I can think of three reasons.\nFirst, as already stated, better predictors would go a long way. The good news is that this is probably the easiest fix. I just need to put the time in to research and collect the data.\nSecond, there may have been more complex hyper-parameter tuning I could have engaged with. Given that I come from a causal inference background, machine learning is not my specialty, and I do not have a wealth of information lodged in my head about all the tuning options for each ML algorithm. However, I’m sure that predictive gains could be there with some hyper-parameter tuning.\nLastly, I think that a different modeling approach could go a long way. And, to demonstrate my reasoning, let’s look at how my trained models are predicting the outcomes of the upcoming Week 2 games.\n\n\nCode\n# To Do This, I Need to Load In 2024 \"Test\" Data That the Model Was Not Trained On\n# This Is Just a Repeat of the Prior Data Cleaning Process for the Training Data\n# So I Don't Annotate Code Here\nqbr_2024 &lt;- load_espn_qbr(\n  seasons = 2024,\n  summary_type = c(\"week\")) %&gt;%\n  filter(season_type == \"Regular\") %&gt;%\n  select(c(team_abb, season, game_week, qbr_total, pts_added)) %&gt;%\n  group_by(season, team_abb) %&gt;%\n  mutate(\n    moving_qbr_mean = cumsum(qbr_total) / game_week,\n    moving_pts_added = cumsum(pts_added / game_week),\n    # Rename Washington for Merging\n    team_abb = ifelse(team_abb == \"WSH\", \"WAS\", team_abb)) %&gt;%\n  # Keep Last Week's Data\n  filter(game_week == 1) %&gt;%\n  # Convert Lagged Game Week to Current Since We're Using Last Week's Predictors\n  mutate(game_week = 2)\n\noffensive_2024 &lt;- load_player_stats(\n  seasons = 2024,\n  stat_type = \"offense\") %&gt;%\n  filter(season_type == \"REG\") %&gt;%\n  group_by(season, recent_team, week) %&gt;%\n  summarise(\n    passing_epa = sum(passing_epa, na.rm = TRUE),\n    rushing_epa = sum(rushing_epa, na.rm = TRUE),\n    receiving_epa = sum(receiving_epa, na.rm = TRUE)\n  ) %&gt;%\n  ungroup() %&gt;%\n  group_by(season, recent_team) %&gt;%\n  mutate(\n    moving_passing_epa = cumsum(passing_epa) / week,\n    moving_rushing_epa = cumsum(rushing_epa) / week,\n    moving_receiving_epa = cumsum(receiving_epa) / week) %&gt;%\n  select(season, recent_team, week, passing_epa, rushing_epa, receiving_epa, moving_passing_epa,\n         moving_rushing_epa, moving_receiving_epa) %&gt;%\n  filter(week == 1) %&gt;%\n  mutate(week = 2) %&gt;%\n  mutate(recent_team = ifelse(recent_team == \"LA\", \"LAR\", recent_team))\n\ndefensive_2024 &lt;- load_player_stats(\n  seasons = 2024,\n  stat_type = \"defense\") %&gt;%\n  filter(season_type == \"REG\") %&gt;%\n  group_by(season, team, week) %&gt;%\n  summarise(\n    tackles = sum(def_tackles, na.rm = TRUE),\n    forced_fumbles = sum(def_fumbles_forced, na.rm = TRUE),\n    sacks = sum(def_sacks, na.rm = TRUE),\n    ints = sum(def_interceptions, na.rm = TRUE),\n    pass_broken = sum( def_pass_defended, na.rm = TRUE)\n  ) %&gt;%\n  ungroup() %&gt;%\n  group_by(season, team) %&gt;%\n  mutate(\n    moving_tackles = cumsum(tackles) / week,\n    moving_forced_fumbles = cumsum(forced_fumbles) / week,\n    moving_sacks = cumsum(sacks) / week,\n    moving_ints = cumsum(ints) / week,\n    moving_pass_broken = cumsum(pass_broken) / week) %&gt;%\n  select(season, team, week, tackles, forced_fumbles, sacks, ints, pass_broken, moving_tackles, \n         moving_forced_fumbles, moving_sacks, moving_ints, moving_pass_broken) %&gt;%\n  filter(week == 1) %&gt;%\n  mutate(week = 2) %&gt;%\n  mutate(team = ifelse(team == \"LA\", \"LAR\", team))\n\nseason_2024 &lt;- load_schedules(seasons = 2024)\n\n# Convert the Data From Dyadic to Monadic\nseason_2024 &lt;- clean_homeaway(season_2024) %&gt;%\n  filter(game_type == \"REG\") %&gt;%\n  mutate(home = ifelse(location == \"home\", 1, 0),\n         win = ifelse(team_score &gt; opponent_score, 1, 0)) %&gt;%\n  select(game_id, season, week, team, opponent, home, win) %&gt;%\n  filter(week == 2) %&gt;%\n  mutate(team = ifelse(team == \"LA\", \"LAR\", team),\n         opponent = ifelse(opponent == \"LA\", \"LAR\", opponent))\n\nmerged_2024 &lt;- inner_join(season_2024, qbr_2024, \n                          by = c(\"team\" = \"team_abb\", \"week\" = \"game_week\", \"season\")) %&gt;%\n  inner_join(offensive_2024, by = c(\"team\" = \"recent_team\", \"week\", \"season\")) %&gt;%\n  inner_join(defensive_2024, by = c(\"team\", \"week\", \"season\")) %&gt;%\n  group_by(game_id) %&gt;%\n  mutate(opp_qbr = lead(moving_qbr_mean),\n         opp_qbr = ifelse(is.na(opp_qbr), \n                          lag(moving_qbr_mean), opp_qbr),\n         opp_pass_epa = lead(moving_passing_epa),\n         opp_pass_epa = ifelse(is.na(opp_pass_epa), \n                               lag(moving_passing_epa), opp_pass_epa),\n         opp_rushing_epa = lead(moving_rushing_epa),\n         opp_rushing_epa = ifelse(is.na(opp_rushing_epa), \n                                  lag(moving_rushing_epa), opp_rushing_epa),\n         opp_receiving_epa = lead(moving_receiving_epa),\n         opp_receiving_epa = ifelse(is.na(opp_receiving_epa), \n                                    lag(moving_receiving_epa), opp_receiving_epa),\n         opp_tackles = lead(moving_tackles),\n         opp_tackles = ifelse(is.na(opp_tackles), \n                              lag(moving_tackles), opp_tackles),\n         opp_forced_fumbles = lead(moving_forced_fumbles),\n         opp_forced_fumbles = ifelse(is.na(opp_forced_fumbles), \n                                     lag(moving_forced_fumbles), opp_forced_fumbles),\n         opp_sacks = lead(moving_sacks),\n         opp_sacks = ifelse(is.na(opp_sacks), \n                            lag(moving_sacks), opp_sacks),\n         opp_ints = lead(moving_ints),\n         opp_ints = ifelse(is.na(opp_ints), \n                           lag(moving_ints), opp_ints),\n         opp_pass_broken = lead(moving_pass_broken),\n         opp_pass_broken = ifelse(is.na(opp_pass_broken), \n                                  lag(moving_pass_broken), opp_pass_broken)\n         ) %&gt;%\n  mutate(\n    qbr_diff = moving_qbr_mean - opp_qbr,\n    pass_epa_diff = moving_passing_epa - opp_pass_epa,\n    rushing_epa_diff = moving_rushing_epa - opp_rushing_epa,\n    receiving_epa_diff = moving_receiving_epa - opp_receiving_epa,\n    tackles_diff = moving_tackles - opp_tackles,\n    forced_fumbles_diff = moving_forced_fumbles - opp_forced_fumbles,\n    sacks_diff = moving_sacks - opp_sacks,\n    ints_diff = moving_ints - opp_ints,\n    pass_broken_diff = moving_pass_broken - opp_pass_broken\n  ) %&gt;%\n  mutate(win = factor(win, levels = c(0, 1), labels = c(\"Lose\", \"Win\"))) %&gt;%\n  # Remove Games with Missing Feature Data\n  filter(!is.na(opp_qbr))\n\n\n\nlog_preds &lt;- predict(log_fit, merged_2024, type = \"prob\")[,2]\nrf_preds &lt;- predict(rf_fit, merged_2024, type = \"prob\")[,2]\nsv_preds &lt;- predict(sv_fit, merged_2024, type = \"prob\")[,2]\nxgb_preds &lt;- predict(xgb_fit, merged_2024, type = \"prob\")[,2]\n\n# Combine Predictions Into a Data Frame\npredictions &lt;- tibble(\n  Team = merged_2024$team,\n  Game_ID = merged_2024$game_id,\n  Logistic_Regression = paste0(round(log_preds * 100, 2), \"%\"),\n  Random_Forest = paste0(round(rf_preds * 100, 2), \"%\"),\n  SVM = paste0(round(sv_preds * 100, 2), \"%\"),\n  XGBoost = paste0(round(xgb_preds * 100, 2), \"%\")\n)\n\n# Load in NFL Logo Data to Make Cool Tables with {gt}\nteam_logos &lt;- nflfastR::teams_colors_logos %&gt;% \n  select(team_abbr, team_logo_espn)\n\nweek2_preds &lt;- predictions %&gt;%\n  left_join(team_logos, by = c(\"Team\" = \"team_abbr\")) %&gt;%\n  select(Team, team_logo_espn, Game_ID, Logistic_Regression, Random_Forest, SVM, XGBoost)\n\n# I Want to Create a Table That Has Teams Who Are Playing Each Other In the Same Row\n# To Do This, I'll Need to Reshape the Data\nreshaped_week2 &lt;- week2_preds %&gt;%\n    group_by(Game_ID) %&gt;%\n    summarise(\n        Team_1 = first(Team),\n        Team_1_Logo = first(team_logo_espn),\n        Team_1_Logistic_Regression = first(Logistic_Regression),\n        Team_1_Random_Forest = first(Random_Forest),\n        Team_1_SVM = first(SVM),\n        Team_1_XGBoost = first(XGBoost),\n        Team_2 = last(Team),\n        Team_2_Logo = last(team_logo_espn),\n        Team_2_Logistic_Regression = last(Logistic_Regression),\n        Team_2_Random_Forest = last(Random_Forest),\n        Team_2_SVM = last(SVM),\n        Team_2_XGBoost = last(XGBoost)\n    )\n\n# Now, I Can Use {gt} To Make a Nice Table\nreshaped_week2 %&gt;%\n  # Start a {gt} Table\n    gt() %&gt;%\n  # Modify Logo Settings\n    text_transform(\n        locations = cells_body(vars(Team_1_Logo, Team_2_Logo)),\n        fn = function(x) {\n            web_image(url = x, height = 40)  # Adjust the height as needed\n        }\n    ) %&gt;%\n  # Remove Columns From the Table\n    cols_hide(\n        columns = c(Game_ID, Team_1, Team_2)\n    ) %&gt;%\n  # Label the Columns\n    cols_label(\n        Team_1_Logo = \"Home\",\n        Team_1_Logistic_Regression = \"Logit\",\n        Team_1_Random_Forest = \"Random Forest\",\n        Team_1_SVM = \"SVM\",\n        Team_1_XGBoost = \"XGBoost\",\n        Team_2_Logo = \"Away\",\n        Team_2_Logistic_Regression = \"Logit\",\n        Team_2_Random_Forest = \"Random Forest\",\n        Team_2_SVM = \"SVM\",\n        Team_2_XGBoost = \"XGBoost\"\n    ) %&gt;%\n  # Create a Title for the Table\n    tab_header(\n        title = \"Predicted Win Probability by Game and Model\"\n    ) %&gt;%\n  # Column Formatting\n    tab_style(\n        style = list(\n            cell_text(weight = \"bold\")\n        ),\n        locations = cells_column_labels(everything())  \n    ) %&gt;%\n    cols_align(\n        align = \"center\",\n        columns = everything()\n    ) %&gt;%\n  # Adjust Column Widths \n    cols_width(\n        Team_1_Logo ~ px(100),  # Adjust as needed\n        Team_1_Logistic_Regression ~ px(85),  # Adjust as needed\n        Team_1_Random_Forest ~ px(85),  # Adjust as needed\n        Team_1_SVM ~ px(85),  # Adjust as needed\n        Team_1_XGBoost ~ px(85),  # Adjust as needed\n        Team_2_Logo ~ px(100),  # Adjust as needed\n        Team_2_Logistic_Regression ~ px(85),  # Adjust as needed\n        Team_2_Random_Forest ~ px(85),  # Adjust as needed\n        Team_2_SVM ~ px(85),  # Adjust as needed\n        Team_2_XGBoost ~ px(85)  # Adjust as needed\n    )\n\nWarning: Since gt v0.3.0, `columns = vars(...)` has been deprecated.\n• Please use `columns = c(...)` instead.\n\n\n\n\n\n\n\n\nPredicted Win Probability by Game and Model\n\n\nHome\nLogit\nRandom Forest\nSVM\nXGBoost\nAway\nLogit\nRandom Forest\nSVM\nXGBoost\n\n\n\n\n\n98.21%\n91.6%\n97.95%\n93.03%\n\n1.66%\n6%\n1.92%\n4.92%\n\n\n\n15.42%\n12.8%\n15.35%\n7.54%\n\n83.61%\n85.2%\n83.75%\n85.47%\n\n\n\n93.46%\n91.8%\n93.25%\n83.63%\n\n6.11%\n8%\n6.34%\n14.28%\n\n\n\n89.19%\n80.2%\n88.54%\n83.76%\n\n10.13%\n19.6%\n10.79%\n11.25%\n\n\n\n90.95%\n89.8%\n91.02%\n79.28%\n\n8.47%\n11.4%\n8.44%\n14.85%\n\n\n\n79.87%\n71%\n76.22%\n64.98%\n\n18.99%\n31.2%\n22.57%\n30.99%\n\n\n\n2.47%\n4.2%\n2.68%\n2.32%\n\n97.35%\n94.4%\n97.14%\n97.02%\n\n\n\n45.8%\n46%\n47.71%\n31.15%\n\n52.4%\n45.4%\n50.59%\n62.23%\n\n\n\n96.01%\n69.8%\n96.07%\n85.33%\n\n3.72%\n26.4%\n3.68%\n13.92%\n\n\n\n21.73%\n19.6%\n20.61%\n14.24%\n\n77.01%\n83%\n78.26%\n81.43%\n\n\n\n92.83%\n87.6%\n92.69%\n78.2%\n\n6.7%\n14%\n6.87%\n17.81%\n\n\n\n10.32%\n24.6%\n10.73%\n13.51%\n\n88.98%\n73.2%\n88.6%\n82.98%\n\n\n\n8.04%\n9.6%\n8.97%\n6.5%\n\n91.41%\n86.4%\n90.46%\n91.34%\n\n\n\n38.9%\n49.2%\n40.41%\n51.87%\n\n59.36%\n48.6%\n57.94%\n54.9%\n\n\n\n92.36%\n88.2%\n91.65%\n81.26%\n\n7.15%\n15.4%\n7.85%\n17.28%\n\n\n\n17.74%\n16.4%\n17.38%\n20.76%\n\n81.18%\n83.6%\n81.63%\n77.53%\n\n\n\n\n\n\n\nAs you can see, there are some wacky predictions for week 2 game outcomes. The Saints are massive favorites over the Cowboys? The Vikings are massive favorites to the 49ers? What?! Well, the answer is not very surprising. In predicting week 2 games, we use all data from prior weeks in the season. In week 2, this means we only have one week of data to draw from. That means that, if a team does exceptionally well in week 1, this great performance is going to impact predictions for week 2. Both the Saints and Vikings had great offensive and defensive performances in week 1, which explains why this model is so bullish on these teams. It stands to reason that such model predictions would probably not show up later on in the season.\nThis gets to my third point on my model performance. When a model is solely impacted by the data, and the available data is not incredibly informative, we are going to get predictions that are pretty counter-intuitive. Basically, please do not put any money down on the Saints or Vikings outright winning this week! I think something to explore in the future would be Bayesian methods to incorporate prior information (i.e. the Cowboys perform well in the early regular season, the 49ers are really good, etc.) that can stabilize the existing limited data with prior knowledge. As the causal inference folks are quick to say… data are dumb, especially when such limited data. Especially early in the season, Bayesian methods may prove really helpful in preventing predictions that are generated from an outlier or two.\nOut of curiosity, I wanted to check how well the model was able to predict the outcome of games by each week in the season. The expectation would be that the model becomes more accurate as the season goes on (we get more information). Below is a plot of the average percent of games whose outcomes are correctly predicted each week from the 2006-2023 seasons.\n\n\nCode\nmerged$log_preds &lt;- predict(log_fit, merged, type = \"prob\")[,2]\nmerged$rf_preds &lt;- predict(rf_fit, merged, type = \"prob\")[,2]\nmerged$rf_preds2 &lt;- ifelse(merged$rf_preds &gt;= 0.5, 1, 0)\nmerged$sv_preds &lt;- predict(sv_fit, merged, type = \"prob\")[,2]\nmerged$xgb_preds &lt;- predict(xgb_fit, merged, type = \"prob\")[,2]\n\nmerged %&gt;%\n  filter(week != 1) %&gt;%\n  mutate(log_class = ifelse(log_preds &gt;= 0.5, \"Win\", \"Lose\"),\n         rf_class = ifelse(rf_preds &gt;= 0.5, \"Win\", \"Lose\"),\n         sv_class = ifelse(sv_preds &gt;= 0.5, \"Win\", \"Lose\"),\n         xgb_class = ifelse(xgb_preds &gt;= 0.5, \"Win\", \"Lose\")) %&gt;%\n  mutate(log_right = ifelse(win == log_class, 1, 0),\n         rf_right = ifelse(win == rf_class, 1, 0),\n         sv_right = ifelse(win == sv_class, 1, 0),\n         xgb_right = ifelse(win == xgb_class, 1, 0)) %&gt;%\n  # By Week, Calculate Predictive Accuracy\n  group_by(week) %&gt;%\n  summarise(log_week_right = mean(log_right),\n         rf_week_right = mean(rf_right),\n         sv_week_right = mean(sv_right),\n         xgb_week_right = mean(xgb_right)) %&gt;%\n  # Pivot to Color by Model Type\n  pivot_longer(cols = starts_with(\"log_week_right\"):starts_with(\"xgb_week_right\"),\n               names_to = \"Model\",\n               values_to = \"Accuracy\") %&gt;%\n  mutate(Model = recode(Model,\n                        log_week_right = \"Logistic Regression\",\n                        rf_week_right = \"Random Forest\",\n                        sv_week_right = \"SVM\",\n                        xgb_week_right = \"XGBoost\")) %&gt;%\n  ggplot(aes(x = week, y = Accuracy, color = Model)) +\n  geom_line() +\n  geom_point() +\n  scale_x_continuous(breaks = 2:18) +\n  scale_y_continuous(breaks = seq(0.6, 1, by = 0.05),\n                     labels = scales::percent) + \n  scale_color_manual(\n    values = c(\"Logistic Regression\" = \"#e31837\", \n               \"Random Forest\" = \"#003594\", \n               \"SVM\" = \"#041e42\", \n               \"XGBoost\" = \"#ffb81c\")\n  ) +\n  labs(title = \"Week 1 is Excluded Due to Lack of In-Season Data\",\n       x = \"Week\",\n       y = \"Average Predictive Accuracy\",\n       color = \"Model\") +\n  blog_theme() + \n  theme(\n    plot.title = element_text(face = \"bold\"), \n    legend.title = element_text(face = \"bold\")  \n  )\n\n\n\n\n\nAverage In-Sample Predictive Accuracy by Model Over NFL Weeks\n\n\n\n\nLike a lot of things in the world of data science, when you plot the data expecting answers, you actually just get a lot more questions. While these report in-sample results (in contrast to cross-validated out-of-sample accuracy metrics… so take these accuracy numbers with a grain of salt), I still would have expected an upward trend over the NFL season, but nope! And there’s other interesting things as well… like how three of the models have a crazy dip in predictive performance in week 10. Don’t really know what that’s about. Well, even if the plot doesn’t support my diagnosis and prescription all that well, I’m convinced that pursuing a modeling strategy that incorporates prior information and domain knowledge would probably result in less “Saints over Cowboys” and “Vikings over 49ers” predictions.\n\n\nSetting Up My Workflow\nLastly, I want to document how I’m going to go about creating predictions every week. After all, I’ve collected data and trained some models, but there is no magic button I can press that will just sequentially update everything every week throughout the remainder of the NFL season. The following code chunk walks through my “workflow” so to speak.\n\n# Establish Global Week Parameters So I Don't Have to Update Every Data Set Individually\nlast_week &lt;- 1\nthis_week &lt;- 2\n\n# Load and Clean Updated Predictor Data\nqbr_2024 &lt;- load_espn_qbr(\n  seasons = 2024,\n  summary_type = c(\"week\")) %&gt;%\n  filter(season_type == \"Regular\") %&gt;%\n  select(c(team_abb, season, game_week, qbr_total, pts_added)) %&gt;%\n  group_by(season, team_abb) %&gt;%\n  mutate(\n    moving_qbr_mean = cumsum(qbr_total) / game_week,\n    moving_pts_added = cumsum(pts_added / game_week),\n    # Rename Washington for Merging\n    team_abb = ifelse(team_abb == \"WSH\", \"WAS\", team_abb)) %&gt;%\n  filter(game_week == last_week) %&gt;%\n  mutate(game_week = this_week)\n\noffensive_2024 &lt;- load_player_stats(\n  seasons = 2024,\n  stat_type = \"offense\") %&gt;%\n  filter(season_type == \"REG\") %&gt;%\n  group_by(season, recent_team, week) %&gt;%\n  summarise(\n    passing_epa = sum(passing_epa, na.rm = TRUE),\n    rushing_epa = sum(rushing_epa, na.rm = TRUE),\n    receiving_epa = sum(receiving_epa, na.rm = TRUE)\n  ) %&gt;%\n  ungroup() %&gt;%\n  group_by(season, recent_team) %&gt;%\n  mutate(\n    moving_passing_epa = cumsum(passing_epa) / week,\n    moving_rushing_epa = cumsum(rushing_epa) / week,\n    moving_receiving_epa = cumsum(receiving_epa) / week) %&gt;%\n  select(season, recent_team, week, passing_epa, rushing_epa, receiving_epa, moving_passing_epa, \n         moving_rushing_epa, moving_receiving_epa) %&gt;%\n  filter(week == last_week) %&gt;%\n  mutate(week = this_week) %&gt;%\n  mutate(recent_team = ifelse(recent_team == \"LA\", \"LAR\", recent_team))\n\ndefensive_2024 &lt;- load_player_stats(\n  seasons = 2024,\n  stat_type = \"defense\") %&gt;%\n  filter(season_type == \"REG\") %&gt;%\n  group_by(season, team, week) %&gt;%\n  summarise(\n    tackles = sum(def_tackles, na.rm = TRUE),\n    forced_fumbles = sum(def_fumbles_forced, na.rm = TRUE),\n    sacks = sum(def_sacks, na.rm = TRUE),\n    ints = sum(def_interceptions, na.rm = TRUE),\n    pass_broken = sum( def_pass_defended, na.rm = TRUE)\n  ) %&gt;%\n  ungroup() %&gt;%\n  group_by(season, team) %&gt;%\n  mutate(\n    moving_tackles = cumsum(tackles) / week,\n    moving_forced_fumbles = cumsum(forced_fumbles) / week,\n    moving_sacks = cumsum(sacks) / week,\n    moving_ints = cumsum(ints) / week,\n    moving_pass_broken = cumsum(pass_broken) / week) %&gt;%\n  select(season, team, week, tackles, forced_fumbles, sacks, ints, pass_broken, moving_tackles, \n         moving_forced_fumbles, moving_sacks, moving_ints, moving_pass_broken) %&gt;%\n  filter(week == last_week) %&gt;%\n  mutate(week = this_week) %&gt;%\n  mutate(team = ifelse(team == \"LA\", \"LAR\", team))\n\nseason_2024 &lt;- load_schedules(seasons = 2024)\n\nseason_2024 &lt;- clean_homeaway(season_2024) %&gt;%\n  filter(game_type == \"REG\") %&gt;%\n  mutate(home = ifelse(location == \"home\", 1, 0),\n         win = ifelse(team_score &gt; opponent_score, 1, 0)) %&gt;%\n  select(game_id, season, week, team, opponent, home, win) %&gt;%\n  filter(week == this_week) %&gt;%\n  mutate(team = ifelse(team == \"LA\", \"LAR\", team),\n         opponent = ifelse(opponent == \"LA\", \"LAR\", opponent))\n\nmerged_2024 &lt;- inner_join(season_2024, qbr_2024, \n                          by = c(\"team\" = \"team_abb\", \"week\" = \"game_week\", \"season\")) %&gt;%\n  inner_join(offensive_2024, by = c(\"team\" = \"recent_team\", \"week\", \"season\")) %&gt;%\n  inner_join(defensive_2024, by = c(\"team\", \"week\", \"season\")) %&gt;%\n  group_by(game_id) %&gt;%\n  mutate(opp_qbr = lead(moving_qbr_mean),\n         opp_qbr = ifelse(is.na(opp_qbr), \n                          lag(moving_qbr_mean), opp_qbr),\n         opp_pass_epa = lead(moving_passing_epa),\n         opp_pass_epa = ifelse(is.na(opp_pass_epa), \n                               lag(moving_passing_epa), opp_pass_epa),\n         opp_rushing_epa = lead(moving_rushing_epa),\n         opp_rushing_epa = ifelse(is.na(opp_rushing_epa), \n                                  lag(moving_rushing_epa), opp_rushing_epa),\n         opp_receiving_epa = lead(moving_receiving_epa),\n         opp_receiving_epa = ifelse(is.na(opp_receiving_epa), \n                                    lag(moving_receiving_epa), opp_receiving_epa),\n         opp_tackles = lead(moving_tackles),\n         opp_tackles = ifelse(is.na(opp_tackles), \n                              lag(moving_tackles), opp_tackles),\n         opp_forced_fumbles = lead(moving_forced_fumbles),\n         opp_forced_fumbles = ifelse(is.na(opp_forced_fumbles), \n                                     lag(moving_forced_fumbles), opp_forced_fumbles),\n         opp_sacks = lead(moving_sacks),\n         opp_sacks = ifelse(is.na(opp_sacks), \n                            lag(moving_sacks), opp_sacks),\n         opp_ints = lead(moving_ints),\n         opp_ints = ifelse(is.na(opp_ints), \n                           lag(moving_ints), opp_ints),\n         opp_pass_broken = lead(moving_pass_broken),\n         opp_pass_broken = ifelse(is.na(opp_pass_broken), \n                                  lag(moving_pass_broken), opp_pass_broken)\n         ) %&gt;%\n  mutate(\n    qbr_diff = moving_qbr_mean - opp_qbr,\n    pass_epa_diff = moving_passing_epa - opp_pass_epa,\n    rushing_epa_diff = moving_rushing_epa - opp_rushing_epa,\n    receiving_epa_diff = moving_receiving_epa - opp_receiving_epa,\n    tackles_diff = moving_tackles - opp_tackles,\n    forced_fumbles_diff = moving_forced_fumbles - opp_forced_fumbles,\n    sacks_diff = moving_sacks - opp_sacks,\n    ints_diff = moving_ints - opp_ints,\n    pass_broken_diff = moving_pass_broken - opp_pass_broken\n  ) %&gt;%\n  mutate(win = factor(win, levels = c(0, 1), labels = c(\"Lose\", \"Win\"))) %&gt;%\n  filter(!is.na(qbr_diff))\n\n# Load Trained Models\nlog_fit &lt;- readRDS(\"data-and-analysis/log_fit_model.rds\")\nrf_fit &lt;- readRDS(\"data-and-analysis/rf_fit_model.rds\")\nsv_fit &lt;- readRDS(\"data-and-analysis/sv_fit_model.rds\")\nxgb_fit &lt;- readRDS(\"data-and-analysis/xgb_fit_model.rds\")\n\n# Create a Data Frame with Model Predictions\nlog_preds &lt;- predict(log_fit, merged_2024, type = \"prob\")[,2]\nrf_preds &lt;- predict(rf_fit, merged_2024, type = \"prob\")[,2]\nsv_preds &lt;- predict(sv_fit, merged_2024, type = \"prob\")[,2]\nxgb_preds &lt;- predict(xgb_fit, merged_2024, type = \"prob\")[,2]\n\npredictions &lt;- tibble(\n  Team = merged_2024$team,\n  Game_ID = merged_2024$game_id,\n  week = merged_2024$week,\n  Logistic_Regression = paste0(round(log_preds * 100, 2), \"%\"),\n  Random_Forest = paste0(round(rf_preds * 100, 2), \"%\"),\n  SVM = paste0(round(sv_preds * 100, 2), \"%\"),\n  XGBoost = paste0(round(xgb_preds * 100, 2), \"%\")\n)\n\n# Create a Back-Up Spreadsheet Before Updating\nglobal_preds &lt;- read_excel(\"data-and-analysis/nfl_2024_global_preds.xlsx\")\nwrite_xlsx(global_preds, \"data-and-analysis/nfl_2024_global_preds_backup.xlsx\")\n\n# Add Model Predictions for This Week to Season-Level Spreadsheet\nupdated_preds &lt;- inner_join(predictions, global_preds, by = c(\n  \"Team\", \"Game_ID\", \"week\", \"Logistic_Regression\", \"Random_Forest\", \"SVM\", \"XGBoost\"\n))\nwrite_xlsx(updated_preds, \"data-and-analysis/nfl_2024_global_preds.xlsx\")\n\nNow that I’ve created this, I hope that clicking “Run” now serves as the magical button that I just have to click and I get new predictions every week. We will see how this goes, as I’m sure there’s some bug/dependency I’m missing."
  },
  {
    "objectID": "blog/2024/dynamic-causal-inference/index.html",
    "href": "blog/2024/dynamic-causal-inference/index.html",
    "title": "An Introduction to Dynamic Causal Inference",
    "section": "",
    "text": "Code\n# Load Libraries\npacman::p_load(\n  \"dplyr\", # Data Manipulation\n  \"ggplot2\", # Data Visualization\n  \"ggtext\", # Labels\n  \"dagitty\", # Creating DAGs\n  \"ggdag\", # Plotting DAGs\n  install = FALSE\n)\n\n# Define a Custom Theme - Taken From Andrew Heiss's Blogs\nblog_theme &lt;- function() {\n  theme_bw() +  # Start with theme_bw\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      plot.title = element_text(face = \"bold\"),\n      axis.title = element_text(face = \"bold\"),\n      strip.text = element_text(face = \"bold\"),\n      strip.background = element_rect(fill = \"grey80\", color = NA),\n      legend.title = element_text(face = \"bold\")\n    )\n}"
  },
  {
    "objectID": "blog/2024/dynamic-causal-inference/index.html#footnotes",
    "href": "blog/2024/dynamic-causal-inference/index.html#footnotes",
    "title": "An Introduction to Dynamic Causal Inference",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote: if you are already familiar with the core concepts of causal inference, feel free to skip down to the section where I start discussing time and why it messes things up.↩︎\nWell… kind of. There are actually a lot of other assumptions that you would need to check for first, like positivity, SUTVA, no measurement error, etc. (which will not be covered here in sufficient detail if you are not familiar with these topics). If you’re not super familiar with these topics, I highly recommend checking out Chatton and Rohrer 2024.↩︎\nI do not condone this attitude… never treat your regressions or research designs this way.↩︎\nHere are some good starting materials for DAGs, simulation, and sensitivity analyses: Rohrer 2018, Blair et al. 2023, Cinelli and Hazlett 2020.↩︎"
  },
  {
    "objectID": "blog/index.html#section",
    "href": "blog/index.html#section",
    "title": "Blog",
    "section": "2024",
    "text": "2024\n\n\n\n\n\n\n\n\n\n\nCausal Inference for Casuals\n\n\n\n\n\n\ncausal inference\n\n\nsimulation\n\n\ndags\n\n\nregression\n\n\n\nLearn about the core concepts of causal inference and the motivations for causal analysis with the help of simulation. Beginners to causal inference welcome!\n\n\n\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting the Outcome of NFL Games in the 2024-2025 Season\n\n\n\n\n\n\nmachine learning\n\n\nnfl\n\n\n\nFollow my attempt to predict the winners and losers of each game in the 2024-2025 NFL season.\n\n\n\n\n\nSeptember 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAn Introduction to Dynamic Causal Inference\n\n\n\n\n\n\ncausal inference\n\n\npanel data\n\n\ndags\n\n\n\nLearn the basics to making causal inferences with panel/longitudinal data.\n\n\n\n\n\nJuly 8, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Research",
    "section": "",
    "text": "My personal research covers the intersection of causal inference, quasi-experimental design, and policy evaluation and a variety of topics that I find interesting, including (but not limited to): peace and conflict, democracy and elections, political and economic development, and the NFL (professional American football)."
  },
  {
    "objectID": "research/index.html#dissertation",
    "href": "research/index.html#dissertation",
    "title": "Research",
    "section": "Dissertation",
    "text": "Dissertation\n\n“Rethinking the Study of Conflict and Peace: Making Causal Inferences in Quantitative Conflict and Peace Research” \n\nManuscript \nCode (Chapter 2) \nCode (Chapter 3)"
  },
  {
    "objectID": "blog/2024/causal-inference-simulation/index.html",
    "href": "blog/2024/causal-inference-simulation/index.html",
    "title": "Causal Inference for Casuals",
    "section": "",
    "text": "Code\n# Load Libraries\npacman::p_load(\n  \"dplyr\", # Data Manipulation\n  \"ggdag\", # Visualizing DAGs\n  \"ggplot2\", # Data Visualization\n  \"scales\", # Plotting Percentage Values\n  install = FALSE\n)\n\n# Define a Custom Theme - Taken From Andrew Heiss's Blogs\nblog_theme &lt;- function() {\n  theme_bw() +  # Start with theme_bw\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      plot.title = element_text(face = \"bold\"),\n      axis.title = element_text(face = \"bold\"),\n      strip.text = element_text(face = \"bold\"),\n      strip.background = element_rect(fill = \"grey80\", color = NA),\n      legend.title = element_text(face = \"bold\")\n    )\n}"
  },
  {
    "objectID": "blog/2024/causal-inference-simulation/index.html#mediators",
    "href": "blog/2024/causal-inference-simulation/index.html#mediators",
    "title": "Causal Inference for Casuals",
    "section": "Mediators",
    "text": "Mediators\nThe first type of variable that we want to avoid controlling for is the mediator. A mediator is any variable that is, in part caused by \\(X\\) which then, in part, causes a change in \\(Y\\).\n\nsimple_dag &lt;- dagify(\n  Y ~ Z + X + M,\n  X ~ Z,\n  M ~ X,\n  exposure = \"X\",\n  outcome = \"Y\",\n  coords = list(x = c(Y = 7, X = 4, Z = 5.5, M = 5.5),\n                y = c(Y = 2, X = 2, Z = 3, M = 1.5)),\n  labels = c(Y = \"Y\", X = \"X\", Z = \"Z\", M = \"M\")\n)\n\n# Convert DAG to a Tidy Object for Plotting\nsimple_dag_tidy &lt;- simple_dag %&gt;% \n  tidy_dagitty() %&gt;%\n  mutate(status = case_when(\n    name == \"X\" ~ \"exposure\",\n    name == \"Y\" ~ \"outcome\",\n    name == \"M\" ~ \"mediator\",\n    TRUE ~ \"other\"\n  ))\n\nstatus_colors &lt;- c(exposure = \"#6f1a1a\", outcome = \"#384d35\", mediator = \"#7d8d67\")\n\n# Create Plot\nggplot(simple_dag_tidy, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point(aes(color = status)) +\n  geom_label(aes(label = label, fill = status),\n             color = \"white\", fontface = \"bold\", nudge_y = 0.15) +\n  scale_color_manual(values = status_colors, na.value = \"grey20\") +\n  scale_fill_manual(values = status_colors, na.value = \"grey20\") +\n  guides(color = \"none\", fill = \"none\") +\n  theme_dag()\n\n\n\n\n\n\n\n\nControlling for a mediator should be avoided because, if we control for it, our estimated effect of \\(X\\) on \\(Y\\) will be a deflated version of the causal effect of interest because controlling for the mediator effectively removes the part of the causal effect of \\(X\\) on \\(Y\\) that is mediated through \\(M\\). We can see this clearly with the following simulated regressions.\n\nset.seed(1234)\n\n# Simulate Data Where X-Y Relationship is Confounded\n# Number of Observations\nn &lt;- 3000\n\n# Simulate the Confounder as a Continuous Variable\nZ &lt;- rnorm(n, mean = 50, sd = 10)\n\n# Simulate the Treatment as a Binary Variable That is Impacted by Z\n# Using the Inverse Logit Function\nprob_X &lt;- 1 / (1 + exp(-(-2.5 + (0.05*Z))))\nX &lt;- rbinom(n, size = 1, prob = prob_X)\n\n# Simulate a Mediator That is Impacted by X\nM &lt;- (10*X) + rnorm(n, mean = 0, sd = 10) \n\n# Simulate the Outcome with a Base Number to Start with (50), the Impact of X,\n# the Impact of Z, the Impact of M Mediated by X, and Random Noise. The Estimated Total Effect of X Should Be 30\nY &lt;- 50 + ((20*X) + M) + (5*Z) + rnorm(n, mean = 0, sd = 10) \n\n# Combine This Data\nsim_data &lt;- tibble(\n  n = 1:n,\n  Y = Y,\n  Z = Z,\n  X = X,\n  M = M,\n)\n\nsummary(lm(Y ~ X + Z + M, data = sim_data))\n\n\nCall:\nlm(formula = Y ~ X + Z + M, data = sim_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.299  -6.454  -0.020   6.449  33.509 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 50.08945    0.91749   54.59   &lt;2e-16 ***\nX           20.17165    0.41190   48.97   &lt;2e-16 ***\nZ            5.00013    0.01851  270.16   &lt;2e-16 ***\nM            0.98048    0.01810   54.18   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.805 on 2996 degrees of freedom\nMultiple R-squared:  0.9708,    Adjusted R-squared:  0.9708 \nF-statistic: 3.325e+04 on 3 and 2996 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(Y ~ X + Z, data = sim_data))\n\n\nCall:\nlm(formula = Y ~ X + Z, data = sim_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-52.431  -9.296   0.177   9.374  45.548 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 48.52958    1.29011   37.62   &lt;2e-16 ***\nX           30.06503    0.51941   57.88   &lt;2e-16 ***\nZ            5.03174    0.02602  193.35   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.79 on 2997 degrees of freedom\nMultiple R-squared:  0.9423,    Adjusted R-squared:  0.9422 \nF-statistic: 2.446e+04 on 2 and 2997 DF,  p-value: &lt; 2.2e-16\n\n\nNote that, in this blog post, \\(X\\) = 20 is the sort of unbiased causal effect we have been looking for so far. However, I’ve modified that a bit here because our mediator contributes to the causal effect of \\(X\\) on \\(Y\\). In reality, our causal effect of \\(X\\) should be the direct effect of \\(X\\) on \\(Y\\) (20) but it should also include the total effect that is mediated through \\(M\\) (10). In the first model, we see our direct effect is correct, but we are not attempting to estimate the direct effect. The direct effect is accurate because we are controlling for our mediator and our mediator is not confounded. That’s right… if you want to work with a mediator to estimate direct and indirect effects (this practice is referred to as mediation analysis) of \\(X\\) and \\(M\\) on \\(Y\\), you’ve doubled the challenge of dealing with confounding because, not only do you have to make sure that \\(X\\) \\(\\rightarrow\\) \\(Y\\) is unconfounded, you also have to make sure that \\(M\\) \\(\\rightarrow\\) \\(Y\\) is also unconfounded. This is a major reason behind the skepticism for mediation analyses. Regardless, as you can see in the second model where \\(M\\) is excluded, we get the correct total effect of \\(X\\) on \\(Y\\)."
  },
  {
    "objectID": "blog/2024/causal-inference-simulation/index.html#colliders",
    "href": "blog/2024/causal-inference-simulation/index.html#colliders",
    "title": "Causal Inference for Casuals",
    "section": "Colliders",
    "text": "Colliders\nColliders are interesting because they are very sneaky. I like to think of them as inverse confounders. You need to control for a confounder (\\(X\\) \\(\\leftarrow\\) \\(Z\\) \\(\\rightarrow\\) \\(Y\\)) but you should not control for a collider (\\(X\\) \\(\\rightarrow\\) \\(C\\) \\(\\leftarrow\\) \\(Y\\)). The crucial difference here is the causal ordering of the third variable as it relates to the treatment and outcome. Whereas a confounder is a common cause of \\(X\\) and \\(Y\\), a collider is a common consequence of \\(X\\) and \\(Y\\). In the following DAG, note that I am not positing that \\(X\\) impacts \\(Y\\). This is intentional.\n\nsimple_dag &lt;- dagify(\n  Y ~ Z,\n  X ~ Z,\n  C ~ X + Y,\n  exposure = \"X\",\n  outcome = \"Y\",\n  coords = list(x = c(Y = 7, X = 4, Z = 5.5, C = 5.5),\n                y = c(Y = 2, X = 2, Z = 3, C = 1.5)),\n  labels = c(Y = \"Y\", X = \"X\", Z = \"Z\", C = \"C\")\n)\n\n# Convert DAG to a Tidy Object for Plotting\nsimple_dag_tidy &lt;- simple_dag %&gt;% \n  tidy_dagitty() %&gt;%\n  mutate(status = case_when(\n    name == \"X\" ~ \"exposure\",\n    name == \"Y\" ~ \"outcome\",\n    name == \"C\" ~ \"collider\",\n    TRUE ~ \"other\"\n  ))\n\nstatus_colors &lt;- c(exposure = \"#6f1a1a\", outcome = \"#384d35\", collider = \"#ec9b00\")\n\n# Create Plot\nggplot(simple_dag_tidy, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point(aes(color = status)) +\n  geom_label(aes(label = label, fill = status),\n             color = \"white\", fontface = \"bold\", nudge_y = 0.15) +\n  scale_color_manual(values = status_colors, na.value = \"grey20\") +\n  scale_fill_manual(values = status_colors, na.value = \"grey20\") +\n  guides(color = \"none\", fill = \"none\") +\n  theme_dag()\n\n\n\n\n\n\n\n\nWhy does controlling for a common consequence of treatment and outcome lead to problems? Because it can suggest a spurious association between \\(X\\) and \\(Y\\). In other words, controlling for a collider can create an association between \\(X\\) and \\(Y\\) that does not reflect a causal relationship. This is why I like to think of colliders as inverse confounders. By default, confounders create spurious associations between \\(X\\) and \\(Y\\), and we combat this and remove the spurious association to make causal inferences by controlling for the confounder. In contrast, we can introduce spurious associations as well by controlling for certain variables; common causes.\nBut this is still vague… What is a hypothetical example of this? To keep thing’s grounded, I’ll borrow an example from Rohrer 2018. On a particularly lonely day, an individual may be reminiscing on their former romantic partners. They start thinking about the traits of their former romantic partners, and they begin to notice that their more attractive partners were less intelligent than their comparatively less attractive partners. (In DAG terms, this individual is consider Attractiveness \\(\\rightarrow\\) Intelligence). Should this person conclude that, among all potential partners, the more attractive they are, the less intelligent they will be? No! Because this observation is conditioned on a collider. And that collider is the fact they have dated. Most people are attracted to physically attractive and intelligent people, so whether or not this individual has dated someone is likely influenced by their attractiveness and intelligence (Attractiveness \\(\\rightarrow\\) Dated \\(\\leftarrow\\) Intelligence). By just looking at their former partners, they are introducing a spurious association between physical attraction and intelligence.\nOkay but, among this subset of people, the individual still finds a negative association between physical attraction and intelligence. Maybe it’s not fair to generalize this relationship to the broader dating pool, but how does that explain this individual’s former partners? We can explain this with a couple of reasons. First, it is rare that this individual could find someone else who is both highly attractive and highly intelligent. Even if they could, that person would likely already be taken. Second, this individual is probably unlikely to date someone who is also low in both physical attraction and intelligence. As a result, their former partners varied. Sometimes, a partner was more attractive, but not equally intelligent. At other times, a partner was more intelligent, but not as physically attractive. If this person would not developed their conclusion based on their prior dating experiences (this is equivalent to controlling for people they have previously dated), they likely would not have found that this negative relationship holds up. In a very roundabout way, we have also described a type of selection bias here. By conditioning on a collider, we are limiting our analysis to a subset of cases that are impacted by treatment and outcomes. This serves to bias our analysis and create associations that some might interpret as causal while no such association actually exists. This issue does not become a problem if you simply don’t control for a collider.\nLet’s put this into practice using simulation. In contrast to the mediator example, I am not going to simulate a relationship between \\(X\\) and \\(Y\\).\n\nset.seed(1234)\n\n# Simulate Data Where X-Y Relationship is Confounded\n# Number of Observations\nn &lt;- 3000\n\n# Simulate the Confounder as a Continuous Variable\nZ &lt;- rnorm(n, mean = 50, sd = 10)\n\n# Simulate the Treatment as a Binary Variable That is Impacted by Z\n# Using the Inverse Logit Function\nprob_X &lt;- 1 / (1 + exp(-(-2.5 + (0.05*Z))))\nX &lt;- rbinom(n, size = 1, prob = prob_X)\n\n# Simulate the Outcome with a Base Number to Start with (50), the Impact of X,\n# the Impact of Z, and Random Noise. The Estimated Effect of X Should Be 20\nY &lt;- 50 + (5*Z) + rnorm(n, mean = 0, sd = 10) \n\n# Simulate a Collider\nC &lt;- (10*X) + (10*Y) + rnorm(n, mean = 0, sd = 10) \n\n# Combine This Data\nsim_data &lt;- tibble(\n  n = 1:n,\n  Y = Y,\n  Z = Z,\n  X = X,\n  C = C,\n)\n\nsummary(lm(Y ~ X + Z + C, data = sim_data))\n\n\nCall:\nlm(formula = Y ~ X + Z + C, data = sim_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3454 -0.6454  0.0037  0.6414  3.3643 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.3664761  0.1262877   2.902  0.00374 ** \nX           -0.9889660  0.0368645 -26.827  &lt; 2e-16 ***\nZ            0.0393890  0.0092391   4.263 2.08e-05 ***\nC            0.0992183  0.0001799 551.507  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9776 on 2996 degrees of freedom\nMultiple R-squared:  0.9996,    Adjusted R-squared:  0.9996 \nF-statistic: 2.735e+06 on 3 and 2996 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(Y ~ X + Z, data = sim_data))\n\n\nCall:\nlm(formula = Y ~ X + Z, data = sim_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-31.885  -6.900  -0.085   6.837  36.475 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 48.40907    0.92564  52.298   &lt;2e-16 ***\nX            0.09033    0.37268   0.242    0.808    \nZ            5.03224    0.01867 269.502   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.897 on 2997 degrees of freedom\nMultiple R-squared:  0.9626,    Adjusted R-squared:  0.9626 \nF-statistic: 3.855e+04 on 2 and 2997 DF,  p-value: &lt; 2.2e-16\n\n\nAs you can see, when I controlled for the collider, a truly non-existent relationship between \\(X\\) and \\(Y\\) shows up. In contrast, when \\(C\\) is omitted, we don’t observe an effect of \\(X\\) on \\(Y\\) which, in this case, is true! Beware colliders, folks and think carefully before you control for a variable."
  },
  {
    "objectID": "blog/2024/causal-inference-simulation/index.html#ancestors-of-outcome-and-treatment",
    "href": "blog/2024/causal-inference-simulation/index.html#ancestors-of-outcome-and-treatment",
    "title": "Causal Inference for Casuals",
    "section": "Ancestors of Outcome and Treatment",
    "text": "Ancestors of Outcome and Treatment\nLastly, we get to the ancestors. These types of variables are interesting because they are very common and you could identify a lot of them, but you don’t have to control for any of them. And, if you do control for them, it might not be a huge deal. Why is this the case? After all, there seemed to be pretty major consequences when controlling for a mediator/collider?\nAs you can see in the DAG below, we have two types of ancestors. An ancestor of \\(X\\) and an ancestor of \\(Y\\). Each of these impact their respective node, but are otherwise unconnected to other nodes in the DAG. Because neither is related to both \\(X\\) and \\(Y\\), controlling/not controlling for these should not impact the causal interpretation of your estimate… with a couple of caveats.\n\nsimple_dag &lt;- dagify(\n  Y ~ Z + X + A_Y,\n  X ~ Z + A_X,\n  exposure = \"X\",\n  outcome = \"Y\",\n  coords = list(x = c(Y = 7, X = 4, Z = 5.5, A_X = 3.25, A_Y = 7.75),\n                y = c(Y = 2, X = 2, Z = 3, A_X = 2.5, A_Y = 2.5)),\n  labels = c(Y = \"Y\", X = \"X\", Z = \"Z\", A_X = \"A_X\", A_Y = \"A_Y\")\n)\n\n# Convert DAG to a Tidy Object for Plotting\nsimple_dag_tidy &lt;- simple_dag %&gt;% \n  tidy_dagitty() %&gt;%\n  mutate(status = case_when(\n    name == \"X\" ~ \"exposure\",\n    name == \"Y\" ~ \"outcome\",\n    name == \"A_X\" ~ \"ax\",\n    name == \"A_Y\" ~ \"ay\",\n    TRUE ~ \"other\"\n  ))\n\nstatus_colors &lt;- c(exposure = \"#6f1a1a\", outcome = \"#384d35\", ax = \"#a32b2b\", ay = \"#597657\")\n\n# Create Plot\nggplot(simple_dag_tidy, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point(aes(color = status)) +\n  geom_label(aes(label = label, fill = status),\n             color = \"white\", fontface = \"bold\", nudge_y = 0.1) +\n  scale_color_manual(values = status_colors, na.value = \"grey20\") +\n  scale_fill_manual(values = status_colors, na.value = \"grey20\") +\n  guides(color = \"none\", fill = \"none\") +\n  theme_dag()\n\n\n\n\n\n\n\n\nTo demonstrate, I am simply simulating a variable that causes a change in \\(X\\) (\\(AX\\)) and a variable that causes a change in \\(Y\\) (\\(AY\\))… and that’s all that these variables do. Then, I run a regression controlling for neither, and a respective regression controlling for each. Let’s see what we get.\n\nset.seed(1234)\n\n# Simulate Data Where X-Y Relationship is Confounded\n# Number of Observations\nn &lt;- 3000\n\n# Simulate the Confounder as a Continuous Variable\nZ &lt;- rnorm(n, mean = 50, sd = 10)\n\n# Simulate Ancestors of Treatment/Outcome\nAX &lt;- rnorm(n, mean = 10, sd = 10)\nAY &lt;- rnorm(n, mean = 10, sd = 10)\n\n# Simulate the Treatment as a Binary Variable That is Impacted by Z and AX\n# Using the Inverse Logit Function\nprob_X &lt;- 1 / (1 + exp(-(-2.5 + (0.05*Z) + (0.1*AX))))\nX &lt;- rbinom(n, size = 1, prob = prob_X)\n\n# Simulate the Outcome with a Base Number to Start with (50), the Impact of X,\n# the Impact of Z, the Impact of AY, and Random Noise. The Estimated Effect of X Should Be 20\nY &lt;- 50 + (20*X) + (5*Z) + (2*AY) + rnorm(n, mean = 0, sd = 10) \n\n# Combine This Data\nsim_data &lt;- tibble(\n  n = 1:n,\n  Y = Y,\n  Z = Z,\n  X = X,\n  AX = AX,\n  AY = AY\n)\n\nsummary(lm(Y ~ X + Z, data = sim_data))\n\n\nCall:\nlm(formula = Y ~ X + Z, data = sim_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-76.469 -14.940   0.352  14.707  83.582 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 70.43861    2.08184   33.84   &lt;2e-16 ***\nX           18.82075    0.89826   20.95   &lt;2e-16 ***\nZ            5.01715    0.04171  120.28   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 22.21 on 2997 degrees of freedom\nMultiple R-squared:  0.849, Adjusted R-squared:  0.8489 \nF-statistic:  8426 on 2 and 2997 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(Y ~ X + Z + AX, data = sim_data))\n\n\nCall:\nlm(formula = Y ~ X + Z + AX, data = sim_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-75.912 -15.042   0.284  14.718  83.053 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 70.87527    2.11189  33.560   &lt;2e-16 ***\nX           19.26880    0.96962  19.873   &lt;2e-16 ***\nZ            5.01320    0.04183 119.834   &lt;2e-16 ***\nAX          -0.05481    0.04468  -1.227     0.22    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 22.21 on 2996 degrees of freedom\nMultiple R-squared:  0.8491,    Adjusted R-squared:  0.8489 \nF-statistic:  5619 on 3 and 2996 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(Y ~ X + Z + AY, data = sim_data))\n\n\nCall:\nlm(formula = Y ~ X + Z + AY, data = sim_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-39.969  -6.847   0.187   6.650  36.293 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 49.81184    0.95232   52.31   &lt;2e-16 ***\nX           19.75974    0.40282   49.05   &lt;2e-16 ***\nZ            5.00853    0.01870  267.80   &lt;2e-16 ***\nAY           1.99510    0.01828  109.15   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.957 on 2996 degrees of freedom\nMultiple R-squared:  0.9697,    Adjusted R-squared:  0.9696 \nF-statistic: 3.192e+04 on 3 and 2996 DF,  p-value: &lt; 2.2e-16\n\n\nWhen neither ancestor is included, we get pretty close to the actual treatment size. Similarly, when either ancestor is included, the estimated effect doesn’t change much and is pretty unbiased. So, what changed? Check out the standard errors! Without controlling for either ancestor, the standard error for the \\(X\\) estimate is 0.9. However, when we control for \\(AX\\), that standard error gets a bit bigger (0.97). In contrast, when we control for \\(AY\\), the standard error shrinks all the way down to 0.4. While controlling for either did not impact our causal estimation, including these variables did have an impact on the precision of our estimate. In other words, they respectively increased/decreased statistical uncertainty regarding whether these effect are statistically different from 0. In all cases, the estimated effect of \\(X\\) was still statistically significant, but that is likely due to how simple my simulated data are.\nSo what are those two caveats that I briefly mentioned a second ago. First, don’t always think of \\(AX\\) being bad. As a general rule, you probably should not control for \\(AX\\), but you should think about what your ancestors of treatment are. If, as I simulated, your ancestor of treatment only impacts \\(Y\\) through \\(X\\) and is totally unrelated to any other node in your DAG, you probably have an instrument which is a whole other can of worms that I won’t open here but basically, the TLDR is that you can use this instrument as a form of randomization… Long story but this is where instrumental variables designs are relevant. Although, another caveat is warranted here because identifying actual instruments is very, very hard in practice.\nThe second caveat is one that confuses me. Notice how I made the effect size of \\(AY\\) so small compared to all of the other effect sizes we have been working with thus far? Well, I did that because making \\(AY\\) large (as ancestors of outcome will often be in practice) really screwed up results and made the estimate for \\(X\\) very biased (like, down in the 13-16 range). I don’t know why this is the case. Theoretically, \\(AY\\) has nothing to do with \\(X\\), so including it should not impact the estimate for \\(X\\). Nonetheless, it did and I do not fully understand why. I don’t think I’ve discovered something novel… in reality, something is going on in the math behind the simulation that I’m sure I’m not understanding. Regardless, controlling for ancestors of outcome often results in increase to statistical precision, so it’s not like its the end of the world to have to control for \\(AY\\)."
  }
]