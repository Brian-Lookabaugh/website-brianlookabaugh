[
  {
    "objectID": "resume/index.html",
    "href": "resume/index.html",
    "title": "Brian Lookabaugh",
    "section": "",
    "text": "Download"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Brian Lookabaugh",
    "section": "",
    "text": "Greetings and welcome to my personal website! My name is Brian Lookabaugh, and I’m a data scientist working in the federal contracting industry, where I conduct quantitative research covering causal inference, survey analysis, and natural language processing.\nMy core passion lies in causal analysis but I’m also deeply interested in a wide range of data science disciplines, including Bayesian statistics, experimental design, machine learning, survival analysis, and decision science.\nPlease feel free to check out my blog where I explore all sorts of quantitative topics and break down what I learn in an accessible, intuitive way. My goal is to make complex ideas in data science both understandable and engaging."
  },
  {
    "objectID": "blog/2025/priors-distributions/index.html",
    "href": "blog/2025/priors-distributions/index.html",
    "title": "Getting Comfortable with Expressing Beliefs as Distributions",
    "section": "",
    "text": "Code\npacman::p_load(\n  \"dplyr\", # Data Manipulation\n  \"ggplot2\", # Data Visualization\n  \"forcats\", # Factors\n  \"ggdist\", # Plotting Distributions\n  \"brms\", # Bayesian Modeling\n  \"tidybayes\", # Bayesian Visualization\n  \"forcats\", # Factors\n  install = FALSE\n)\n\n# Define a Custom Theme\nblog_theme &lt;- function() {\n  theme_bw() +  \n    theme(\n      panel.grid.major = element_line(color = \"gray80\", size = 0.3),\n      panel.grid.minor = element_blank(),\n      panel.border = element_blank(),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      plot.title = element_text(face = \"bold\", size = 16, margin = margin(t = 0, r = 0, b = 15, l = 0)),\n      axis.title.x = element_text(face = \"bold\", size = 14, margin = margin(t = 15, r = 0, b = 0, l = 0)),\n      axis.title.y = element_text(face = \"bold\", size = 14, margin = margin(t = 0, r = 15, b = 0, l = 0)),\n      strip.text = element_text(face = \"bold\"),\n      axis.text.x = element_text(face = \"bold\", size = 10), \n      axis.text.y = element_text(face = \"bold\", size = 10), \n      axis.ticks.x = element_blank(), \n      axis.ticks.y = element_blank(), \n      strip.background = element_rect(fill = \"grey80\", color = NA),\n      legend.title = element_text(face = \"bold\", size = 14),\n      legend.text = element_text(face = \"bold\", size = 10, color = \"grey25\"),\n    )\n}\n\n# Establish a Custom Color Scheme\ncolors &lt;- c(\n  \"#0a697d\",\n  \"#0091af\",\n  \"#ddb067\",\n  \"#c43d56\",\n  \"#ab2a42\",\n  \"gray50\"\n)\n\n\n\nIntro\nI think that one of the more intimidating things about Bayesian statistics is just how much thinking you have to do before even running a model. I remember my first couple of semesters in grad school, writing out OLS and logit models in Stata, not really knowing what I was doing at all, and thinking to myself “wow, this sure is easy!”. But, given how pervasive the problem of “garbage can” models are, perhaps running statistical models should not be as easy as a line or two of code, hitting “run” and getting results back 3 seconds later.\nA great things about Bayesian statistical modeling is that (if you’re doing it honestly) it forces you to do some upfront thinking about your model before you even get any results. But this “thinking” is not always the most intuitive, specifically when it comes to figuring out what priors you are going to use. Understanding why priors are important and helpful isn’t so hard to grasp, but thinking about beliefs about parameters of a model in terms of distributions is not very intuitive for a lot of people (that was for sure the case with me!)\n\n\n\n\n\n\nBefore Continuing…\n\n\n\nIf you don’t really know what I’m talking about so far, then I would imagine you’re not very familiar with Bayesian statistics and, if my assumption is correct, it’s probably worth checking out some introductory resources before continuing with this blog. I wrote a somewhat intro-friendly (although not comprehensive) blog but this free online book is also really good if you have the time. You don’t need to be an expert by any means for this blog, just know the basics of Bayesian statistics and what priors are.\n\n\n\n\nBeliefs as Distributions\nLearning Bayesian statistics sort of re-framed how I view everything with uncertainty in life. For example, I’ve found (much to the dismay of much of my family and friends) that “I don’t know” responses rarely are as uninformative as they sound.\nSay that I’m planning to get lunch with a friend and I show up on time, but my friend is going to get there late. I text my friend and ask “hey, what time do you think you’ll be here?”. My friend responds, “I don’t know”. Well great, that’s extremely unhelpful. I ask them what’s the hold-up. They respond that they left their house late and that they are in traffic. Let’s say that I know where they live (15 miles away from the restaurant) and so, even though the severity of traffic is a bit variable, I know that there are some values that are basically impossible, like my friend being 2 hours late.\nBut I probe for more information. “Can you give me a time frame?”, I ask. “I don’t know”, they reply. “I’m maybe 20 minutes away, give or take 10 minutes, depending on whether or not I catch a break. But this traffic is really bad, so maybe it gets worse”. So now I’ve got a time frame at least to work with. Most likely, my friend will arrive in 20 minutes, but I shouldn’t think that my friend arriving in 10 minutes or 30 minutes is unreasonable either.\nFinally, I receive another text that says, “Google Maps re-directed me to a shortcut to avoid traffic. I’ll be there in 20 minutes”. Great, I can expect my friend to get to the restaurant in 20 minutes with a good degree of confidence, but obviously, there’s probably a little bit of uncertainty anyways. For example, Maps could also be directing other drivers to take the short-cut, which could create a bit of traffic as well.\nFor this simple realistic scenario, I went through 4 different states of belief that can be expressed as the distribution seen below.\n\n\nCode\n# Set Prior Assumptions for 4 Different Expectations\nlunch_priors &lt;- c(\n  prior(uniform(0, 200)),\n  prior(lognormal(3.27, 0.77)),\n  prior(lognormal(2.85, 0.68)),\n  prior(lognormal(2.85, 0.27))\n)\n\n# Get a Data Frame of Distribution Data\nlunch_priors_df &lt;- lunch_priors |&gt;\n  parse_dist() |&gt;\n  mutate(prior = case_when(\n    prior == \"uniform(0, 200)\" ~ \"IDK - Uniform(0, 200)\",\n    prior == \"lognormal(3.27, 0.77)\" ~ \"Vibes - Log-Normal(3.27, 0.77)\",\n    prior == \"lognormal(2.85, 0.68)\" ~ \"Best Guestimate - Log-Normal(2.85, 0.68)\",\n    prior == \"lognormal(2.85, 0.27)\" ~ \"Maps Shortcut - Log-Normal(2.85, 0.27)\"\n  )) |&gt; \n  mutate(prior = fct_inorder(prior))\n\nggplot(lunch_priors_df, aes(y = 0, dist = .dist, args = .args)) +\n  stat_halfeye(point_size = 3, aes(slab_fill = prior),\n               point_interval = median_qi, .width = c(0.95, 0.80, 0.50), \n               interval_size_range = c(0.75, 2), show.legend = FALSE) +\n  facet_wrap(~ prior, scales = \"free_x\") +\n  scale_fill_manual(values = colors[c(6, 4, 3, 2)], aesthetics = \"slab_fill\", guide = \"none\") +\n  labs(x = \"Minutes To Arrival\", y = \"\", title = \"\") +\n  scale_y_continuous(labels = NULL) +\n  blog_theme()\n\n\n\n\n\nBeliefs About Friend’s ETA\n\n\n\n\nDon’t fret too much about the parameters of the distributions right now (the numbers in the parentheses). Focus instead on how the distributions of possible beliefs change with different information. If someone says “I don’t know” then we’re working with nothing. Any possible value is as likely as any other (I arbitrarily set a range of values from 0 to 200 minutes but “I don’t know” really means any value from -\\(\\infty\\) to \\(\\infty\\) has the same probability of being true as any other value in that range).\nHowever, leveraging background knowledge, I was able to form a more realistic and informative range of possible values, with the bulk of my belief being that my friend would show up somewhere in the 2-50 minutes range, but I still allow for a very long right tail, meaning I don’t rule it out that my friend could be very late. Now that’s super wide but, when I got the information that they thought they were roughly 20 minutes away, give or take 10 minutes, the distribution compresses and we get closer to a 15-35 minutes range, still though, there is a long right tail, so I’m not ruling out a very late arrival. Finally, with more concrete information that I can be more certain of (a Google Maps estimate), I can make the distribution of beliefs much tighter and be confident that my friend will arrive in 15-25 minutes or so.\n\n\nSome Helpful Distributions to Know\nNow that we’ve reviewed the intuition behind expressing beliefs as distributions, I’ll do a very quick review of some common distributions (again, this is by no mean’s exhaustive) and present what they look like, the parameters that define the distribution, and real-world examples in which you might use these distributions as priors for Bayesian statistical modeling.\n\nNormal Distribution\nThis is the one that everybody knows, although they might call it other things (i.e. a “bell curve” or a Gaussian Distribution). The normal distribution peaks at its mean and is symmetrical on either side around its mean. 68% of the data falls within 1 standard deviation of the mean and 95% of the data falls within 2 standard deviations. The distribution is an attractive one because of the simplicity of its parameters and it’s assumption-lite properties in the sense that we don’t have have to think much about skewness or values not being allowed to go above or below a certain value.\n\n\nCode\nnormal_priors &lt;- c(\n  prior(normal(20, 30)),\n  prior(normal(20, 15)),\n  prior(normal(20, 5))\n)\n\nnormal_priors_df &lt;- normal_priors |&gt;\n  parse_dist() |&gt;\n  mutate(prior = fct_inorder(prior))\n\nggplot(normal_priors_df, aes(y = 0, dist = .dist, args = .args)) +\n  stat_slab(alpha = 0.7, aes(slab_fill = prior)) +\n  scale_fill_manual(values = colors[c(4, 3, 2)], aesthetics = \"slab_fill\", guide = \"none\") +\n  labs(x = \"\", y = \"\", title = \"\") +\n  scale_x_continuous(breaks = seq(-60, 100, by = 20)) +\n  scale_y_continuous(labels = NULL) +\n  blog_theme()\n\n\n\n\n\nDifferent Normal Distributions\n\n\n\n\nAbove are three normal distributions who all share a mean of 20, but the uncertainty of 20-ish values varies by distribution. The mean (\\(\\mu\\)) and the standard deviation (\\(\\sigma\\)) are the two parameters that determine the shape of the normal distribution. These are concepts that even the general public are pretty familiar with, so it makes interpreting the distribution pretty easy for most folks. For each of these distributions, the parameters can be interpreted as:\n\n\\(\\mathcal{N}(20, 30)\\)\n\nMean of 20 and 68% of the data is within 20 \\(\\pm\\) 30 and 95% of the data is roughly within 20 \\(\\pm\\) 60. This is the red distribution.\n\n\\(\\mathcal{N}(20, 15)\\)\n\nMean of 20 and 68% of the data is within 20 \\(\\pm\\) 15 and 95% of the data is roughly within 20 \\(\\pm\\) 30. This is the yellow distribution.\n\n\\(\\mathcal{N}(20, 5)\\)\n\nMean of 20 and 68% of the data is within 20 \\(\\pm\\) 5 and 95% of the data is roughly within 20 \\(\\pm\\) 10. This is the blue distribution.\n\n\n\n\nStudent’s t-Distribution\nThe Student’s-t distribution (pictured below) at first glance doesn’t really seem any different from the normal distribution. After all, its highest point is its mean (\\(\\mu\\)), it is symmetric around \\(\\mu\\) and it is scaled by \\(\\sigma\\). So what is different about this? Well, take a look at the range of values on the x-axis. Then, eyeball it and look at how much is covered under the tails of each distribution. This is what differentiates the normal distribution from the Student-t distribution; its heavy tails, which are established by the \\(\\nu\\).\n\n\nCode\nstudent_priors &lt;- c(\n  prior(student_t(7, 20, 30)),\n  prior(student_t(7, 20, 15)),\n  prior(student_t(7, 20, 5))\n)\n\nstudent_priors_df &lt;- student_priors |&gt;\n  parse_dist() |&gt;\n  mutate(prior = fct_inorder(prior))\n\nggplot(student_priors_df, aes(y = 0, dist = .dist, args = .args)) +\n  stat_slab(alpha = 0.7, aes(slab_fill = prior)) +\n  scale_fill_manual(values = colors[c(4, 3, 2)], aesthetics = \"slab_fill\", guide = \"none\") +\n  labs(x = \"\", y = \"\", title = \"\") +\n  scale_x_continuous(breaks = seq(-120, 160, by = 20)) +\n  scale_y_continuous(labels = NULL) +\n  blog_theme()\n\n\n\n\n\nDifferent Student’s-t Distributions\n\n\n\n\nSo, why might you use this distribution in the first place if it is so similar to the normal distribution? Primarily, you might consider using the Student-t distribution if you are modeling something that you assume to follow a normal distribution, but your sample size is small and, accordingly, you lack the confidence to rule out the more extreme values that a normal distribution would naturally rule out. You can modify how heavy the tails are by manipulating \\(\\nu\\). The larger you set \\(\\nu\\), the tails get thinner and there is less of an allowance for extreme values.\n\n\nExponential Distribution\nIn my opinion, the exponential distribution is a bit trickier to understand than the normal or Student-t distributions, but it is nonetheless very important and you’ll encounter it often in Bayesian statistical modeling. The simple part of the exponential distribution is its single parameter, \\(\\lambda\\) (although sometimes you will see \\(\\beta\\) instead which is just 1 / \\(\\lambda\\)). Below, I show a plot of three different exponential distributions with \\(\\lambda\\) getting progressively larger.\n\n\nCode\nexponential_priors &lt;- c(\n  prior(exponential(0.25)),\n  prior(exponential(0.5)),\n  prior(exponential(1))\n)\n\n# You Could Also Do This\n# exponential_priors &lt;- c(\n#   prior(exponential(1/4)),\n#   prior(exponential(1/2)),\n#   prior(exponential(1/1))\n# )\n\nexponential_priors_df &lt;- exponential_priors |&gt;\n  parse_dist() |&gt;\n  mutate(prior = fct_inorder(prior))\n\nggplot(exponential_priors_df, aes(y = 0, dist = .dist, args = .args)) +\n  stat_slab(alpha = 0.7, aes(slab_fill = prior)) +\n  scale_fill_manual(values = colors[c(4, 3, 2)], aesthetics = \"slab_fill\", guide = \"none\") +\n  labs(x = \"\", y = \"\", title = \"\") +\n  scale_x_continuous(breaks = seq(0, 25, by = 5)) +\n  scale_y_continuous(labels = NULL) +\n  blog_theme()\n\n\n\n\n\nDifferent Exponential Distributions\n\n\n\n\nI think two big questions are appropriate to answer for this distribution; 1) why would I use it? and 2) what values should I plug in for \\(\\lambda\\)? Regarding the former, note that there are no negative values for these distributions. That is important. You could consider the exponential distribution to model anything that cannot be negative. A little bit of a sneak peak into the next section is that an exponential prior is appropriate for \\(\\sigma\\) in a linear regression because the model’s uncertainty/noise cannot be negative. While there are other distributions that do not allow for negative values, the exponential distribution has the nice feature of concentrating much of its mass around smaller values, which leads to the second question…\nWhat should \\(\\lambda\\) be? On a simple matter, the higher that \\(\\lambda\\) is, more concentration of values are clustered towards 0 and larger values are tolerated less. Substantively, you can set \\(\\lambda\\) to be 1 / \\(\\sigma\\) (standard deviation of \\(Y\\) given the covariates included in the model, assuming you’re using the exponential distribution as a prior for a model’s residual uncertainty). Much like other priors, this is an assumption that you set and it’s one that you have to sit down and think about. If you’re not comfortable with a tighter exponential prior, you can always just set \\(\\lambda\\) to 1, which still allows for a lot of noise if the data is adamant about that. Remember, that’s a great thing about priors… you’re allowed to be unsure, just make sure to be transparent about it.\n\n\n\nLinear Regression Example\nDemonstrating a common application of thinking about beliefs as distributions, I think it makes sense/is helpful to walk through an example with Bayesian linear regression where we can cover priors for the intercept, coefficients, residual variance, model notation, interpretation, etc. But first, we need data to work with. Usually, I’d simulate some data, but, while simulated data is helpful for stuff like evaluating how well a model performs, rooting this in some concrete examples/data is probably more helpful than dealing with abstract variables like (my favorites) \\(X\\), \\(Y\\), \\(Z_1\\), \\(Z_2\\), etc. So I am going to be basic and we are going to be working with the very simplistic gapminder data set. Our goal is to look at how GDP per capita (\\(\\beta_1\\)) is associated with life expectancy (\\(Y\\)).\n\nlibrary(gapminder)\n\ndata(gapminder)\n\ngap &lt;- gapminder\n\nhead(gap, 10)\n\n# A tibble: 10 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n\n\nWe start building our Bayesian linear regression model by specifying a prior for the intercept. It makes little sense to assume that our regression line should start at 0 since any country throughout all of history has never had a life expectancy of literally zero. Instead, let’s use a normal distribution centered around the average life expectancy from our data and allow a good degree of variance around this mean because the life expectancy in places like Western Europe are going to be quite different from places like Sub-Saharan Africa. And it’s also going to vary over time (yes, this would normally be a good place to use multilevel models, but that is not for this blog)… This prior could be expressed like this:\n\\(\\beta_0 \\sim \\text{Normal}(65, 10)\\)\nWhich means that our intercept follows a normal distribution with a mean of 65 years but about 95% of the prior mass is within \\(\\pm\\) 20 years around the mean. Likewise, it also means that about 68% of the prior mass is within \\(\\pm\\) 10 years around the mean.\nNext, we need to set a prior for our association between GDP per capita and life expectancy. Importantly, unless I am trying to do causal analysis, I should not think about my prior as “my best guess for the effect of GDP per capita on life expectancy”. Instead, I need to think about how much I think life expectancy moves when GDP per capita moves. Obviously, these two things are very correlated. As societies become wealthier, people live longer. For the purposes of thinking about the scale of the prior more intuitively, I am going to convert GDP per capita to GDP per capita in thousands of dollars. That way, I can say things like “I expect that, when GDP per capita goes up by $1,000, life expectancy increases by X (instead of when GDP per capita goes up by 1 dollar because the likely associated change in life expectancy would probably be super small).\n\ngap &lt;- gap |&gt; \n  mutate(gdpPercap = gdpPercap / 1000)\n\nI think that something like “when GDP per capita (in thousands of dollars) increases by 1 unit ($1,000) that life expectancy pretty strongly follows. Perhaps something like 0.5 years. And I feel pretty strongly about that. To be precise, I feel pretty strongly that life expectancy goes up by 0.5 years-ish. Like, I wouldn’t be shocked if the actual increase was 0.4 years or 0.6 years or something like that. In fact, I feel so strongly that I just about know that there is no negative association, but I’m open-minded that the association is actually much larger than 0.5 years. I could use a distribution that totally excludes negative values, but it’s not the case that it’s impossible for the association to be negative, just super duper unlikely. Because of that, I am going to use a Student’s t-distribution, even though that’s not exactly right because I don’t think far-left negative ends of the tail are as likely as far-right positive ends of the tail, but close enough. So, I can express the prior for the coefficient like this:\n\\(\\beta_1 \\sim \\text{Student}(5, 0.5, 0.1)\\)\nLastly, I need to think about the amount of residual variance that remains if all I include in this model to predict life expectancy is GPD per capita. Here, I’ll stick with my assumptions that the standard deviation of life expectancy is 10 years (as I assumed with the intercept.) Accordingly, if I divide 1 / 10, I get \\(\\lambda\\), so I will be setting:\n\\(\\sigma \\sim \\text{Exponential}(0.1)\\)\nAnd now, I can plot these three priors and we can get an idea of what they actually look like and see if they’re reasonable. Here’s the prior for our intercept:\n\n\nCode\nmodel_priors &lt;- c(\n  prior(normal(65, 10), class = Intercept),\n  prior(student_t(5, 0.5, 0.1), class = b, coef = gdpPercap),\n  prior(exponential(0.1), class = sigma)\n)\n\nmodel_priors_df &lt;- model_priors |&gt;\n  parse_dist() |&gt;\n  mutate(prior = fct_inorder(prior))\n\nmodel_priors_df |&gt;\n  filter(prior == \"normal(65, 10)\") |&gt;\n  ggplot(aes(y = 0, dist = .dist, args = .args)) +\n  stat_halfeye(\n    aes(\n      slab_alpha = after_stat(pdf),\n      slab_fill = prior\n    ),\n    fill_type = \"segments\",\n    .width = c(0.95, 0.80, 0.50),\n    interval_size_range = c(0.75, 2),\n    scale = 0.75,\n    point_size = 3,\n    show.legend = FALSE\n  ) +\n  scale_fill_manual(\n    values = colors[4],\n    aesthetics = \"slab_fill\",\n    guide = \"none\"\n  ) +\n  scale_slab_alpha_continuous(range = c(0.05, 1)) +\n  scale_x_continuous(breaks = seq(30, 90, by = 10)) +\n  scale_y_continuous(labels = NULL) +\n  labs(\n    x = \"\",\n    y = \"\",\n    title = \"\"\n  ) +\n  blog_theme()\n\n\n\n\n\nIntercept Prior\n\n\n\n\nAnd the prior for \\(\\beta_1\\):\n\n\nCode\nmodel_priors_df |&gt;\n  filter(prior == \"student_t(5, 0.5, 0.1)\") |&gt;\n  ggplot(aes(y = 0, dist = .dist, args = .args)) +\n  stat_halfeye(\n    aes(\n      slab_alpha = after_stat(pdf),\n      slab_fill = prior\n    ),\n    fill_type = \"segments\",\n    .width = c(0.95, 0.80, 0.50),\n    interval_size_range = c(0.75, 2),\n    scale = 0.75,\n    point_size = 3,\n    show.legend = FALSE\n  ) +\n  scale_fill_manual(\n    values = colors[3],\n    aesthetics = \"slab_fill\",\n    guide = \"none\"\n  ) +\n  scale_slab_alpha_continuous(range = c(0.05, 1)) +\n  scale_x_continuous(breaks = seq(0, 1, by = 0.2)) +\n  scale_y_continuous(labels = NULL) +\n  labs(\n    x = \"\",\n    y = \"\",\n    title = \"\"\n  ) +\n  blog_theme()\n\n\n\n\n\nBeta Prior\n\n\n\n\nAnd lastly, the prior for \\(\\sigma\\):\n\n\nCode\nmodel_priors_df |&gt;\n  filter(prior == \"exponential(0.1)\") |&gt;\n  ggplot(aes(y = 0, dist = .dist, args = .args)) +\n  stat_halfeye(\n    aes(\n      slab_alpha = after_stat(pdf),\n      slab_fill = prior\n    ),\n    fill_type = \"segments\",\n    .width = c(0.95, 0.80, 0.50),\n    interval_size_range = c(0.75, 2),\n    scale = 0.75,\n    point_size = 3,\n    show.legend = FALSE\n  ) +\n  scale_fill_manual(\n    values = colors[2],\n    aesthetics = \"slab_fill\",\n    guide = \"none\"\n  ) +\n  scale_slab_alpha_continuous(range = c(0.05, 1)) +\n  scale_x_continuous(breaks = seq(0, 60, by = 10)) +\n  scale_y_continuous(labels = NULL) +\n  labs(\n    x = \"\",\n    y = \"\",\n    title = \"\"\n  ) +\n  blog_theme()\n\n\n\n\n\nSigma Prior\n\n\n\n\nOn a quick visual inspection, everything looks good. Now that we’re set with our priors, we can formally write our model out as the following:\n\\[\n\\begin{aligned}\n\\text{Life Expectancy} &\\sim \\text{Normal}(\\mu, \\sigma) \\\\\n\\mu &= \\beta_0 + \\beta_1 \\, \\text{GDP per capita} \\\\\n\\beta_0 &\\sim \\text{Normal}(65, 10) \\\\\n\\beta_1 &\\sim \\text{Student}(5, 0.5, 0.1) \\\\\n\\sigma &\\sim \\text{Exponential}(0.1)\n\\end{aligned}\n\\] And now we can actually fit our Bayesian model and see how it compares to a standard frequentist model.\n\n\nCode\n# Bayesian Model\nmod_fit &lt;- brm(\n  data = gap,\n  family = gaussian,\n  formula = lifeExp ~ gdpPercap,\n  prior = model_priors,\n  chains = 4, cores = 4, iter = 1000,\n  seed = 1234, backend = \"cmdstanr\",\n)\n\n# Extract Bayesian Info from the Model\nbayes_draws &lt;- mod_fit |&gt; \n  spread_draws(b_gdpPercap)\n\nbayes_df &lt;- bayes_draws |&gt; \n  rename(estimate = b_gdpPercap) |&gt; \n  mutate(model = \"Bayesian\")\n\n# Frequentist Model\nfreq_mod_fit &lt;- lm(lifeExp ~ gdpPercap, data = gap)\n\n# Extract Frequentist Estimate and Confidence Intervals\nfreq_ci &lt;- confint(freq_mod_fit)[\"gdpPercap\", ]\nfreq_est &lt;- coef(freq_mod_fit)[\"gdpPercap\"]\n\nfreq_df &lt;- tibble(\n  model = \"Frequentist\",\n  estimate = freq_est,\n  lower = freq_ci[1],\n  upper = freq_ci[2]\n)\n\nggplot(bayes_df, aes(x = estimate, y = model)) +\n  stat_halfeye(\n    aes(slab_alpha = after_stat(pdf)),\n    .width = c(0.95, 0.80, 0.50),\n    interval_size_range = c(0.75, 2),\n    point_interval = median_qi,\n    fill = colors[2],\n    fill_type = \"segments\",       \n    show.legend = FALSE,\n    scale = 0.75\n  ) +\n  geom_pointinterval(\n    data = freq_df,\n    aes(x = estimate, y = model, xmin = lower, xmax = upper),\n    color = colors[5],\n    size = 6\n  ) +\n  scale_x_continuous(breaks = seq(from = 0.68, to = 0.88, by = 0.02)) +\n  scale_slab_alpha_continuous(range = c(0.05, 1)) +\n  labs(\n    x = \"Estimated Association of GDP per capita and Life Expectancy\",\n    y = \"\"\n  ) +\n  blog_theme()\n\n\n\n\n\nModel Results\n\n\n\n\nAs we can see, the results are very similar, but it appears our Bayesian model is ever so slightly more conservative than our frequentist model, a feature of our priors. It’s notable, however, that the data appears quite strong, as the coefficient for GDP per capita is 0.76 whereas our prior mean for the association was 0.5. But, with that being said, what’s the point in learning to think about beliefs expressed as distributions if the results look so similar? Why not just take the easy route and run frequentist models?\nWell, first of all, they won’t always look this similar. After all, this is a super simple teaching example. But, even when they do appear similarly, the Bayesian results provide superior interpretation and visual communication. Not only does the Bayesian estimate simply look pretty, we can also assign probabilistic interpretations to different values. For example, we can say that there is a 95% chance that the association between GDP per capita and life expectancy is between 0.71 and 0.81. The frequentist confidence interval cannot provide such an interpretation. Seriously, I wish anyone luck explaining what the 95% confidence interval actually says to a group of non-technical stakeholders in a manner that does not compromise what the confidence interval actually says and satisfies stakeholders.\nAnother reason why the Bayesian approach is worth it arises when we’re trying to make causal inferences. Especially for the social sciences, where hundreds of things impact the outcome of interest in varying effect sizes, Bayesian priors are great for regularization. In many cases, due to poor modeling decisions, including variables in a model that should not be included or excluding variables that should be included, we can get crazy effect sizes that are obviously wrong but, if the data is tortured enough, it will say what is says, regardless of how far from the truth it is. Recognizing our limitations, priors can be great for regularizing our estimates towards something much more likely true. Again, this won’t always be the case. But scientists should generally be conservative before making a claim such as “X has a super strong effect on Y”. Priors utilize existing knowledge to prevent such whacky claims from surfacing from tortured data lone. Learning how to frame your prior beliefs as distributions is, for this reason alone, a very powerful skill to develop."
  },
  {
    "objectID": "blog/2025/mutual-adjustment/index.html",
    "href": "blog/2025/mutual-adjustment/index.html",
    "title": "Give Your Hypotheses Space!",
    "section": "",
    "text": "Code\n# Load Libraries\npacman::p_load(\n  \"dplyr\", # Data Manipulation\n  \"dagitty\", # DAGs\n  \"ggdag\", # Visualizing DAGs\n  \"ggraph\", # More DAG Visualizations\n  \"ggplot2\", # Data Visualization\n  \"tidyr\", # Re-Shaping\n  \"modelsummary\", # Presenting Model Results\n  install = FALSE\n)\n\n# Define a Custom Theme\nblog_theme &lt;- function() {\n  theme_bw() +  \n    theme(\n      panel.grid.major = element_line(color = \"gray80\", size = 0.3),\n      panel.grid.minor = element_blank(),\n      panel.border = element_blank(),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      plot.title = element_text(face = \"bold\", size = 16, margin = margin(t = 0, r = 0, b = 15, l = 0)),\n      axis.title.x = element_text(face = \"bold\", size = 14, margin = margin(t = 15, r = 0, b = 0, l = 0)),\n      axis.title.y = element_text(face = \"bold\", size = 14, margin = margin(t = 0, r = 15, b = 0, l = 0)),\n      strip.text = element_text(face = \"bold\"),\n      axis.text.x = element_text(face = \"bold\", size = 10), \n      axis.text.y = element_text(face = \"bold\", size = 10), \n      axis.ticks.x = element_blank(), \n      axis.ticks.y = element_blank(), \n      strip.background = element_rect(fill = \"grey80\", color = NA),\n      legend.title = element_text(face = \"bold\", size = 14),\n      legend.text = element_text(face = \"bold\", size = 10, color = \"grey25\"),\n    )\n}\n\n# Establish a Custom Color Scheme\ncolors &lt;- c(\n  \"1\" = \"#133a29\",\n  \"2\" = \"#ab3d29\",\n  \"3\" = \"#f9ba6d\",\n  \"4\" = \"#314318\",\n  \"5\" = \"#63221f\"\n)\n\n\n\nIntro\nIt is tempting for practitioners and clients alike to feel that we can develop a “model” that can answer our questions. But the idea of shooting for a single model is flawed, regardless of what you’re trying to do. If you are seeking to do machine learning things, there’s a lot of virtue in ensemble approaches. From the causal inference approach, it is very easy to let a single model do too much and, as a consequence, severely damage the credibility of your results.\nThis is especially a problem when asked to do some sort of “shotgun causal inference”. I have no clue if someone else has used this term before, but the desire for shotgun causal inference is apparent in academic and non-academic settings. It all starts with this question: “what are the predictors/explanations/factors that impact”, etc. What is the problem here? Well, for one, the task is insurmountable. All the things?\nIn reality, such questions can really be boiled down to “find as many variables that you think explain some outcome of interest and test them”. But this is actually a very hard causal inference problem (which is part of the reason why academic hypotheses tend to be narrow and only focus on one potential causal factor for each research project). Even though shotgun causal inference is hard, there is one rule that should never be violated…\nWhen testing multiple hypotheses, do not throw everything into the same model and assume that this model provides interpretable results. To demonstrate why this should be avoided, let’s go over quick refresher on the concept of “bad controls” and what they do. This will be important for understanding why we should not test multiple hypotheses within the same model.\n\n\nA Refresher on the Consequences of “Bad Controls”\nFor those not familiar, I go into greater detail on this subject in a prior blog post. However, the TLDR of this point is that different hypotheses require different adjustment sets. Adjustment sets are the unique combination/specification of covariates that are required to be adjusted for to give any \\(X \\rightarrow Y\\) relationship a causal interpretation.\nThe problem is that the same covariate can have a different causal role for different hypotheses. Confounders (\\(X \\leftarrow Z \\rightarrow Y\\)) should always be controlled for, but what happens when a confounder for the \\(H_{1} \\rightarrow Y\\) relationship is a bad control for the \\(H_{2} \\rightarrow Y\\) relationship?\nLet’s assume that we are interested in testing the \\(H_{1} \\rightarrow Y\\) relationship and we add some other controls that, while necessary for the other hypotheses, are harmful for identifying \\(H_{1} \\rightarrow M \\rightarrow Y\\). Below, I simulate three different situations where the influence of these bad controls ranges from large to medium to small. I simulate two post-treatment controls (a mediator: \\(X \\rightarrow Y\\) and a collider: \\(X \\rightarrow C \\leftarrow Y\\)) and an instance of M-Bias. The idea for this simulation exercise is to show how much our model deviates from estimating the true causal effect when adjusting for other bad covariates.\n\nset.seed(1234)\nn &lt;- 100000\n\n# Data Where \"Bad Controls\" Have a Large Effect\nlarge &lt;- tibble(\n  u1 = rnorm(n, 0, 1),\n  u2 = rnorm(n, 2, 0.5),\n  m_bias = 0.8 * u1 + 0.8 * u2 + rnorm(n, 0, 0.25),\n  confounder = rnorm(n, 0, 1),\n  exposure = confounder * 1.5 + u1 * 0.8 + rnorm(n, 0, 0.5),\n  mediator = exposure * 2 + rnorm(n, 0, 0.5),\n  outcome = (1.5 * exposure + mediator) + confounder * 3 + u2 * 0.8 + rnorm(n, 0, 0.5),\n  collider = exposure * 1.25 + outcome * 0.7 + rnorm(n, 0, 0.5)\n)\n\n# Estimate 4 Regressions Adding Increasingly More Controls\nlarge_unbiased &lt;- lm(outcome ~ exposure + confounder, data = large)\nlarge_mediator &lt;- lm(outcome ~ exposure + confounder + mediator, data = large)\nlarge_collider &lt;- lm(outcome ~ exposure + confounder + mediator + collider, data = large)\nlarge_mbias &lt;- lm(outcome ~ exposure + confounder + mediator + collider + m_bias, data = large)\n\n# Data Where \"Bad Controls\" Have a Medium Effect\nmedium &lt;- tibble(\n  u1 = rnorm(n, 0, 1),\n  u2 = rnorm(n, 2, 0.5),\n  m_bias = 0.4 * u1 + 0.4 * u2 + rnorm(n, 0, 0.25),\n  confounder = rnorm(n, 0, 1),\n  exposure = confounder * 0.75 + u1 * 0.4 + rnorm(n, 0, 0.5),\n  mediator = exposure * 1 + rnorm(n, 0, 0.5),\n  outcome = (1.5 * exposure + mediator) + confounder * 3 + u2 * 0.4 + rnorm(n, 0, 0.5),\n  collider = exposure * 0.625 + outcome * 0.7 + rnorm(n, 0, 0.5)\n)\n\n# Estimate 4 Regressions Adding Increasingly More Controls\nmedium_unbiased &lt;- lm(outcome ~ exposure + confounder, data = medium)\nmedium_mediator &lt;- lm(outcome ~ exposure + confounder + mediator, data = medium)\nmedium_collider &lt;- lm(outcome ~ exposure + confounder + mediator + collider, data = medium)\nmedium_mbias &lt;- lm(outcome ~ exposure + confounder + mediator + collider + m_bias, data = medium)\n\n# Data Where \"Bad Controls\" Have a Small Effect Effect\nsmall &lt;- tibble(\n  u1 = rnorm(n, 0, 1),\n  u2 = rnorm(n, 2, 0.5),\n  m_bias = 0.2 * u1 + 0.2 * u2 + rnorm(n, 0, 0.25),\n  confounder = rnorm(n, 0, 1),\n  exposure = confounder * 0.375 + u1 * 0.2 + rnorm(n, 0, 0.5),\n  mediator = exposure * 0.5 + rnorm(n, 0, 0.5),\n  outcome = (1.5 * exposure + mediator) + confounder * 3 + u2 * 0.2 + rnorm(n, 0, 0.5),\n  collider = exposure * 0.3125 + outcome * 0.7 + rnorm(n, 0, 0.5)\n)\n\nsmall_unbiased &lt;- lm(outcome ~ exposure + confounder, data = small)\nsmall_mediator &lt;- lm(outcome ~ exposure + confounder + mediator, data = small)\nsmall_collider &lt;- lm(outcome ~ exposure + confounder + mediator + collider, data = small)\nsmall_mbias &lt;- lm(outcome ~ exposure + confounder + mediator + collider + m_bias, data = small)\n\nAgain, strictly for testing \\(H_{1}\\), the data generating process can be represented with this DAG:\n\n\nCode\ndgp1_dag &lt;- dagitty('dag {\n    bb=\"0,0,1,1\"\n    \"M_Bias_Node\" [pos=\"0.47,0.35\"]\n    \"Unobserved (Outcome)\" [pos=\"0.545,0.28\"]\n    \"Unobserved (Treatment)\" [pos=\"0.4,0.28\"]\n    Collider [pos=\"0.47,0.56\"]\n    Confounder [pos=\"0.47,0.225\"]\n    Exposure [exposure,pos=\"0.4,0.4\"]\n    Mediator [pos=\"0.47,0.45\"]\n    Outcome [outcome,pos=\"0.545,0.4\"]\n    \"Unobserved (Outcome)\" -&gt; \"M_Bias_Node\"\n    \"Unobserved (Outcome)\" -&gt; Outcome\n    \"Unobserved (Treatment)\" -&gt; \"M_Bias_Node\"\n    \"Unobserved (Treatment)\" -&gt; Exposure\n    Confounder -&gt; Exposure\n    Confounder -&gt; Outcome\n    Exposure -&gt; Collider\n    Exposure -&gt; Mediator\n    Exposure -&gt; Outcome\n    Mediator -&gt; Outcome\n    Outcome -&gt; Collider\n  }') %&gt;%\n  tidy_dagitty() %&gt;%\n  mutate(y = 1 - y, yend = 1 - yend) \n\nggplot(dgp1_dag, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges_link(\n    size = 4,  \n    arrow = arrow(length = unit(0.3, \"cm\"), type = \"closed\"), \n    start_cap = circle(0.05, \"npc\"),  \n    end_cap = circle(0.05, \"npc\")  \n  ) +\n  geom_point(aes(x = x, y = y), color = \"white\", size = 1) +\n  geom_label(aes(\n    label = case_when(\n      name == \"M_Bias_Node\" ~ \"M-Bias\\nCovariate\",\n      name == \"Unobserved (Outcome)\" ~ \"Latent Outcome\\nPredictor\",\n      name == \"Unobserved (Treatment)\" ~ \"Latent Treatment\\nPredictor\",\n      name == \"Collider\" ~ \"Collider\",\n      name == \"Confounder\" ~ \"Confounder\",\n      name == \"Exposure\" ~ \"Exposure\",\n      name == \"Mediator\" ~ \"Mediator\",\n      name == \"Outcome\" ~ \"Outcome\",\n      TRUE ~ name\n    ),\n    fill = case_when(\n      name == \"M_Bias_Node\" ~ \"grey20\",\n      name == \"Unobserved (Outcome)\" ~ \"#ab3d29\",\n      name == \"Unobserved (Treatment)\" ~ \"grey20\",\n      name == \"Collider\" ~ \"grey20\",\n      name == \"Confounder\" ~ \"#63221f\",\n      name == \"Exposure\" ~ \"#314318\",\n      name == \"Mediator\" ~ \"grey20\",\n      name == \"Outcome\" ~ \"#f9ba6d\",\n      TRUE ~ \"grey80\"\n    )\n  ),\n  color = \"white\", fontface = \"bold\", size = 4, label.size = 0.2, label.r = unit(0.2, \"lines\")) +\n  scale_fill_identity() +\n  theme_dag()\n\n\n\n\n\nDAG for Bad Controls DGP\n\n\n\n\nAnd the difference in bias when I iteraviley add more bad controls can be shown in the following figure:\n\n\nCode\neffects &lt;- tibble(\n   dgp = c(\"Bad Controls with Large Effects\", \"Bad Controls with Medium Effects\", \"Bad Controls with Small Effects\"),\n  unbiased = c(large_unbiased$coefficients[2], medium_unbiased$coefficients[2], small_unbiased$coefficients[2]),\n  mediator = c(large_mediator$coefficients[2], medium_mediator$coefficients[2], small_mediator$coefficients[2]),\n  collider = c(large_collider$coefficients[2], medium_collider$coefficients[2], small_collider$coefficients[2]),\n  mbias = c(large_mbias$coefficients[2], medium_mbias$coefficients[2], small_mbias$coefficients[2])\n) %&gt;%\n  mutate(\n    true_effect = c(3.5, 2.5, 2.0),\n    bias_unbiased = true_effect - unbiased,\n    bias_mediator = true_effect - mediator,\n    bias_collider = true_effect - collider,\n    bias_mbias = true_effect - mbias,\n    effect_size = c(\"large\", \"medium\", \"small\")\n  ) %&gt;%\n  select(effect_size, starts_with(\"bias_\")) %&gt;%\n  pivot_longer(\n    cols = starts_with(\"bias_\"),\n    names_to = \"control_type\",\n    names_prefix = \"bias_\",\n    values_to = \"bias\"\n  )\n\nggplot(effects, aes(x = control_type, y = bias, fill = effect_size)) +\n  geom_col(position = position_dodge(width = 0.8), width = 0.7, alpha = 0.9, color = \"black\", size = 0.75) +\n  scale_x_discrete(\n    limits = c(\"unbiased\", \"mediator\", \"collider\", \"mbias\"),\n    labels = c(\n      \"unbiased\" = \"Only Confounder\",\n      \"mediator\" = \"Confounder +\\nMediator\",\n      \"collider\" = \"Confounder +\\nMediator +\\nCollider\",\n      \"mbias\" = \"Confounder +\\nMediator + Collider +\\nM-Bias\"\n    )\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"large\" = \"#133a29\",\n      \"medium\" = \"#ab3d29\",\n      \"small\" = \"#f9ba6d\"\n    ),\n    labels = c(\n      \"large\" = \"Large Effects\",\n      \"medium\" = \"Medium Effects\",\n      \"small\" = \"Small Effects\"\n    )\n  ) +\n  scale_y_continuous(\n    limits = c(0, 3.5), \n    breaks = seq(0, 3.5, by = 0.5) \n  ) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"black\", size = 1) + \n  geom_hline(yintercept = 3.5, linetype = \"dashed\", color = \"#133a29\", size = 1) +\n  geom_hline(yintercept = 2.5, linetype = \"dashed\", color = \"#ab3d29\", size = 1) +\n  geom_hline(yintercept = 2, linetype = \"dashed\", color = \"#f9ba6d\", size = 1) +\n  geom_label(aes(x = 1, y = 3.35, label = \"True Effect = 3.5\"),\n             fill = \"#133a29\", color = \"white\", fontface = \"bold\", size = 4, label.size = 0.4, label.padding = unit(0.1, \"inches\")) + \n  geom_label(aes(x = 1, y = 2.35, label = \"True Effect = 2.5\"),\n             fill = \"#ab3d29\", color = \"white\", fontface = \"bold\", size = 4, label.size = 0.4, label.padding = unit(0.1, \"inches\")) + \n  geom_label(aes(x = 1, y = 1.85, label = \"True Effect = 2.0\"),\n             fill = \"#f9ba6d\", color = \"white\", fontface = \"bold\", size = 4, label.size = 0.4, label.padding = unit(0.1, \"inches\")) +  \n  \n  labs(\n    title = \"\",\n    x = \"Covariates Included\",\n    y = \"True Effect - Estimated Effect\\n(Bias)\",\n    fill = \"'Bad Control'\\nEffect Size\"\n  ) +\n  blog_theme()\n\n\n\n\n\nCumulative Bias When Adding More and More Bad Controls\n\n\n\n\nThis is a lot of information, so what can we get from it? First, for the scenario when the effects of the bad controls are small, the true effect is 2. When the bad controls have a medium-sized effect, the true effect is 2.5. And when the bad controls have a large effect, the true effect is 3.5. The x-axis reports the types of variables that are adjusted for in the model and the y-axis reports the difference between the true effect and the estimated effect. If the difference is 0, then there is no bias since the regression model was able to identify the exact correct causal effect.\nAnd that pans out perfectly when we only adjust for all confounders (and, unlike the real world, it was very easy to do this, since there are not that many confounders in the scenario I simulated). The moment a mediator variable is included the bias jumps considerably. When a collider is included, the amount of bias (for the “bad controls have large effects” model in particular) is almost identical to the size of the true effect itself. That’s bad! Inducing M-Bias doesn’t seem to make things worse than they already are here, as the bias only changes marginally.\nThe takeaways here are that 1) we should be careful with the types of variables that we control for and 2) different \\(X\\)s required different adjustment sets. In the following DAG, we can clearly see a super simplified demonstration of this.\n\n\nCode\ndgp2_dag &lt;- dagitty('dag {\n    bb=\"0,0,1,1\"\n    X1 [exposure,pos=\"0.38,0.43\"]\n    X2 [exposure,pos=\"0.47,0.2\"]\n    X3 [exposure,pos=\"0.58,0.43\"]\n    Y [outcome,pos=\"0.47,0.43\"]\n    Z1 [pos=\"0.38,0.25\"]\n    Z2 [pos=\"0.58,0.25\"]\n    Z3 [pos=\"0.47,0.6\"]\n    X1 -&gt; Y\n    X1 -&gt; Z3\n    X2 -&gt; Y\n    X2 -&gt; Z1\n    X3 -&gt; Y\n    Z1 -&gt; X1\n    Z1 -&gt; Y\n    Z2 -&gt; X2\n    Z2 -&gt; Y\n    Z3 -&gt; X3\n    Z3 -&gt; Y\n  }')\n\nggplot(dgp2_dag, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges_link(\n    size = 4,  \n    arrow = arrow(length = unit(0.3, \"cm\"), type = \"closed\"), \n    start_cap = circle(0.05, \"npc\"),  \n    end_cap = circle(0.05, \"npc\")  \n  ) +\n  geom_point(aes(x = x, y = y), color = \"white\", size = 1) +\n  geom_label(aes(\n    label = case_when(\n      name == \"X1\" ~ \"Exposure 1\",\n      name == \"X2\" ~ \"Exposure 2\",\n      name == \"X3\" ~ \"Exposure 3\",\n      name == \"Z1\" ~ \"Coviarate 1\",\n      name == \"Z2\" ~ \"Coviarate 2\",\n      name == \"Z3\" ~ \"Coviarate 3\",\n      name == \"Y\" ~ \"Outcome\",\n      TRUE ~ name\n    ),\n    fill = case_when(\n      name == \"X1\" ~ \"#314318\",\n      name == \"X2\" ~ \"#314318\",\n      name == \"X3\" ~ \"#314318\",\n      name == \"Z1\" ~ \"#63221f\",\n      name == \"Z2\" ~ \"#63221f\",\n      name == \"Z3\" ~ \"#63221f\",\n      name == \"Y\" ~ \"#f9ba6d\",\n      TRUE ~ \"grey80\"\n    )\n  ),\n  color = \"white\", fontface = \"bold\", size = 4, label.size = 0.2, label.r = unit(0.2, \"lines\")) +\n  scale_fill_identity() +\n  theme_dag()\n\n\n\n\n\nDAG with Three Exposures and Adjustment Sets\n\n\n\n\nAccording to this DAG, if we want to estimate the effect of Exposure 1 on the Outcome, we need to adjust for Covariate 1. But if we were to do this all in one model, we would cause a post-treatment bias for evaluating the \\(X_{2] \\rightarrow Y\\) relationship by blocking the Exposure 2 \\(\\rightarrow\\) Covariate 1 \\(\\rightarrow\\) Outcome path. And if we want to estimate the effect of Exposure 3 on the Outcome, we need to adjust for Covariate 3, but then we can clearly see that adjusting for Covariate 3 blocks the Exposure 1 \\(\\rightarrow\\) Covariate 3 \\(\\rightarrow\\) Outcome path. If you choose to include everything in one model, you are truly damned if you do and damned if you don’t!\nSo what to do instead? Simply give your hypotheses space! According to this DAG, to test the effect of Exposure 1 on Outcome, we only need to adjust for Covariate 1. To estimate the effect of Exposure 2 on Outcome, we only need to adjust for Covariate 2, etc. There is no one magic model. Trying to include everything in one model is a near guarantee to get very bad, not true results.\n\n\nDon’t Bother Interpreting Your Controls\nWhen estimating a regression model, you get a lot of numbers back and it’s tempting to try and glean some information from each other. In particular, if we are on the same page and agree that we should only use one model to test each hypothesis, then the vast majority of our coefficients are for our control variables. Surely all that information is useful?\nNope! At least, it probably is not, and the reasons discussed above help explain why coefficients for control variables ought to be avoided. Recall that we design models to test a specific hypothesis. That is, we use control variables to de-confound the relationship of interest (whether that is the effect of \\(H_{1}\\), \\(H_{2}\\), etc.). Subsequently, any other estimate from that model is most likely, not de-confounded. To demonstrate this, I am going to simulate some data with one effect of interest that is confounded by five covariates. However, each of these covariates is itself confounded by another confounder that confounds the Confouder \\(\\rightarrow Y\\) relationship, but does not confound the \\(X \\rightarrow Y\\) relationship.\n\nset.seed(1234)\nn &lt;- 100000\n\ntable2 &lt;- tibble(\n  # Generate a Bunch of Unobserved Variables That Don't Have Anything to Do with X\n  u1 = rnorm(n, 0, 1),\n  u2 = rnorm(n, 1, 0.5),\n  u3 = rnorm(n, 0.5, 0.25),\n  u4 = rnorm(n, 2, 1.5),\n  u5 = rnorm(n, 0.75, 1),\n  # Generate Variables That Confound the X -&gt; Y Relationship\n  z1 = u1 * 0.3 + rnorm(n, 0, 1),\n  z2 = u2 * 0.6 + rnorm(n, 0, 0.25),\n  z3 = u3 * 1.2 + rnorm(n, 0, 0.5),\n  z4 = u4 * -0.5 + rnorm(n, 0, 1),\n  z5 = u5 * -1.5 + rnorm(n, 0, 0.75),\n  # Create an Exposure of Interest\n  x = 0.2 * z1 + 1.6 * z2 + -0.3 * z3 + -0.75 * z4 + 0.15 * z5 + rnorm(n, 0, 0.25),\n  # Create an Outcome That is Impacted by All Variables\n  y = 0.3 * u1 + 0.15 * u2 + 0.5 * u3 + -0.5 * u4 + 1.2 * u5 +\n      0.8 * z1 + -1.7 * z2 + 0.9 * z3 + -0.8 * z4 + -0.7 * z5 +\n      1.25 * x + rnorm(n, 0, 0.15)\n)\n\ntable2_reg &lt;- lm(y ~ x + z1 + z2 + z3 + z4 + z5, data = table2)\n\nWe can show this data generating process with this messy DAG (and I’m keeping it messy for a reason!)\n\n\nCode\ndgp3_dag &lt;- dagitty('dag {\n    bb=\"0,0,1,1\"\n    U1 [pos=\"0.47,0.83\"]\n    U2 [pos=\"0.47,0.28\"]\n    U3 [pos=\"0.52,0.80\"]\n    U4 [pos=\"0.36,0.80\"]\n    U5 [pos=\"0.47,0.54\"]\n    X [exposure,pos=\"0.36,0.58\"]\n    Y [outcome,pos=\"0.57,0.58\"]\n    Z1 [pos=\"0.47,0.73\"]\n    Z2 [pos=\"0.47,0.38\"]\n    Z3 [pos=\"0.57,0.69\"]\n    Z4 [pos=\"0.36,0.69\"]\n    Z5 [pos=\"0.47,0.45\"]\n    U1 -&gt; Y\n    U1 -&gt; Z1\n    U2 -&gt; Y\n    U2 -&gt; Z2\n    U3 -&gt; Y\n    U3 -&gt; Z3\n    U4 -&gt; Y\n    U4 -&gt; Z4\n    U5 -&gt; Y\n    U5 -&gt; Z5\n    X -&gt; Y\n    Z1 -&gt; X\n    Z1 -&gt; Y\n    Z2 -&gt; X\n    Z2 -&gt; Y\n    Z3 -&gt; X\n    Z3 -&gt; Y\n    Z4 -&gt; X\n    Z4 -&gt; Y\n    Z5 -&gt; X\n    Z5 -&gt; Y\n  }')\n\nggplot(dgp3_dag, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges_link(\n    size = 4,  \n    arrow = arrow(length = unit(0.3, \"cm\"), type = \"closed\"), \n    start_cap = circle(0.05, \"npc\"),  \n    end_cap = circle(0.05, \"npc\")  \n  ) +\n  geom_point(aes(x = x, y = y), color = \"white\", size = 1) +\n  geom_label(aes(\n    label = case_when(\n      name == \"X\" ~ \"Exposure\",\n      name == \"Y\" ~ \"Outcome\",\n      name == \"Z1\" ~ \"Confounder 1\",\n      name == \"Z2\" ~ \"Confounder 2\",\n      name == \"Z3\" ~ \"Confounder 3\",\n      name == \"Z4\" ~ \"Confounder 4\",\n      name == \"Z5\" ~ \"Confounder 5\",\n      name == \"U1\" ~ \"Confounder of\\nConfounder 1\",\n      name == \"U2\" ~ \"Confounder of\\nConfounder 2\",\n      name == \"U3\" ~ \"Confounder of\\nConfounder 3\",\n      name == \"U4\" ~ \"Confounder of\\nConfounder 4\",\n      name == \"U5\" ~ \"Confounder of\\nConfounder 5\",\n      TRUE ~ name\n    ),\n    fill = case_when(\n      name == \"X\" ~ \"#314318\",\n      name == \"Y\" ~ \"#f9ba6d\",\n      name == \"Z1\" ~ \"#63221f\",\n      name == \"Z2\" ~ \"#63221f\",\n      name == \"Z3\" ~ \"#63221f\",\n      name == \"Z4\" ~ \"#63221f\",\n      name == \"Z5\" ~ \"#63221f\",\n      name == \"U1\" ~ \"grey20\",\n      name == \"U2\" ~ \"grey20\",\n      name == \"U3\" ~ \"grey20\",\n      name == \"U4\" ~ \"grey20\",\n      name == \"U5\" ~ \"grey20\",\n      TRUE ~ \"grey80\"\n    )\n  ),\n  color = \"white\", fontface = \"bold\", size = 4, label.size = 0.2, label.r = unit(0.2, \"lines\")) +\n  scale_fill_identity() +\n  theme_dag()\n\n\n\n\n\nMessy DAG\n\n\n\n\nAgain, what is this DAG showing us? It’s showing us that we need to adjust for Confounders 1-5 in order to de-confound the \\(X \\rightarrow Y\\) relationship. However, if we want to de-confound the Confounder \\(\\rightarrow Y\\) relationship, we need to adjust for Confounder of Confounders 1-5 as well. Now, imagine that we can’t control for these confounder of confounders. Perhaps they are concepts that are difficult to measure or that we simply don’t have the data on hand (maybe we haven’t even though to collect the data!) Or, as demonstrated earlier, perhaps including some confounder of confounder introduces bias for the relationship that we are the most interested in estimating (\\(X \\rightarrow Y\\)).\nWith that being said, let’s estimate a model regressing \\(Y\\) on \\(X\\), \\(Z1\\), \\(Z2\\), \\(Z3\\), \\(Z4\\), and \\(Z5\\) and let’s see how well the coefficients align with the vaues that we know to be true from our simulation.\n\n\nCode\ntrue_values &lt;- c(\n  \"x\"  = 1.25,\n  \"z1\" = 0.8,\n  \"z2\" = -1.7,\n  \"z3\" = 0.9,\n  \"z4\" = -0.8,\n  \"z5\" = -0.7\n)\n\n# Extract Coefficients\nestimates &lt;- coef(table2_reg)[names(true_values)]\n\n# Calculate Bias\nbias_vals &lt;- round(estimates - true_values, 3)\n\n# Combine True Effect and Bias\nextra_columns &lt;- data.frame(\n  \"True Value\" = true_values,\n  \"Bias\" = bias_vals\n)\n\nmodelsummary(\n  models = list(\"Coefficient\" = table2_reg),\n  output = \"markdown\",\n  gof_omit = \".\",\n  coef_map = c(\n    \"x\" = \"Exposure (X)\",\n    \"z1\" = \"Covariate Z1\",\n    \"z2\" = \"Covariate Z2\",\n    \"z3\" = \"Covariate Z3\",\n    \"z4\" = \"Covariate Z4\",\n    \"z5\" = \"Covariate Z5\"\n  ),\n  statistic = NULL,\n  stars = TRUE,\n  add_columns = extra_columns\n)\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                Coefficient\n                True.Value\n                Bias\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  Exposure (X)\n                  1.249***\n                  1.250\n                  -0.001\n                \n                \n                  Covariate Z1\n                  0.880***\n                  0.800\n                  0.080\n                \n                \n                  Covariate Z2\n                  -1.550***\n                  -1.700\n                  0.150\n                \n                \n                  Covariate Z3\n                  1.006***\n                  0.900\n                  0.106\n                \n                \n                  Covariate Z4\n                  -0.439***\n                  -0.800\n                  0.361\n                \n                \n                  Covariate Z5\n                  -1.341***\n                  -0.700\n                  -0.641\n                \n        \n      \n    \n\n\n\nAs you might imagine, our coefficient is just about spot on at recovering the average causal effect of \\(X\\) on \\(Y\\). Not shocking since it is confounded by exactly 5 variables and we adjust for all 5. However… things start to go downhill as we try and give a causal interpretation for the control variables…\nCovariates 1-3 aren’t too bad, although they are for sure biased, but the magnitude of bias isn’t nearly as severe as Covariates 4 and 5. But if all the covaraites are biased, does that make them “good” or “bad” controls? Well, they’re bad if you’re trying to interpret them, but you should… not be trying to interpret them. What makes a control variable good is it’s ability to de-confound the \\(X \\rightarrow Y\\) relationship. Don’t ask your controls to do more than that.\nIn other terms, folks will also refer to the role of control variables as “nuisance parameters”. In other words, while adjusting for them is necessary for de-confounding purposes, they themselves have no interpretability. If you’re familiar with alternative approaches like matching, this intuition makes more sense because, with matching, there are no control variable parameters to interpret in the first place. The design of matching itself adjusts for the control variables, but because you don’t get any statistical output along with those control variables, there is nothing to interpret.\nSimilarly, we should treat regression output for controls in a similar manner. Yes, it is there and yes it exists, but it is functionally meaningless. The controls have done their work to get us our coefficient for \\(X\\) and we should not ask for them to do anything more than that. So, why do we publish these super long regression tables with all of these numbers if most of them are just nuisance parameters? I don’t know! It’s kind of a waste of space! But all you and I can really do is not do this and save more page space and word counts for ourselves!\n\n\nConclusion\nIn conclusion, I’ll return to the title of this blog and emphasize give your hypotheses space! The key takeaways from this are montra are:\n\nUnderstand that each hypothesis requires its own model (it’s own space) and its own unique adjustment set.\nWhen asked to find out what the “factors that explain \\(Y\\) are”, realize that this is an incredibly complex task that will require a lot of models and a lot of time to think through what each adjustment set will need to be. You cannot throw 30 variables into a model and expect something causally useful.\nOnly interpret the output for your exposure of interest. Control variable output is causally uninterpretable."
  },
  {
    "objectID": "blog/2025/extrapolation/index.html",
    "href": "blog/2025/extrapolation/index.html",
    "title": "Be Wary of Regression-Based Causal Effects! (Or At Least Be Careful)…",
    "section": "",
    "text": "Code\n# Load Libraries\npacman::p_load(\n  \"dplyr\", # Data Manipulation\n  \"ggplot2\", # Data Visualization\n  \"tidyr\", # Re-Shaping\n  \"scales\", # Scaling Visualizations\n  \"ggtext\", # Colored Text in Plots\n  \"gt\", # Nice Tables\n  install = FALSE\n)\n\n# Define a Custom Theme\nblog_theme &lt;- function() {\n  theme_bw() +  \n    theme(\n      panel.grid.major = element_line(color = \"gray80\", size = 0.3),\n      panel.grid.minor = element_blank(),\n      panel.border = element_blank(),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      plot.title = element_text(face = \"bold\", size = 16, margin = margin(t = 0, r = 0, b = 15, l = 0)),\n      axis.title.x = element_text(face = \"bold\", size = 14, margin = margin(t = 15, r = 0, b = 0, l = 0)),\n      axis.title.y = element_text(face = \"bold\", size = 14, margin = margin(t = 0, r = 15, b = 0, l = 0)),\n      strip.text = element_text(face = \"bold\"),\n      axis.text.x = element_text(face = \"bold\", size = 10), \n      axis.text.y = element_text(face = \"bold\", size = 10), \n      axis.ticks.x = element_blank(), \n      axis.ticks.y = element_blank(), \n      strip.background = element_rect(fill = \"grey80\", color = NA),\n      legend.title = element_text(face = \"bold\", size = 14),\n      legend.text = element_text(face = \"bold\", size = 10, color = \"grey25\"),\n    )\n}\n\n# Establish a Custom Color Scheme\ncolors &lt;- c(\n  \"1\" = \"#133a29\",\n  \"2\" = \"#ab3d29\",\n  \"3\" = \"#f9ba6d\",\n  \"4\" = \"#314318\",\n  \"5\" = \"#63221f\"\n)\n\n\nRegression is used reflexively in statistics, really regardless of what you’re trying to do. Clearly, the tool has earned its place and I am not targeting it. I use regression all of the time and will use it probably for as long as I live. However, in this blog, I am going to demonstrate what many others have already documented thoroughly.\nPrimarily, that researchers should be aware of their data and aware of what estimates regression can produce even if the data used to estimate a regression model violate key assumptions for making causal inferences. For this problem in particular, output from really any statistical package/software will not tell you “THIS IS A PROBLEM” when you, in fact, have run into this specific problem. Instead, the burden is on us as the researcher to catch the problem (if the problem exists).\nBefore getting to the problem itself, it is useful to return to the potential outcomes framework (POF) for making causal inferences. After reviewing the POF, the tricky thing about regression to be on the lookout for should be more apparent.\n\nRefresher on Potential Outcomes\nThe POF is a way of conceptualizing and formalizing causality and causal effects. The basic idea is that we can identify a causal effect by looking a unit who is exposed or not exposed to some exposure of interest and compare their outcome to the exact same unit whose exposure status is different. By doing so, we isolate the effect of the exposure of interest and so we don’t have to worry about bias resulting from other things that affect the exposure and outcome of interest.\nThe fundamental problem of causal inference is that observing (in the case of a binary exposure, for example) both potential outcomes is impossible. You either received the exposure or you didn’t. We need both ingredients to know the causal effect (\\(Y_{X_1} - Y_{X_0}\\)) but we only ever observe one.\nThankfully, absent a time machine, clever designs and statistical methods allow us to make inferences about average causal effects rather than more precise individual-level causal effects. If we had access to individual-level causal effects, we could also make average inferences by taking the average of the exposed and control outcomes and then take the difference. Below is a plot that shows simulated potential outcomes for 100 units who are randomly assigned to exposure/control. Also, note that I got this plot idea from Dr. Julia Rohrer, so credit to her for this visualization.\n\n\nCode\n# Simulate Data\nset.seed(1234)\nn &lt;- 100\n\n# Simulate Potential Outcomes and Random Treatment Assignment\nY_X_0 &lt;- rnorm(n, 0, 0.05)\nY_X_1 &lt;- 0.2 + rnorm(n, 0, 0.05)\nX &lt;- rbinom(n, 1, 0.5)\nY &lt;- ifelse(X == 1, Y_X_1, Y_X_0)\n\n# Create a Data Frame\nrandom_assignment &lt;- data.frame(\n  id = 1:n,\n  X = X,\n  Y = Y,\n  Y_X_0 = Y_X_0,\n  Y_X_1 = Y_X_1\n)\n\n# Create a Long Data Set\nrandom_assignment_long &lt;- random_assignment %&gt;%\n  pivot_longer(cols = c(Y_X_0, Y_X_1),\n               names_to = \"potential_type\",\n               values_to = \"value\") %&gt;%\n  mutate(\n    potential_type = recode(potential_type,\n                            Y_X_0 = \"Control\",\n                            Y_X_1 = \"Treated\")\n  )\n\n# Create a Wide Data Set for Connecting Lines\nrandom_assignment_wide &lt;- random_assignment_long %&gt;%\n  pivot_wider(names_from = potential_type, values_from = value)\n\n# Extract Density for Treated and Control for Overlay Plotting\nY_X_0_dens &lt;- density(random_assignment$Y_X_0)\nY_X_1_dens &lt;- density(random_assignment$Y_X_1)\n\ndensity &lt;- bind_rows(\n  data.frame(x = Y_X_0_dens$x, y = rescale(Y_X_0_dens$y, to = c(1, 100)), type = \"Control\"),\n  data.frame(x = Y_X_1_dens$x, y = rescale(Y_X_1_dens$y, to = c(1, 100)), type = \"Treated\"))\n\n# Create the Dot Plots with Overlayed Distributions\nggplot() +\n  # Filled Density for Control\n  geom_area(data = density %&gt;% filter(type == \"Control\"),\n            aes(x = x, y = y, fill = type),\n            alpha = 0.3) +\n  # Filled Density for Treated\n  geom_area(data = density %&gt;% filter(type == \"Treated\"),\n            aes(x = x, y = y, fill = type),\n            alpha = 0.3) +\n  # Thicker Outline for Control Density\n  geom_line(data = density %&gt;% filter(type == \"Control\"),\n            aes(x = x, y = y, color = type),\n            size = 1.5) +\n  # Thicker Outline for Treated Density\n  geom_line(data = density %&gt;% filter(type == \"Treated\"),\n            aes(x = x, y = y, color = type),\n            size = 1.5) +\n  # Connect Lines Between Potential Outcomes Dots\n  geom_segment(data = random_assignment_wide,\n               aes(x = Control, xend = Treated,\n                   y = id, yend = id),\n               color = \"gray30\",\n               size = 0.5) +\n  # Dots for Potential Outcomes\n  geom_point(data = random_assignment_long,\n             aes(x = value, y = id, color = potential_type),\n             size = 2.5) +\n  scale_color_manual(values = c(\"Control\" = \"#133a29\", \"Treated\" = \"#63221f\")) +\n  scale_fill_manual(values = c(\"Control\" = \"#133a29\", \"Treated\" = \"#63221f\")) +\n  labs(\n    title = \"\",\n    subtitle = \"Difference between the average &lt;b&gt;&lt;span style='color:#63221f'&gt;exposed outcome (0.205)&lt;/span&gt;&lt;/b&gt; and the average &lt;b&gt;&lt;span style='color:#133a29'&gt;control outcome (0.005)&lt;/span&gt;&lt;/b&gt;\",\n    x = \"Potential Outcome Value\",\n    y = \"\",\n    color = \"Exposure Status\",\n    fill = \"Exposure Status\"\n  ) +\n  xlim(-0.2, 0.4) +\n  blog_theme() +\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    legend.position = \"none\",\n    plot.subtitle = ggtext::element_markdown(size = 11)\n  )\n\n\n\n\n\nDistribution of Individual-Level Potential Outcomes\n\n\n\n\nThis plot connects each unit’s potential outcomes (via the gray line) between their known outcome when they are exposed (red) v. when they are not exposed (green). The average difference between the two 0.2 units which, if you look at the simulation code, is the exact causal effect that I specified it to be.\nBut, as I said before, we only ever know one of the potential outcomes. We will never know both, but we are still interested in estimating the average causal effect. How do we do this? Well, because I randomized which units were actually exposed to the exposure (\\(X\\)), this allows us to estimate the average causal effect free from bias (check out this blog post if you want to know how that process works).\nAnd because \\(X\\) is randomized, I can simply regress the observed outcomes on the randomized exposure and I should also get 0.2 (the average causal effect we know to be true because I simulated data explicitly ordering it to be as such!)\n\nlm(Y ~ X, data = random_assignment)\n\n\nCall:\nlm(formula = Y ~ X, data = random_assignment)\n\nCoefficients:\n(Intercept)            X  \n  -0.002366     0.199678  \n\n\nYup, and that’s what we see! 0.199678 is pretty close to 0.2! But, what happens when \\(X\\) is not randomly assigned? Here, we run into a challenging problem. Many researchers address this by operating under a causal identification strategy known as “selection on observables” (see more about this here) where we statistically adjust for each variable that influences both \\(X\\) and \\(Y\\). But that itself introduces some problems that we will get into for the next section.\nAnd yes, these problems are the ones that warrant the caution about regression that this blog is titled after.\n\n\nPositivity/Common Support\nA key assumption for causal inference is the positivity assumption (also known as common support). The key point is that, within each combination of covariates (aka stratum), the probability of any unit being exposed to the exposure is non-zero. That is, however the data is separated (i.e. \\(Z_1\\) = 1 and \\(Z_2\\) = 1 or \\(Z_1\\) = 1 and \\(Z_2\\) = 0, etc.), there should always be a possibility for exposure.\nThe following plot demonstrates this by looking at an exposure-outcome relationship confounded by two binary confounders. It plots the percent of observations that, within each covariate combination, are exposed v. control.\n\n\nCode\n# Simulate Data\nset.seed(1234)\nn &lt;- 10000\n\n# Simulate Potential Outcomes and Random Treatment Assignment\nZ1 &lt;- rbinom(n, 1, 0.3)\nZ2 &lt;- rbinom(n, 1, 0.7)\nX &lt;- rbinom(n, 1, prob = plogis(0.5 + 0.3 * Z1 + -0.6 * Z2))\nY_X_0 &lt;- 0 + 0.4 * Z1 + 0.7 * Z2 + rnorm(n, 0, 0.25)\nY_X_1 &lt;- 0.2 + 0.4 * Z1 + 0.7 * Z2 + rnorm(n, 0, 0.25)\nY &lt;- Y_X_1 * X + Y_X_0 * (1 - X) \n\n# Create a Data Frame\nconfounded &lt;- data.frame(\n  id = 1:n,\n  X = X,\n  Y = Y,\n  Y_X_0 = Y_X_0,\n  Y_X_1 = Y_X_1,\n  Z1 = Z1,\n  Z2 = Z2\n)\n\n# Create a Grouped Bar Plot to Show Exposure/Control Distribution per Covariate Combination\nconfounded %&gt;%\n  mutate(group = paste0(\"Z1 = \", Z1, \", Z2 = \", Z2)) %&gt;%\n  count(group, X) %&gt;%\n  group_by(group) %&gt;%\n  mutate(percentage = n / sum(n) * 100,\n         status = ifelse(X == 1, \"Exposure\", \"Control\")) %&gt;%\n  ungroup() %&gt;%\n  ggplot(aes(x = group, y = percentage, fill = status)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", width = 0.7, color = \"black\", size = 0.75) +\n  geom_text(aes(label = paste0(round(percentage, 1), \"%\")),\n            position = position_dodge(width = 0.7),\n            vjust = -0.5, size = 3.5, fontface = \"bold\") +\n  scale_fill_manual(values = c(\"Control\" = \"#133a29\", \"Exposure\" = \"#63221f\")) +\n  scale_y_continuous(labels = function(x) paste0(x, \"%\")) +\n  labs(\n    title = \"\",\n    x = \"Covariate Combinations\",\n    y = \"Percentage of Observations\",\n    fill = \"\"\n  ) +\n  blog_theme()\n\n\n\n\n\nExposure/Control Distribution Across Covariate Combinations\n\n\n\n\nEven though, within each stratum, the distribution of the exposure is not even, the probability is all non-zero. To put this in other terms, we observe both exposed and control for units where \\(Z_1\\) = 0 and where \\(Z_2\\) = 0, we observe both exposed and control for units where \\(Z_1\\) = 0 and where \\(Z_2\\) = 1, etc. Within each stratum, we are able to observe counterfactuals for each exposure condition (exposed v. control) and are thus able to reconstruct the potential outcomes.\nThis situation is good, but it is also incredibly naive (for one, there are only two binary confounders… very unlikely in real life.) We also are lucky because we happen to not struggle with a positivity violation here. But what might such a violation look like?\n\n\nViolating Posivity with Regression is Easy!\nTo dive into this question, I am going to borrow from the same data, but I am going to make it so that the stratum \\(Z_1\\) = 1 and \\(Z_2\\) = 1 are never exposed.\n\nlack_overlap &lt;- confounded %&gt;%\n  mutate(X = ifelse(Z1 == 1 & Z2 == 1, 0, X))\n\nAdmittedly, the way I am handling a lack of overlap is a bit heavy handed. As you can see in the code, I am simulating a data set where there is full treatment overlap in the covariates, but I am artificially creating a new data set from the simulated one where one combination of covariates results in the exposure always being 0. However, there are several ways that this could happen with real data.\nFor example, perhaps simply due to sampling error, you manage to get a combination of covariates where there is only one treatment status. Or, perhaps we naturally observe in the world that a specific combination of covariates happens to always lead to one treatment status. For example, assume that the exposure of interest in a certain study is a binary indicator of civil conflict. Imagine that I control for GDP per capita and the presence of a UN peacekeeping operation. I imagine that it is very unlikely to ever observe an instance where a rich country that does not have a UN peacekeeping operation is also experiencing a civil war. This probably sounds a bit niche, but a lack of overlap might not be incredibly systematic. It truly might only exist in one stratum of your data.\nBut, all that being said, the following bar plot helps visualize why a lack of overlap is an issue.\n\n\nCode\n# Create a Grouped Bar Plot to Show Exposure/Control Distribution per Covariate Combination\nlack_overlap %&gt;%\n  mutate(group = paste0(\"Z1 = \", Z1, \", Z2 = \", Z2)) %&gt;%\n  count(group, X) %&gt;%\n  mutate(status = ifelse(X == 1, \"Exposure\", \"Control\")) %&gt;%\n  select(-X) %&gt;%\n  complete(group, status, fill = list(n = 0)) %&gt;%\n  group_by(group) %&gt;%\n  mutate(percentage = n / sum(n) * 100) %&gt;%\n  ungroup() %&gt;%\n  ggplot(aes(x = group, y = percentage, fill = status)) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.7),\n           width = 0.7, color = \"black\", size = 0.75) +\n  geom_text(aes(label = paste0(round(percentage, 1), \"%\")),\n            position = position_dodge(width = 0.7),\n            vjust = -0.5, size = 3.5, fontface = \"bold\") +\n  scale_fill_manual(values = c(\"Control\" = \"#133a29\", \"Exposure\" = \"#63221f\")) +\n  scale_y_continuous(labels = function(x) paste0(x, \"%\")) +\n  labs(\n    title = \"\",\n    x = \"Covariate Combinations\",\n    y = \"Percentage of Observations\",\n    fill = \"\"\n  ) +\n  blog_theme()\n\n\n\n\n\nExposure/Control Distribution Across Covariate Combinations with Positivity Violated\n\n\n\n\nSo, in the \\(Z_1\\) = 1 and \\(Z_2\\) = 1 stratum, all units are control observations… What consequence might that have for estimating the causal effect of the exposure (\\(X\\))? Well, recall that the true causal effect is 0.2. And, we can demonstrate that we can estimate this causal effect even with confounders through the following regression:\n\nlm(Y ~ X + Z1 + Z2, data = confounded)\n\n\nCall:\nlm(formula = Y ~ X + Z1 + Z2, data = confounded)\n\nCoefficients:\n(Intercept)            X           Z1           Z2  \n  -0.003636     0.201851     0.398327     0.700668  \n\n\nSo, that being said, how does the coefficient for \\(X\\) change using the data where the positivity assumption is violated? Let’s check that out!\n\nlack_overlap_mod &lt;- lm(Y ~ X + Z1 + Z2, data = lack_overlap)\n\nlack_overlap_mod\n\n\nCall:\nlm(formula = Y ~ X + Z1 + Z2, data = lack_overlap)\n\nCoefficients:\n(Intercept)            X           Z1           Z2  \n   -0.01668      0.18888      0.47062      0.72909  \n\n\nAs we can see, we have found some bias even though all the confounders are accounted for. So clearly, violating the positivity assumption, in some way, matters for estimating unbiased causal effects. But… how exactly does that work? Well, the bias can be explained, in part, by what is happening on a more micro-level.\nReporting average marginal effects (AMEs) is a popular strategy for reporting predicted causal effects from regression output. I bring this up because how AMEs are calculated can help demonstrate where the bias of positivity violations comes from.\nThe basic idea of AMEs is this:\n\nEstimate a model regressing the outcome on the exposure of interest + all confounders\nUse the model to predict the outcome by “plugging in” the exposure and confounder values. For example, if a unit was exposed to treatment and had a \\(Z1\\) and \\(Z2\\) value of 0, than the marginal effect for that unit, using our biased regression model, would be calculated using: \\(Y = -0.017 + 0.189 (1) + 0.471 (0) + 0.729 (0)\\). So, for this unit, the predicted outcome when exposed to the treatment would be 0.172\nUse the model to predict the potential outcome by changing the exposure value in the regression equation. Again, using our hypothetical example, we are going to “plug in” all the same values but just change the exposure value to control, which would look like: \\(Y = -0.017 + 0.189 (0) + 0.471 (0) + 0.729 (0)\\). So, for this unit, the predicted outcome when not exposed to the treatment would be -0.017.\nTake the difference between the observed predicted outcome and the counterfactual predicted outcome: 0.172 - -0.017 = 0.189 (this is our coefficient!)\nDo this for every unit and take the average difference of the differences. That is our average marginal effect.\n\nSo how do AMEs help explain why our coefficient becomes biased when positivity is violated? Well, let’s return to our pesky stratum of \\(Z_1\\) = 1 and \\(Z_2\\) = 1. Remember, we know that all units in this stratum are controls. So we can predict the outcome for these units easily by: \\(Y = -0.017 + 0.189 (0) + 0.471 (1) + 0.729 (1)\\) which sums to 1.183. Easy enough, but what about the next step. We want to know the counterfactual and that formula would be: \\(Y = -0.017 + 0.189 (1) + 0.471 (1) + 0.729 (1)\\).\nUh-oh… we already know that, within the \\(Z_1\\) = 1 and \\(Z_2\\) = 1 stratum, no units are exposed… so how can we know what the impact of exposure would be? Well, our model will gladly give us a predicted outcome, but that outcome is not generated from any real data points within that stratum. In other words, it is made up! Or, in fancier terms, regression is extrapolating beyond the support of the data. It is making a guess without any data points as reference. That’s kind of a problem! And we see the fruits of this problem bear out in a biased coefficient.\nNow, because I know what each potential outcome actually is (because I simulated it), we can do something really interesting where we estimate the predicted counterfactual outcome (when units in this stratum are exposed) and compare it to the true counterfactual outcome. So, that’s what the following table reports.\nFrom the data set of 10,000 units, I randomly sampled observations from the \\(Z_1\\) = 1 and \\(Z_2\\) = 1 stratum and compared their predicted counterfactual outcome to the true counterfactual outcome. In an ideal world, our predicted counterfactual outcome is equivalent to the true counterfactual outcome.\n\n# Estimate Potential Outcomes for Units with No Covariate Overlap\nsampled &lt;- lack_overlap %&gt;%\n  filter(Z1 == 1, Z2 == 1) %&gt;%\n  filter(Y != Y_X_1) %&gt;%\n  sample_n(5)\n\nsampled_counterfactual &lt;- sampled %&gt;%\n  mutate(X = 1)\n\nsampled_counterfactual &lt;- sampled_counterfactual %&gt;%\n  mutate(Y_pred = predict(lack_overlap_mod, newdata = sampled_counterfactual))\n\n# Present Select Predicted Results with a Table\nsampled_counterfactual %&gt;%\n  mutate(bias = Y_X_1 - Y_pred) %&gt;%\n  select(id, Y, Y_pred, Y_X_1, bias) %&gt;%\n  gt() %&gt;%\n  tab_header(title = \"Counterfactual Outcome Predictions\") %&gt;%\n  cols_label(\n    id = \"Observation #\",\n    Y = \"Real Outcome\",\n    Y_pred = \"Predicted Counterfactual\",\n    Y_X_1 = \"True Counterfactual\",\n    bias = \"Bias\"\n  ) %&gt;%\n  fmt_number(columns = c(Y, Y_pred, Y_X_1, bias), decimals = 3) %&gt;%\n  cols_align(\n    align = \"center\",\n    columns = everything()\n  )\n\n\n\n\n\n\n\nCounterfactual Outcome Predictions\n\n\nObservation #\nReal Outcome\nPredicted Counterfactual\nTrue Counterfactual\nBias\n\n\n\n\n8171\n0.961\n1.372\n1.729\n0.357\n\n\n7383\n1.144\n1.372\n1.103\n−0.269\n\n\n2682\n1.213\n1.372\n1.291\n−0.081\n\n\n8621\n0.991\n1.372\n0.995\n−0.377\n\n\n7492\n0.556\n1.372\n1.129\n−0.243\n\n\n\n\n\n\n\nAnd, unsurprisingly, we see that there is bias… No shocker there.\nSo, with all of this being said, what are you supposed to do if positivity is violated? Or, for the keen readers out there, how are you supposed to check for positivity? Most of us are working with adjustment sets that have continuous, ordinal, and count confounders in addition to binary confounders. And we certainly have a lot more than two… How are you supposed to check for positivity under these conditions?\n\n\nChecking for Posivity or Considering Other Approaches\nIn other words, we might say that realistic situations involve adjustment sets that are multi-dimensional. If we just had two binary confounders, we can check for overlap with a simple 2x2 square. But adding more confounders and more different types of confounders makes such a simplistic approach impossible.\nOne solution is to estimate a propensity score and check for overlap between exposed and control units. For those not familiar, the propensity score is simply the predicted probability of a unit receiving their level of exposure conditional on the confounders. You can do this with something as simple as a logit model. In the context of our simple two-confounders example, you could get propensity scores by:\n\nps_score_model &lt;- glm(X ~ Z1 + Z2, data = confounded, family = binomial(link = \"logit\"))\n\nconfounded$ps &lt;- predict(ps_score_model, type = \"response\")\n\nThe propensity score is useful because it collapses all of the information from \\(k\\) confounders and summarizes what we need to know (the probability of receiving treatment) into one metric. Once we have obtained propensity scores, we can compare their distribution between the exposed and control units. Take, for example, a plot I made for another blog post below:\n\nThis plot shows us that, for treated units, the probability of receiving treatment ranges from about 20% to about 80% while the distribution for control units ranges from 0% to about 80%. What do we learn from this? Well, it shows us that, for the analysis of treated units, there are potentially comparable control units that can allow us to estimate the potential outcomes for the treated units. In other terms, for each propensity score that a treated unit may have, there is at least one other control unit that has the same propensity score.\nSo, propensity scores are one potential for evaluating positivity, but… isn’t there still a problem here? What about the control units whose propensity scores range from 0% to 19%? According to the propensity scores, we can’t estimate potential outcomes for these units. And, in this case, that would be correct, but it doesn’t mean that your study would be doomed. One solution to violations of positivity is simply to answer another question.\nSure, in this case, you wouldn’t be able to estimate the potential outcomes for every unit in your study, but you can estimate the potential outcomes for every treated unit. This gets to the concept of an estimand or a targeted theoretical value that you are trying to estimate. The estimand is the ideal that we are searching for out in the wild and the estimate is our best attempt to locate it. Not every estimand requires as strict positivity assumptions.\nSure, if you want to estimate potential outcomes for every unit in your study, then you would need complete overlap. This is what is required to estimate the average treatment effect (ATE) that is often estimated in experimental designs where the treated and control units have the exact same probability of receiving treatment. But, as is the case from this example, the ATE may not always be a feasible estimand (and sometimes, it may not even be desired). Instead, for our purposes with this hypothetical example, we could instead estimate the average treatment effect for the treated (the ATT), which is exactly what it sounds like - the average effect of the treatment for the treated units. We make that caveat because, in this context, we would be unable to estimate the average effect of the treatment for the control units (this is known as the ATU or ATC).\nSituated within the context of the positivity assumption, we violate positivity in our context dependent on the estimand we are targeting. For this example, we would violate positvity is we tried to estimate the ATE or ATU, but we would not violate positivity if we shot for the ATT. However, remember that we still should pay attention to specific stratum in our data that might be sneakily flying under the radar as well (such as the \\(Z_1\\) = 1 and \\(Z_2\\) = 1 example). It is still possible that such nuances could go undetected with a “dimensionality reduction” statistic like the propensity score. As always with anything causal inference related, know your data well and really dig into it looking for stuff exactly like this before diving into analysis. As regression demonstrates, not all methods will let you know when you’re violating a key assumption."
  },
  {
    "objectID": "blog/2025/bayes-conflict-research/index.html",
    "href": "blog/2025/bayes-conflict-research/index.html",
    "title": "Three Points for Bayesian Statistics in Conflict Research",
    "section": "",
    "text": "Code\n# Load Libraries\npacman::p_load(\n  \"dplyr\", # Data Manipulation\n  \"ggplot2\", # Data Visualization\n  \"ggtext\", # Colored Text in Plots\n  \"ggdist\", # Cool Distribution Plots\n  \"brms\", # Bayesian Regression\n  \"cmdstanr\", # Bayesian Computation\n  \"tidybayes\", # Bayesian Visualization\n  \"patchwork\", # Combining Plots\n  install = FALSE\n)\n\n# Define a Custom Theme\nblog_theme &lt;- function() {\n  theme_bw() +  \n    theme(\n      panel.grid.major = element_line(color = \"gray80\", size = 0.3),\n      panel.grid.minor = element_blank(),\n      panel.border = element_blank(),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      plot.title = element_text(face = \"bold\", size = 16, margin = margin(t = 0, r = 0, b = 15, l = 0)),\n      axis.title.x = element_text(face = \"bold\", size = 14, margin = margin(t = 15, r = 0, b = 0, l = 0)),\n      axis.title.y = element_text(face = \"bold\", size = 14, margin = margin(t = 0, r = 15, b = 0, l = 0)),\n      strip.text = element_text(face = \"bold\"),\n      axis.text.x = element_text(face = \"bold\", size = 10), \n      axis.text.y = element_text(face = \"bold\", size = 10), \n      axis.ticks.x = element_blank(), \n      axis.ticks.y = element_blank(), \n      strip.background = element_rect(fill = \"grey80\", color = NA),\n      legend.title = element_text(face = \"bold\", size = 14),\n      legend.text = element_text(face = \"bold\", size = 10, color = \"grey25\"),\n    )\n}\n\n# Establish a Custom Color Scheme\ncolors &lt;- c(\n  \"#0a697d\",\n  \"#0091af\",\n  \"#ddb067\",\n  \"#c43d56\",\n  \"#ab2a42\"\n)\n\noptions(scipen = 999)\nBayesian methods are basically unheard of in quantitative conflict research (QCR) - aka the statistical study of things like civil war, terrorism, violence against civilians, conflict resolution, etc. This might lead to the impression that Bayesian methods are simply not needed in QCR. But such an impression is far from the truth!\nI would argue that Bayesian methods are incredibly under-represented in QCR due to entrenched institutional norms, a lack of method familiarity with reviewers and peers, and the added (but oftentimes necessary) burden that conducting a Bayesian analysis requires (prior elicitation, computation time, etc.).\nWhile these excuses may not be the best, the lack of Bayesian representation in QCR would not be that big of a deal if Bayesian statistics were jusr not very needed. However, I would argue the very opposite is true. Bayesian approaches offer a wide range of features that lend themselves well to QCR. Likewise, and crucially important, frequentist (the default) statistical approaches are accompanied by many properties that are very difficult to justify in the context of QCR.\nIn this blog, I will go over three points that ask the quantitative conflict researcher why they should use frequentist statistics in favor of Bayesian statistics. Too often, the decision to use frequentist statistics is never justified and is assumed as the de facto approach. But I want to challenge this and force the quantitative conflict researcher to justify their decision on this methodological front.\nAnd for those not familiar, I will not be providing a primer on the differences between frequentist and Bayesian statistics.1 However, I can provide two very simple points where these approaches differ because I think that knowing these things is helpful for understanding the methodological points that are made in this blog.\nFirst, frequentists and Bayesians operate under different definitions of probability. For the frequentist, probability is understood as the long-run frequency of a given event over the total number of instances. The canonical example is a coin flip. If I flipped a coin 100 times and it landed on heads 51 out of those 100 times, a frequentist definition of probability would say that, from this trial, the probability of that coin landing on heads is 51%.\nIn contrast, Bayesians view probability as a subjective degree of belief. The value of Bayesian statistics becomes manifest when we depart from the coin flip example, which happens to suit the frequentist understanding of probability very well. For example, consider the 2024 U.S. presidential election. We cannot run that election over and over again 100, 1,000, or a million times and see how many times Trump won versus how many times Harris won. It is only possible for one 2024 presidential election to occur.\nUnder the Bayesian approach for this case, we can still discuss probability, but as a degree of belief instead of the number of times one candidate wins over multiple hypothetical trials. For example, say I built a model that gave Donald Trump a 90% probability to win the election. As a Bayesian, I could simply say that Donald Trump has a 90% chance to win the election, which means that it is very likely that Trump will win the election. While that interpretation is a bit more vague, it is a definition that is both intuitive for human understanding and is applicable for the situation at hand.\nAnd if you’re still unsure, imagine that I interpreted this 90% under the frequentist framework (90% of the time, Donald Trump wins the election)… So what does that mean, again, if there is only one election? If we are talking about thousands of hypothetical 2024 presidential elections, on what grounds are we able to simulate the elections that are different from the one we observed? What is the source of random variability between every run of the hypothetical election and how can we justify that?\nAs a final note before we move on to the three points, it’s also important to know that the estimated probability of something under a Bayesian approach is not something that is necessarily entirely estimated directly from the data. We can formally build in prior information to influence what we find. That might sound fishy to those unfamiliar, but we will talk about that in Points 2 and 3.\nAnd I think that’s all the background information that we’ll need for now, so let’s jump into it!"
  },
  {
    "objectID": "blog/2025/bayes-conflict-research/index.html#footnotes",
    "href": "blog/2025/bayes-conflict-research/index.html#footnotes",
    "title": "Three Points for Bayesian Statistics in Conflict Research",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor those totally unfamiliar with the difference, you can do basically anything that you want under each approach (regression, matching, machine learning, etc.). Each can be used for causal inference, forecasting, description and meassurement, etc. This is not the domain on which they differ.↩︎\nOf course, a Bayesian researcher could add very biased priors to the design intentionally. However, any proper Bayesian analysis worth its salt will report its priors and everyone else can subsequently critique the results on the grounds of the priors. This process is certainly more transparent than researchers “playing around” with control variables behind the scene to either get results that they like or get results that “look” right.↩︎"
  },
  {
    "objectID": "blog/2024/nfl-workflow/index.html",
    "href": "blog/2024/nfl-workflow/index.html",
    "title": "Predicting the Outcome of NFL Games in the 2024-2025 Season",
    "section": "",
    "text": "Code\n# Load Libraries\npacman::p_load(\n  \"dplyr\", # Data Manipulation\n  \"nflverse\", # NFL Verse Environment\n  \"gt\", # Nice Tables\n  \"tidyr\", # Reshaping Data\n  \"stringr\", # Working with Strings\n  \"caret\", # Machine Learning\n  \"scales\", # Percent Formatting\n  \"readxl\", # Reading Excel Files\n  \"writexl\", # Writing Excel Files\n  install = FALSE\n)\n\n# Define a Custom Theme - Taken From Andrew Heiss's Blogs\nblog_theme &lt;- function() {\n  theme_bw() +  # Start with theme_bw\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      plot.title = element_text(face = \"bold\"),\n      axis.title = element_text(face = \"bold\"),\n      strip.text = element_text(face = \"bold\"),\n      strip.background = element_rect(fill = \"grey80\", color = NA),\n      legend.title = element_text(face = \"bold\")\n    )\n}\n\n\nI am a massive fan of NFL football. I look forward to the inaugural start of the regular season every September and it feels all too soon when the season ends when the Super Bowl is played in February. As much as I love the game-play, the sports shows spinning their narratives, and the social aspect of NFL Sundays, I have been looking for excuses to get my hands on NFL data and having fun with an additional aspect of the game.\nRecently, in pursuit of this goal, I went to the Playoff Predictors website, where you can go game-by-game and pick who you think will win each game. It’s a fun exercise that I look forward to every year when the NFL schedule is released and it gives me a picture of what I intuitively think the standings might look like at the conclusion of the upcoming season. Once I got these standings, I played around with the {nflreadr} and {gt} packages to present my predicted standings in a more aesthetically pleasing way.\n\n\nCode\n# Create My Vibes Tribble - Adding Extra Spacing\nvibes &lt;- tribble(\n  ~east, ~record_1, ~space_1, ~north, ~record_2, ~space_2, ~south, ~record_3, ~space_3, ~west, ~record_4, ~conf,\n  \"BUF\", \"10-7\", \" \", \"BAL\", \"13-4\", \" \", \"IND\", \"13-4\", \" \", \"KC\", \"12-5\", \"AFC\",\n  \"NYJ\", \"9-8\", \" \", \"CIN\", \"10-7\", \" \", \"HOU\", \"11-6\", \" \", \"LAC\", \"11-6\", \"AFC\",\n  \"MIA\", \"7-10\", \" \", \"PIT\", \"8-9\", \" \",\"TEN\", \"9-8\", \" \", \"DEN\", \"7-10\", \"AFC\",\n  \"NE\", \"3-14\", \" \", \"CLE\", \"8-9\", \" \", \"JAX\", \"9-8\", \" \", \"LV\", \"6-11\", \"AFC\",\n  \"PHI\", \"9-8\", \" \", \"GB\", \"12-5\", \" \", \"ATL\", \"9-8\", \" \", \"LAR\", \"11-6\", \"NFC\",\n  \"WSH\", \"8-9\", \" \", \"DET\", \"11-6\", \" \", \"TB\", \"7-10\", \" \", \"SF\", \"10-7\", \"NFC\",\n  \"DAL\", \"7-10\", \" \", \"CHI\", \"9-8\", \" \", \"CAR\", \"5-12\", \" \", \"ARZ\", \"9-8\", \"NFC\",\n  \"NYG\", \"5-12\", \" \", \"MIN\", \"6-11\", \" \", \"NO\", \"4-13\", \" \", \"SEA\", \"4-13\", \"NFC\"\n)\n\nvibes %&gt;%\n  # Group By Conference\n  gt(groupname_col = \"conf\") %&gt;%\n  # Create Columns Labels\n  cols_label(\n    east = \"\",\n    record_1 = \"East\",\n    space_1 = \"\",\n    north = \"\",\n    record_2 = \"North\",\n    space_2 = \"\",\n    south = \"\",\n    record_3 = \"South\",\n    space_3 = \"\",\n    west = \"\",\n    record_4 = \"West\"\n  ) %&gt;%\n  # Align Column Title Text\n  tab_style(style = cell_text(align = \"center\"), locations = cells_column_labels()) %&gt;%\n  # Align Body Text\n  tab_style(style = cell_text(align = \"center\"), locations = cells_body()) %&gt;%\n  # Distinguish Division Rows\n  tab_style(\n    style = list(\n      cell_fill(color = \"#bcc0be\")),\n    locations = cells_body(rows = which(vibes$east %in% c(\"AFC\", \"NFC\")))) %&gt;%\n  # Add Team Logos\n  nflplotR::gt_nfl_logos(columns = c(\"east\", \"north\", \"south\", \"west\"))\n\n\n\n\n\n\n\n\n\nEast\n\n\nNorth\n\n\nSouth\n\n\nWest\n\n\n\n\nAFC\n\n\n\n10-7\n\n\n13-4\n\n\n13-4\n\n\n12-5\n\n\n\n9-8\n\n\n10-7\n\n\n11-6\n\n\n11-6\n\n\n\n7-10\n\n\n8-9\n\n\n9-8\n\n\n7-10\n\n\n\n3-14\n\n\n8-9\n\n\n9-8\n\n\n6-11\n\n\nNFC\n\n\n\n9-8\n\n\n12-5\n\n\n9-8\n\n\n11-6\n\n\n\n8-9\n\n\n11-6\n\n\n7-10\n\n\n10-7\n\n\n\n7-10\n\n\n9-8\n\n\n5-12\n\n\n9-8\n\n\n\n5-12\n\n\n6-11\n\n\n4-13\n\n\n4-13\n\n\n\n\n\n\n\nI like to refer to these as my “vibes-based” predictions because that’s really all they are. However, as a trained social scientist, I am well aware that “vibes” are not wholly informative, well-defined, nor do they contain a great deal of explanatory or predictive power. So, I thought, why not get my hands on more NFL data and try to work up a machine learning based approach? And that is what this blog is for.\n\nData/Feature Collection\nPrior to any fancy modeling, I need to collect some data to predict who wins each game. I want to start off with a major caveat here. I am doing this for fun and educational purposes. Undoubtedly, the predictors I have selected are not reflective of the most advanced analytics nor are they comprehensive. I chose the “lowest hanging fruit” for ease of access. This is probably going to hurt the predictive power of the models (models predict better with more predictive data), but again, humor me!\nOverall, I am using the following variables as predictors: whether a team is playing at home, QBR (quarterback rating), passing EPA (expected points added), rushing EPA, receiving EPA, forced fumbles, sacks, interceptions, and passes broken up. Because each prediction is at the game-level, I am using a differenced variable for computational ease (i.e., rather than include the home team’s passing EPA and the away team’s passing EPA in the same model, I just create a difference between the two and use this difference as a predictor for each team). Regardless of which method is used, the predictive performance remained the same after testing.\nThis selection leaves a lot to be desired. What about more advanced metrics like ELO? What about schematic data (like what type of offense the home team runs v. what type of defense the away team runs, etc.)? What about circumstantial data like whether a key player is out? These are all great things to add that will need to be included in the future! If you’re curious about the data collection syntax, check out the code fold below!\n\n\nCode\n# Load and Clean the QBR Data\nqbr &lt;- load_espn_qbr(\n  # Select the 2006-2023 Seasons as Training Data\n  seasons = 2006:2023,\n  # Aggregate at the Week-Level\n  summary_type = c(\"week\")) %&gt;%\n  # Exclude Playoff Games\n  filter(season_type == \"Regular\") %&gt;%\n  # Select Relevant Columns\n  select(c(team_abb, season, game_week, qbr_total, pts_added)) %&gt;%\n  # Create Cumulative Averages\n  group_by(season, team_abb) %&gt;%\n  mutate(\n    moving_qbr_mean = cumsum(qbr_total) / game_week,\n    moving_pts_added = cumsum(pts_added / game_week),\n    # Rename Washington for Merging\n    team_abb = ifelse(team_abb == \"WSH\", \"WAS\", team_abb))\n\n# Load and Clean Offensive Stats Data\noffensive &lt;- load_player_stats(\n  # Select the 2006-2023 Seasons as Training Data\n  seasons = 2006:2023,\n  # Filter to Offense\n  stat_type = \"offense\") %&gt;%\n  # Exclude the Playoffs\n  filter(season_type == \"REG\") %&gt;%\n  # Create Team-Level Stats\n  group_by(season, recent_team, week) %&gt;%\n  summarise(\n    passing_epa = sum(passing_epa, na.rm = TRUE),\n    rushing_epa = sum(rushing_epa, na.rm = TRUE),\n    receiving_epa = sum(receiving_epa, na.rm = TRUE)\n  ) %&gt;%\n  ungroup() %&gt;%\n  # Create Cumulative Averages\n  group_by(season, recent_team) %&gt;%\n  mutate(\n    moving_passing_epa = cumsum(passing_epa) / week,\n    moving_rushing_epa = cumsum(rushing_epa) / week,\n    moving_receiving_epa = cumsum(receiving_epa) / week) %&gt;%\n  # Keep Relevant Columns\n  select(season, recent_team, week, passing_epa, rushing_epa, receiving_epa, \n         moving_passing_epa, moving_rushing_epa, moving_receiving_epa) %&gt;%\n  # Convert Team Abbreviations to a More Standard Form for Merging\n  mutate(recent_team = ifelse(recent_team == \"LA\", \"LAR\", recent_team))\n\n# Load and Clean Defensive Stats Data\ndefensive &lt;- load_player_stats(\n  # Select the 2006-2023 Seasons as Training Data\n  seasons = 2006:2023,\n  # Filter to Defense\n  stat_type = \"defense\") %&gt;%\n  # Exclude Playoff Games\n  filter(season_type == \"REG\") %&gt;%\n  # Create Team-Level Stats\n  group_by(season, team, week) %&gt;%\n  summarise(\n    tackles = sum(def_tackles, na.rm = TRUE),\n    forced_fumbles = sum(def_fumbles_forced, na.rm = TRUE),\n    sacks = sum(def_sacks, na.rm = TRUE),\n    ints = sum(def_interceptions, na.rm = TRUE),\n    pass_broken = sum( def_pass_defended, na.rm = TRUE)\n  ) %&gt;%\n  ungroup() %&gt;%\n  # Create Cumulative Averages\n  group_by(season, team) %&gt;%\n  mutate(\n    moving_tackles = cumsum(tackles) / week,\n    moving_forced_fumbles = cumsum(forced_fumbles) / week,\n    moving_sacks = cumsum(sacks) / week,\n    moving_ints = cumsum(ints) / week,\n    moving_pass_broken = cumsum(pass_broken) / week) %&gt;%\n  # Keep Relevant Columns\n  select(season, team, week, tackles, forced_fumbles, sacks, ints, pass_broken, moving_tackles, \n         moving_forced_fumbles, moving_sacks, moving_ints, moving_pass_broken) %&gt;%\n  # Convert Team Abbreviations to a More Standard Form for Merging\n  mutate(team = ifelse(team == \"LA\", \"LAR\", team))\n\n# Load and Clean Schedules Data\nseasons &lt;- load_schedules(seasons = 2006:2023)\n\n# Convert the Data From Dyadic to Monadic\nseasons &lt;- clean_homeaway(seasons) %&gt;%\n  # Exclude Playoff Games\n  filter(game_type == \"REG\") %&gt;%\n  # Create a Home Team Variable\n  mutate(home = ifelse(location == \"home\", 1, 0),\n         # Create a Win Variable\n         win = ifelse(team_score &gt; opponent_score, 1, 0)) %&gt;%\n  # Keep Relevant Columns\n  select(game_id, season, week, team, opponent, home, win) %&gt;%\n   # Convert Team Abbreviations to a More Standard Form for Merging\n  mutate(team = ifelse(team == \"LA\", \"LAR\", team),\n         opponent = ifelse(opponent == \"LA\", \"LAR\", opponent))\n\n# Merge This Data\nmerged &lt;- inner_join(seasons, qbr, by = c(\"season\", \"team\" = \"team_abb\", \"week\" = \"game_week\")) %&gt;%\n  inner_join(offensive, by = c(\"season\", \"team\" = \"recent_team\", \"week\")) %&gt;%\n  inner_join(defensive, by = c(\"season\", \"team\", \"week\"))\n\nmerged &lt;- merged %&gt;%\n  group_by(game_id) %&gt;%\n  # Create Opponent Columns\n  # This Work Because Each Team Opponent Is In a Paired Set of Rows\n  # The Opponent Is Always the Second Observation\n  # Basically, This Just Reverses Cumulative Stats For Each Team Under a Different Name\n  mutate(opp_qbr = lead(moving_qbr_mean),\n         opp_qbr = ifelse(is.na(opp_qbr), lag(moving_qbr_mean), opp_qbr),\n         opp_pass_epa = lead(moving_passing_epa),\n         opp_pass_epa = ifelse(is.na(opp_pass_epa), \n                               lag(moving_passing_epa), opp_pass_epa),\n         opp_rushing_epa = lead(moving_rushing_epa),\n         opp_rushing_epa = ifelse(is.na(opp_rushing_epa), \n                                  lag(moving_rushing_epa), opp_rushing_epa),\n         opp_receiving_epa = lead(moving_receiving_epa),\n         opp_receiving_epa = ifelse(is.na(opp_receiving_epa), \n                                    lag(moving_receiving_epa), opp_receiving_epa),\n         opp_tackles = lead(moving_tackles),\n         opp_tackles = ifelse(is.na(opp_tackles), \n                              lag(moving_tackles), opp_tackles),\n         opp_forced_fumbles = lead(moving_forced_fumbles),\n         opp_forced_fumbles = ifelse(is.na(opp_forced_fumbles), \n                                     lag(moving_forced_fumbles), opp_forced_fumbles),\n         opp_sacks = lead(moving_sacks),\n         opp_sacks = ifelse(is.na(opp_sacks), \n                            lag(moving_sacks), opp_sacks),\n         opp_ints = lead(moving_ints),\n         opp_ints = ifelse(is.na(opp_ints), \n                           lag(moving_ints), opp_ints),\n         opp_pass_broken = lead(moving_pass_broken),\n         opp_pass_broken = ifelse(is.na(opp_pass_broken), \n                                  lag(moving_pass_broken), opp_pass_broken)\n         ) %&gt;%\n  # Create Differenced Columns\n  mutate(\n    qbr_diff = moving_qbr_mean - opp_qbr,\n    pass_epa_diff = moving_passing_epa - opp_pass_epa,\n    rushing_epa_diff = moving_rushing_epa - opp_rushing_epa,\n    receiving_epa_diff = moving_receiving_epa - opp_receiving_epa,\n    tackles_diff = moving_tackles - opp_tackles,\n    forced_fumbles_diff = moving_forced_fumbles - opp_forced_fumbles,\n    sacks_diff = moving_sacks - opp_sacks,\n    ints_diff = moving_ints - opp_ints,\n    pass_broken_diff = moving_pass_broken - opp_pass_broken\n  ) %&gt;%\n  # Make the Outcome Column Suitable for Classification\n  mutate(win = factor(win, levels = c(0, 1), labels = c(\"Lose\", \"Win\"))) %&gt;%\n  # Drop NAs Because They Will Create Problems\n  drop_na()\n\n\n\n\nMachine Learning Algorithms Limitations\nOkay, now onto the actual machine learning algorithms that will be used. Again, nothing super fancy here. In the interest of keeping things simple at first, I chose to just explore how predictive accuracy fluctuates between four popular ML algorithms (logistic regression… which makes me cringe to refer to it as “ML”, random forest, support vector machine (SVM), and XGBoost). For those curious, I did engage in hyper-parameter tuning, but, no amount of tuning really improved the model results that much, and I felt that, in the interest of simplicity and computational time, it would be best to just include four basic ML algorithms for now.\n\n# For Reproducibility\nset.seed(1234)\n\n# Establish a Cross-Validation Method\ncv_method &lt;- trainControl(method = \"cv\",\n                          number = 10,\n                          classProbs = TRUE,\n                          summaryFunction = twoClassSummary)\n\n# Fit Models\n# Logistic Regression\nlog_fit &lt;- train(win ~ home + qbr_diff + pass_epa_diff + rushing_epa_diff + receiving_epa_diff +\n                       forced_fumbles_diff + sacks_diff + ints_diff + pass_broken_diff, \n                  data = merged,\n                  method = \"glm\",\n                  family = \"binomial\",\n                  trControl = cv_method,\n                  metric = \"ROC\")\n\n# Save Model Results So I Don't Have to Re-Train Every Time\nsaveRDS(log_fit, \"data-and-analysis/log_fit_model.rds\")\n\n# Random Forest\nrf_fit &lt;- train(win ~ home + qbr_diff + pass_epa_diff + rushing_epa_diff + receiving_epa_diff +\n                       forced_fumbles_diff + sacks_diff + ints_diff + pass_broken_diff, \n                  data = merged,\n                  method = \"rf\",\n                  trControl = cv_method,\n                  metric = \"ROC\")\n\nsaveRDS(rf_fit, \"data-and-analysis/rf_fit_model.rds\")\n\n# Support Vector Machine\nsv_fit &lt;- train(win ~ home + qbr_diff + pass_epa_diff + rushing_epa_diff + receiving_epa_diff +\n                       forced_fumbles_diff + sacks_diff + ints_diff + pass_broken_diff, \n                data = merged,\n                method = \"svmLinear\",\n                trControl = cv_method,\n                metric = \"ROC\")\n\nsaveRDS(sv_fit, \"data-and-analysis/sv_fit_model.rds\")\n\n# XGBoost\nxgb_fit &lt;- train(win ~ qbr_diff + pass_epa_diff + rushing_epa_diff + receiving_epa_diff +\n                        forced_fumbles_diff + sacks_diff + ints_diff + pass_broken_diff, \n                 data = merged,\n                 method = \"xgbTree\",\n                 trControl = cv_method,\n                 metric = \"ROC\")\n\nsaveRDS(xgb_fit, \"data-and-analysis/xgb_fit_model.rds\")\n\n# Store the Predictive Accuracy Results in a Table\nresults &lt;- tibble(\n  Model = c(\"Logistic Regression\", \"Random Forest\", \"SVM\", \"XGBoost\"),\n  # Store ROC Metrics\n  ROC = c(\n    # which.max() Doesn't Do Anything Here, But It Would If I Had Tons of Different\n    # Models for Each Model Type. It Would Select the Model with the Highest Predictive\n    # Power. Not Helpful Here Since I Am Only Running One Model of Each Type, But It's\n    # A Useful Reference That I Want to Keep for the Future\n    log_fit$results[which.max(log_fit$results$ROC), \"ROC\"], \n    rf_fit$results[which.max(rf_fit$results$ROC), \"ROC\"], \n    sv_fit$results[which.max(sv_fit$results$ROC), \"ROC\"], \n    xgb_fit$results[which.max(xgb_fit$results$ROC), \"ROC\"]\n  ),\n  # Store Accurate Predictions Percentage\n  Accuracy = c(\n    (log_fit$results$Spec[which.max(log_fit$results$ROC)] + \n       log_fit$results$Sens[which.max(log_fit$results$ROC)]) / 2,\n    (rf_fit$results$Spec[which.max(rf_fit$results$ROC)] + \n       rf_fit$results$Sens[which.max(rf_fit$results$ROC)]) / 2,\n    (sv_fit$results$Spec[which.max(sv_fit$results$ROC)] + \n       sv_fit$results$Sens[which.max(sv_fit$results$ROC)]) / 2,\n    (xgb_fit$results$Spec[which.max(xgb_fit$results$ROC)] + \n       xgb_fit$results$Sens[which.max(xgb_fit$results$ROC)]) / 2\n  )\n)\n\n\n\nModel Evaluation\nSo, how did these models fair? Eh… not great, as you can see below\n\nresults    \n\n# A tibble: 4 × 3\n  Model                 ROC Accuracy\n  &lt;chr&gt;               &lt;dbl&gt;    &lt;dbl&gt;\n1 Logistic Regression 0.798    0.718\n2 Random Forest       0.785    0.715\n3 SVM                 0.798    0.717\n4 XGBoost             0.792    0.720\n\n\n70-ish% isn’t terrible. It’s better than a coin flip. But really, how impressive is that? Just off of vibes, anyone who sort of knows the NFL will probably get 70% of game predictions right. Honestly, you might even do better if you just follow Vegas and predict the winner based on who is the betting favorite to win. That’s not very satisfying is it? A truly impressive ML algorithm should be able to predict not only when a favorite wins but also when a favorite does not win. These very crude models don’t appear to have that predictive complexity. Why is that the case? I can think of three reasons.\nFirst, as already stated, better predictors would go a long way. The good news is that this is probably the easiest fix. I just need to put the time in to research and collect the data.\nSecond, there may have been more complex hyper-parameter tuning I could have engaged with. Given that I come from a causal inference background, machine learning is not my specialty, and I do not have a wealth of information lodged in my head about all the tuning options for each ML algorithm. However, I’m sure that predictive gains could be there with some hyper-parameter tuning.\nLastly, I think that a different modeling approach could go a long way. And, to demonstrate my reasoning, let’s look at how my trained models are predicting the outcomes of the upcoming Week 2 games.\n\n\nCode\n# To Do This, I Need to Load In 2024 \"Test\" Data That the Model Was Not Trained On\n# This Is Just a Repeat of the Prior Data Cleaning Process for the Training Data\n# So I Don't Annotate Code Here\nqbr_2024 &lt;- load_espn_qbr(\n  seasons = 2024,\n  summary_type = c(\"week\")) %&gt;%\n  filter(season_type == \"Regular\") %&gt;%\n  select(c(team_abb, season, game_week, qbr_total, pts_added)) %&gt;%\n  group_by(season, team_abb) %&gt;%\n  mutate(\n    moving_qbr_mean = cumsum(qbr_total) / game_week,\n    moving_pts_added = cumsum(pts_added / game_week),\n    # Rename Washington for Merging\n    team_abb = ifelse(team_abb == \"WSH\", \"WAS\", team_abb)) %&gt;%\n  # Keep Last Week's Data\n  filter(game_week == 1) %&gt;%\n  # Convert Lagged Game Week to Current Since We're Using Last Week's Predictors\n  mutate(game_week = 2)\n\noffensive_2024 &lt;- load_player_stats(\n  seasons = 2024,\n  stat_type = \"offense\") %&gt;%\n  filter(season_type == \"REG\") %&gt;%\n  group_by(season, recent_team, week) %&gt;%\n  summarise(\n    passing_epa = sum(passing_epa, na.rm = TRUE),\n    rushing_epa = sum(rushing_epa, na.rm = TRUE),\n    receiving_epa = sum(receiving_epa, na.rm = TRUE)\n  ) %&gt;%\n  ungroup() %&gt;%\n  group_by(season, recent_team) %&gt;%\n  mutate(\n    moving_passing_epa = cumsum(passing_epa) / week,\n    moving_rushing_epa = cumsum(rushing_epa) / week,\n    moving_receiving_epa = cumsum(receiving_epa) / week) %&gt;%\n  select(season, recent_team, week, passing_epa, rushing_epa, receiving_epa, moving_passing_epa,\n         moving_rushing_epa, moving_receiving_epa) %&gt;%\n  filter(week == 1) %&gt;%\n  mutate(week = 2) %&gt;%\n  mutate(recent_team = ifelse(recent_team == \"LA\", \"LAR\", recent_team))\n\ndefensive_2024 &lt;- load_player_stats(\n  seasons = 2024,\n  stat_type = \"defense\") %&gt;%\n  filter(season_type == \"REG\") %&gt;%\n  group_by(season, team, week) %&gt;%\n  summarise(\n    tackles = sum(def_tackles, na.rm = TRUE),\n    forced_fumbles = sum(def_fumbles_forced, na.rm = TRUE),\n    sacks = sum(def_sacks, na.rm = TRUE),\n    ints = sum(def_interceptions, na.rm = TRUE),\n    pass_broken = sum( def_pass_defended, na.rm = TRUE)\n  ) %&gt;%\n  ungroup() %&gt;%\n  group_by(season, team) %&gt;%\n  mutate(\n    moving_tackles = cumsum(tackles) / week,\n    moving_forced_fumbles = cumsum(forced_fumbles) / week,\n    moving_sacks = cumsum(sacks) / week,\n    moving_ints = cumsum(ints) / week,\n    moving_pass_broken = cumsum(pass_broken) / week) %&gt;%\n  select(season, team, week, tackles, forced_fumbles, sacks, ints, pass_broken, moving_tackles, \n         moving_forced_fumbles, moving_sacks, moving_ints, moving_pass_broken) %&gt;%\n  filter(week == 1) %&gt;%\n  mutate(week = 2) %&gt;%\n  mutate(team = ifelse(team == \"LA\", \"LAR\", team))\n\nseason_2024 &lt;- load_schedules(seasons = 2024)\n\n# Convert the Data From Dyadic to Monadic\nseason_2024 &lt;- clean_homeaway(season_2024) %&gt;%\n  filter(game_type == \"REG\") %&gt;%\n  mutate(home = ifelse(location == \"home\", 1, 0),\n         win = ifelse(team_score &gt; opponent_score, 1, 0)) %&gt;%\n  select(game_id, season, week, team, opponent, home, win) %&gt;%\n  filter(week == 2) %&gt;%\n  mutate(team = ifelse(team == \"LA\", \"LAR\", team),\n         opponent = ifelse(opponent == \"LA\", \"LAR\", opponent))\n\nmerged_2024 &lt;- inner_join(season_2024, qbr_2024, \n                          by = c(\"team\" = \"team_abb\", \"week\" = \"game_week\", \"season\")) %&gt;%\n  inner_join(offensive_2024, by = c(\"team\" = \"recent_team\", \"week\", \"season\")) %&gt;%\n  inner_join(defensive_2024, by = c(\"team\", \"week\", \"season\")) %&gt;%\n  group_by(game_id) %&gt;%\n  mutate(opp_qbr = lead(moving_qbr_mean),\n         opp_qbr = ifelse(is.na(opp_qbr), \n                          lag(moving_qbr_mean), opp_qbr),\n         opp_pass_epa = lead(moving_passing_epa),\n         opp_pass_epa = ifelse(is.na(opp_pass_epa), \n                               lag(moving_passing_epa), opp_pass_epa),\n         opp_rushing_epa = lead(moving_rushing_epa),\n         opp_rushing_epa = ifelse(is.na(opp_rushing_epa), \n                                  lag(moving_rushing_epa), opp_rushing_epa),\n         opp_receiving_epa = lead(moving_receiving_epa),\n         opp_receiving_epa = ifelse(is.na(opp_receiving_epa), \n                                    lag(moving_receiving_epa), opp_receiving_epa),\n         opp_tackles = lead(moving_tackles),\n         opp_tackles = ifelse(is.na(opp_tackles), \n                              lag(moving_tackles), opp_tackles),\n         opp_forced_fumbles = lead(moving_forced_fumbles),\n         opp_forced_fumbles = ifelse(is.na(opp_forced_fumbles), \n                                     lag(moving_forced_fumbles), opp_forced_fumbles),\n         opp_sacks = lead(moving_sacks),\n         opp_sacks = ifelse(is.na(opp_sacks), \n                            lag(moving_sacks), opp_sacks),\n         opp_ints = lead(moving_ints),\n         opp_ints = ifelse(is.na(opp_ints), \n                           lag(moving_ints), opp_ints),\n         opp_pass_broken = lead(moving_pass_broken),\n         opp_pass_broken = ifelse(is.na(opp_pass_broken), \n                                  lag(moving_pass_broken), opp_pass_broken)\n         ) %&gt;%\n  mutate(\n    qbr_diff = moving_qbr_mean - opp_qbr,\n    pass_epa_diff = moving_passing_epa - opp_pass_epa,\n    rushing_epa_diff = moving_rushing_epa - opp_rushing_epa,\n    receiving_epa_diff = moving_receiving_epa - opp_receiving_epa,\n    tackles_diff = moving_tackles - opp_tackles,\n    forced_fumbles_diff = moving_forced_fumbles - opp_forced_fumbles,\n    sacks_diff = moving_sacks - opp_sacks,\n    ints_diff = moving_ints - opp_ints,\n    pass_broken_diff = moving_pass_broken - opp_pass_broken\n  ) %&gt;%\n  mutate(win = factor(win, levels = c(0, 1), labels = c(\"Lose\", \"Win\"))) %&gt;%\n  # Remove Games with Missing Feature Data\n  filter(!is.na(opp_qbr))\n\n\n\nlog_preds &lt;- predict(log_fit, merged_2024, type = \"prob\")[,2]\nrf_preds &lt;- predict(rf_fit, merged_2024, type = \"prob\")[,2]\nsv_preds &lt;- predict(sv_fit, merged_2024, type = \"prob\")[,2]\nxgb_preds &lt;- predict(xgb_fit, merged_2024, type = \"prob\")[,2]\n\n# Combine Predictions Into a Data Frame\npredictions &lt;- tibble(\n  Team = merged_2024$team,\n  Game_ID = merged_2024$game_id,\n  Logistic_Regression = paste0(round(log_preds * 100, 2), \"%\"),\n  Random_Forest = paste0(round(rf_preds * 100, 2), \"%\"),\n  SVM = paste0(round(sv_preds * 100, 2), \"%\"),\n  XGBoost = paste0(round(xgb_preds * 100, 2), \"%\")\n)\n\n# Load in NFL Logo Data to Make Cool Tables with {gt}\nteam_logos &lt;- nflfastR::teams_colors_logos %&gt;% \n  select(team_abbr, team_logo_espn)\n\nweek2_preds &lt;- predictions %&gt;%\n  left_join(team_logos, by = c(\"Team\" = \"team_abbr\")) %&gt;%\n  select(Team, team_logo_espn, Game_ID, Logistic_Regression, Random_Forest, SVM, XGBoost)\n\n# I Want to Create a Table That Has Teams Who Are Playing Each Other In the Same Row\n# To Do This, I'll Need to Reshape the Data\nreshaped_week2 &lt;- week2_preds %&gt;%\n    group_by(Game_ID) %&gt;%\n    summarise(\n        Team_1 = first(Team),\n        Team_1_Logo = first(team_logo_espn),\n        Team_1_Logistic_Regression = first(Logistic_Regression),\n        Team_1_Random_Forest = first(Random_Forest),\n        Team_1_SVM = first(SVM),\n        Team_1_XGBoost = first(XGBoost),\n        Team_2 = last(Team),\n        Team_2_Logo = last(team_logo_espn),\n        Team_2_Logistic_Regression = last(Logistic_Regression),\n        Team_2_Random_Forest = last(Random_Forest),\n        Team_2_SVM = last(SVM),\n        Team_2_XGBoost = last(XGBoost)\n    )\n\n# Now, I Can Use {gt} To Make a Nice Table\nreshaped_week2 %&gt;%\n  # Start a {gt} Table\n    gt() %&gt;%\n  # Modify Logo Settings\n    text_transform(\n        locations = cells_body(vars(Team_1_Logo, Team_2_Logo)),\n        fn = function(x) {\n            web_image(url = x, height = 40)  # Adjust the height as needed\n        }\n    ) %&gt;%\n  # Remove Columns From the Table\n    cols_hide(\n        columns = c(Game_ID, Team_1, Team_2)\n    ) %&gt;%\n  # Label the Columns\n    cols_label(\n        Team_1_Logo = \"Home\",\n        Team_1_Logistic_Regression = \"Logit\",\n        Team_1_Random_Forest = \"Random Forest\",\n        Team_1_SVM = \"SVM\",\n        Team_1_XGBoost = \"XGBoost\",\n        Team_2_Logo = \"Away\",\n        Team_2_Logistic_Regression = \"Logit\",\n        Team_2_Random_Forest = \"Random Forest\",\n        Team_2_SVM = \"SVM\",\n        Team_2_XGBoost = \"XGBoost\"\n    ) %&gt;%\n  # Create a Title for the Table\n    tab_header(\n        title = \"Predicted Win Probability by Game and Model\"\n    ) %&gt;%\n  # Column Formatting\n    tab_style(\n        style = list(\n            cell_text(weight = \"bold\")\n        ),\n        locations = cells_column_labels(everything())  \n    ) %&gt;%\n    cols_align(\n        align = \"center\",\n        columns = everything()\n    ) %&gt;%\n  # Adjust Column Widths \n    cols_width(\n        Team_1_Logo ~ px(100),  # Adjust as needed\n        Team_1_Logistic_Regression ~ px(85),  # Adjust as needed\n        Team_1_Random_Forest ~ px(85),  # Adjust as needed\n        Team_1_SVM ~ px(85),  # Adjust as needed\n        Team_1_XGBoost ~ px(85),  # Adjust as needed\n        Team_2_Logo ~ px(100),  # Adjust as needed\n        Team_2_Logistic_Regression ~ px(85),  # Adjust as needed\n        Team_2_Random_Forest ~ px(85),  # Adjust as needed\n        Team_2_SVM ~ px(85),  # Adjust as needed\n        Team_2_XGBoost ~ px(85)  # Adjust as needed\n    )\n\nWarning: Since gt v0.3.0, `columns = vars(...)` has been deprecated.\n• Please use `columns = c(...)` instead.\n\n\n\n\n\n\n\n\nPredicted Win Probability by Game and Model\n\n\nHome\nLogit\nRandom Forest\nSVM\nXGBoost\nAway\nLogit\nRandom Forest\nSVM\nXGBoost\n\n\n\n\n\n98.21%\n91.6%\n97.95%\n93.03%\n\n1.66%\n6%\n1.92%\n4.92%\n\n\n\n15.42%\n12.8%\n15.35%\n7.54%\n\n83.61%\n85.2%\n83.75%\n85.47%\n\n\n\n93.46%\n91.8%\n93.25%\n83.63%\n\n6.11%\n8%\n6.34%\n14.28%\n\n\n\n89.19%\n80.2%\n88.54%\n83.76%\n\n10.13%\n19.6%\n10.79%\n11.25%\n\n\n\n90.95%\n89.8%\n91.02%\n79.28%\n\n8.47%\n11.4%\n8.44%\n14.85%\n\n\n\n79.87%\n71%\n76.22%\n64.98%\n\n18.99%\n31.2%\n22.57%\n30.99%\n\n\n\n2.47%\n4.2%\n2.68%\n2.32%\n\n97.35%\n94.4%\n97.14%\n97.02%\n\n\n\n45.8%\n46%\n47.71%\n31.15%\n\n52.4%\n45.4%\n50.59%\n62.23%\n\n\n\n96.01%\n69.8%\n96.07%\n85.33%\n\n3.72%\n26.4%\n3.68%\n13.92%\n\n\n\n21.73%\n19.6%\n20.61%\n14.24%\n\n77.01%\n83%\n78.26%\n81.43%\n\n\n\n92.83%\n87.6%\n92.69%\n78.2%\n\n6.7%\n14%\n6.87%\n17.81%\n\n\n\n10.32%\n24.6%\n10.73%\n13.51%\n\n88.98%\n73.2%\n88.6%\n82.98%\n\n\n\n8.04%\n9.6%\n8.97%\n6.5%\n\n91.41%\n86.4%\n90.46%\n91.34%\n\n\n\n38.9%\n49.2%\n40.41%\n51.87%\n\n59.36%\n48.6%\n57.94%\n54.9%\n\n\n\n92.36%\n88.2%\n91.65%\n81.26%\n\n7.15%\n15.4%\n7.85%\n17.28%\n\n\n\n17.74%\n16.4%\n17.38%\n20.76%\n\n81.18%\n83.6%\n81.63%\n77.53%\n\n\n\n\n\n\n\nAs you can see, there are some wacky predictions for week 2 game outcomes. The Saints are massive favorites over the Cowboys? The Vikings are massive favorites to the 49ers? What?! Well, the answer is not very surprising. In predicting week 2 games, we use all data from prior weeks in the season. In week 2, this means we only have one week of data to draw from. That means that, if a team does exceptionally well in week 1, this great performance is going to impact predictions for week 2. Both the Saints and Vikings had great offensive and defensive performances in week 1, which explains why this model is so bullish on these teams. It stands to reason that such model predictions would probably not show up later on in the season.\nThis gets to my third point on my model performance. When a model is solely impacted by the data, and the available data is not incredibly informative, we are going to get predictions that are pretty counter-intuitive. Basically, please do not put any money down on the Saints or Vikings outright winning this week! I think something to explore in the future would be Bayesian methods to incorporate prior information (i.e. the Cowboys perform well in the early regular season, the 49ers are really good, etc.) that can stabilize the existing limited data with prior knowledge. As the causal inference folks are quick to say… data are dumb, especially when such limited data. Especially early in the season, Bayesian methods may prove really helpful in preventing predictions that are generated from an outlier or two.\nOut of curiosity, I wanted to check how well the model was able to predict the outcome of games by each week in the season. The expectation would be that the model becomes more accurate as the season goes on (we get more information). Below is a plot of the average percent of games whose outcomes are correctly predicted each week from the 2006-2023 seasons.\n\n\nCode\nmerged$log_preds &lt;- predict(log_fit, merged, type = \"prob\")[,2]\nmerged$rf_preds &lt;- predict(rf_fit, merged, type = \"prob\")[,2]\nmerged$rf_preds2 &lt;- ifelse(merged$rf_preds &gt;= 0.5, 1, 0)\nmerged$sv_preds &lt;- predict(sv_fit, merged, type = \"prob\")[,2]\nmerged$xgb_preds &lt;- predict(xgb_fit, merged, type = \"prob\")[,2]\n\nmerged %&gt;%\n  filter(week != 1) %&gt;%\n  mutate(log_class = ifelse(log_preds &gt;= 0.5, \"Win\", \"Lose\"),\n         rf_class = ifelse(rf_preds &gt;= 0.5, \"Win\", \"Lose\"),\n         sv_class = ifelse(sv_preds &gt;= 0.5, \"Win\", \"Lose\"),\n         xgb_class = ifelse(xgb_preds &gt;= 0.5, \"Win\", \"Lose\")) %&gt;%\n  mutate(log_right = ifelse(win == log_class, 1, 0),\n         rf_right = ifelse(win == rf_class, 1, 0),\n         sv_right = ifelse(win == sv_class, 1, 0),\n         xgb_right = ifelse(win == xgb_class, 1, 0)) %&gt;%\n  # By Week, Calculate Predictive Accuracy\n  group_by(week) %&gt;%\n  summarise(log_week_right = mean(log_right),\n         rf_week_right = mean(rf_right),\n         sv_week_right = mean(sv_right),\n         xgb_week_right = mean(xgb_right)) %&gt;%\n  # Pivot to Color by Model Type\n  pivot_longer(cols = starts_with(\"log_week_right\"):starts_with(\"xgb_week_right\"),\n               names_to = \"Model\",\n               values_to = \"Accuracy\") %&gt;%\n  mutate(Model = recode(Model,\n                        log_week_right = \"Logistic Regression\",\n                        rf_week_right = \"Random Forest\",\n                        sv_week_right = \"SVM\",\n                        xgb_week_right = \"XGBoost\")) %&gt;%\n  ggplot(aes(x = week, y = Accuracy, color = Model)) +\n  geom_line() +\n  geom_point() +\n  scale_x_continuous(breaks = 2:18) +\n  scale_y_continuous(breaks = seq(0.6, 1, by = 0.05),\n                     labels = scales::percent) + \n  scale_color_manual(\n    values = c(\"Logistic Regression\" = \"#e31837\", \n               \"Random Forest\" = \"#003594\", \n               \"SVM\" = \"#041e42\", \n               \"XGBoost\" = \"#ffb81c\")\n  ) +\n  labs(title = \"Week 1 is Excluded Due to Lack of In-Season Data\",\n       x = \"Week\",\n       y = \"Average Predictive Accuracy\",\n       color = \"Model\") +\n  blog_theme() + \n  theme(\n    plot.title = element_text(face = \"bold\"), \n    legend.title = element_text(face = \"bold\")  \n  )\n\n\n\n\n\nAverage In-Sample Predictive Accuracy by Model Over NFL Weeks\n\n\n\n\nLike a lot of things in the world of data science, when you plot the data expecting answers, you actually just get a lot more questions. While these report in-sample results (in contrast to cross-validated out-of-sample accuracy metrics… so take these accuracy numbers with a grain of salt), I still would have expected an upward trend over the NFL season, but nope! And there’s other interesting things as well… like how three of the models have a crazy dip in predictive performance in week 10. Don’t really know what that’s about. Well, even if the plot doesn’t support my diagnosis and prescription all that well, I’m convinced that pursuing a modeling strategy that incorporates prior information and domain knowledge would probably result in less “Saints over Cowboys” and “Vikings over 49ers” predictions.\n\n\nSetting Up My Workflow\nLastly, I want to document how I’m going to go about creating predictions every week. After all, I’ve collected data and trained some models, but there is no magic button I can press that will just sequentially update everything every week throughout the remainder of the NFL season. The following code chunk walks through my “workflow” so to speak.\n\n# Establish Global Week Parameters So I Don't Have to Update Every Data Set Individually\nlast_week &lt;- 1\nthis_week &lt;- 2\n\n# Load and Clean Updated Predictor Data\nqbr_2024 &lt;- load_espn_qbr(\n  seasons = 2024,\n  summary_type = c(\"week\")) %&gt;%\n  filter(season_type == \"Regular\") %&gt;%\n  select(c(team_abb, season, game_week, qbr_total, pts_added)) %&gt;%\n  group_by(season, team_abb) %&gt;%\n  mutate(\n    moving_qbr_mean = cumsum(qbr_total) / game_week,\n    moving_pts_added = cumsum(pts_added / game_week),\n    # Rename Washington for Merging\n    team_abb = ifelse(team_abb == \"WSH\", \"WAS\", team_abb)) %&gt;%\n  filter(game_week == last_week) %&gt;%\n  mutate(game_week = this_week)\n\noffensive_2024 &lt;- load_player_stats(\n  seasons = 2024,\n  stat_type = \"offense\") %&gt;%\n  filter(season_type == \"REG\") %&gt;%\n  group_by(season, recent_team, week) %&gt;%\n  summarise(\n    passing_epa = sum(passing_epa, na.rm = TRUE),\n    rushing_epa = sum(rushing_epa, na.rm = TRUE),\n    receiving_epa = sum(receiving_epa, na.rm = TRUE)\n  ) %&gt;%\n  ungroup() %&gt;%\n  group_by(season, recent_team) %&gt;%\n  mutate(\n    moving_passing_epa = cumsum(passing_epa) / week,\n    moving_rushing_epa = cumsum(rushing_epa) / week,\n    moving_receiving_epa = cumsum(receiving_epa) / week) %&gt;%\n  select(season, recent_team, week, passing_epa, rushing_epa, receiving_epa, moving_passing_epa, \n         moving_rushing_epa, moving_receiving_epa) %&gt;%\n  filter(week == last_week) %&gt;%\n  mutate(week = this_week) %&gt;%\n  mutate(recent_team = ifelse(recent_team == \"LA\", \"LAR\", recent_team))\n\ndefensive_2024 &lt;- load_player_stats(\n  seasons = 2024,\n  stat_type = \"defense\") %&gt;%\n  filter(season_type == \"REG\") %&gt;%\n  group_by(season, team, week) %&gt;%\n  summarise(\n    tackles = sum(def_tackles, na.rm = TRUE),\n    forced_fumbles = sum(def_fumbles_forced, na.rm = TRUE),\n    sacks = sum(def_sacks, na.rm = TRUE),\n    ints = sum(def_interceptions, na.rm = TRUE),\n    pass_broken = sum( def_pass_defended, na.rm = TRUE)\n  ) %&gt;%\n  ungroup() %&gt;%\n  group_by(season, team) %&gt;%\n  mutate(\n    moving_tackles = cumsum(tackles) / week,\n    moving_forced_fumbles = cumsum(forced_fumbles) / week,\n    moving_sacks = cumsum(sacks) / week,\n    moving_ints = cumsum(ints) / week,\n    moving_pass_broken = cumsum(pass_broken) / week) %&gt;%\n  select(season, team, week, tackles, forced_fumbles, sacks, ints, pass_broken, moving_tackles, \n         moving_forced_fumbles, moving_sacks, moving_ints, moving_pass_broken) %&gt;%\n  filter(week == last_week) %&gt;%\n  mutate(week = this_week) %&gt;%\n  mutate(team = ifelse(team == \"LA\", \"LAR\", team))\n\nseason_2024 &lt;- load_schedules(seasons = 2024)\n\nseason_2024 &lt;- clean_homeaway(season_2024) %&gt;%\n  filter(game_type == \"REG\") %&gt;%\n  mutate(home = ifelse(location == \"home\", 1, 0),\n         win = ifelse(team_score &gt; opponent_score, 1, 0)) %&gt;%\n  select(game_id, season, week, team, opponent, home, win) %&gt;%\n  filter(week == this_week) %&gt;%\n  mutate(team = ifelse(team == \"LA\", \"LAR\", team),\n         opponent = ifelse(opponent == \"LA\", \"LAR\", opponent))\n\nmerged_2024 &lt;- inner_join(season_2024, qbr_2024, \n                          by = c(\"team\" = \"team_abb\", \"week\" = \"game_week\", \"season\")) %&gt;%\n  inner_join(offensive_2024, by = c(\"team\" = \"recent_team\", \"week\", \"season\")) %&gt;%\n  inner_join(defensive_2024, by = c(\"team\", \"week\", \"season\")) %&gt;%\n  group_by(game_id) %&gt;%\n  mutate(opp_qbr = lead(moving_qbr_mean),\n         opp_qbr = ifelse(is.na(opp_qbr), \n                          lag(moving_qbr_mean), opp_qbr),\n         opp_pass_epa = lead(moving_passing_epa),\n         opp_pass_epa = ifelse(is.na(opp_pass_epa), \n                               lag(moving_passing_epa), opp_pass_epa),\n         opp_rushing_epa = lead(moving_rushing_epa),\n         opp_rushing_epa = ifelse(is.na(opp_rushing_epa), \n                                  lag(moving_rushing_epa), opp_rushing_epa),\n         opp_receiving_epa = lead(moving_receiving_epa),\n         opp_receiving_epa = ifelse(is.na(opp_receiving_epa), \n                                    lag(moving_receiving_epa), opp_receiving_epa),\n         opp_tackles = lead(moving_tackles),\n         opp_tackles = ifelse(is.na(opp_tackles), \n                              lag(moving_tackles), opp_tackles),\n         opp_forced_fumbles = lead(moving_forced_fumbles),\n         opp_forced_fumbles = ifelse(is.na(opp_forced_fumbles), \n                                     lag(moving_forced_fumbles), opp_forced_fumbles),\n         opp_sacks = lead(moving_sacks),\n         opp_sacks = ifelse(is.na(opp_sacks), \n                            lag(moving_sacks), opp_sacks),\n         opp_ints = lead(moving_ints),\n         opp_ints = ifelse(is.na(opp_ints), \n                           lag(moving_ints), opp_ints),\n         opp_pass_broken = lead(moving_pass_broken),\n         opp_pass_broken = ifelse(is.na(opp_pass_broken), \n                                  lag(moving_pass_broken), opp_pass_broken)\n         ) %&gt;%\n  mutate(\n    qbr_diff = moving_qbr_mean - opp_qbr,\n    pass_epa_diff = moving_passing_epa - opp_pass_epa,\n    rushing_epa_diff = moving_rushing_epa - opp_rushing_epa,\n    receiving_epa_diff = moving_receiving_epa - opp_receiving_epa,\n    tackles_diff = moving_tackles - opp_tackles,\n    forced_fumbles_diff = moving_forced_fumbles - opp_forced_fumbles,\n    sacks_diff = moving_sacks - opp_sacks,\n    ints_diff = moving_ints - opp_ints,\n    pass_broken_diff = moving_pass_broken - opp_pass_broken\n  ) %&gt;%\n  mutate(win = factor(win, levels = c(0, 1), labels = c(\"Lose\", \"Win\"))) %&gt;%\n  filter(!is.na(qbr_diff))\n\n# Load Trained Models\nlog_fit &lt;- readRDS(\"data-and-analysis/log_fit_model.rds\")\nrf_fit &lt;- readRDS(\"data-and-analysis/rf_fit_model.rds\")\nsv_fit &lt;- readRDS(\"data-and-analysis/sv_fit_model.rds\")\nxgb_fit &lt;- readRDS(\"data-and-analysis/xgb_fit_model.rds\")\n\n# Create a Data Frame with Model Predictions\nlog_preds &lt;- predict(log_fit, merged_2024, type = \"prob\")[,2]\nrf_preds &lt;- predict(rf_fit, merged_2024, type = \"prob\")[,2]\nsv_preds &lt;- predict(sv_fit, merged_2024, type = \"prob\")[,2]\nxgb_preds &lt;- predict(xgb_fit, merged_2024, type = \"prob\")[,2]\n\npredictions &lt;- tibble(\n  Team = merged_2024$team,\n  Game_ID = merged_2024$game_id,\n  week = merged_2024$week,\n  Logistic_Regression = paste0(round(log_preds * 100, 2), \"%\"),\n  Random_Forest = paste0(round(rf_preds * 100, 2), \"%\"),\n  SVM = paste0(round(sv_preds * 100, 2), \"%\"),\n  XGBoost = paste0(round(xgb_preds * 100, 2), \"%\")\n)\n\n# Create a Back-Up Spreadsheet Before Updating\nglobal_preds &lt;- read_excel(\"data-and-analysis/nfl_2024_global_preds.xlsx\")\nwrite_xlsx(global_preds, \"data-and-analysis/nfl_2024_global_preds_backup.xlsx\")\n\n# Add Model Predictions for This Week to Season-Level Spreadsheet\nupdated_preds &lt;- inner_join(predictions, global_preds, by = c(\n  \"Team\", \"Game_ID\", \"week\", \"Logistic_Regression\", \"Random_Forest\", \"SVM\", \"XGBoost\"\n))\nwrite_xlsx(updated_preds, \"data-and-analysis/nfl_2024_global_preds.xlsx\")\n\nNow that I’ve created this, I hope that clicking “Run” now serves as the magical button that I just have to click and I get new predictions every week. We will see how this goes, as I’m sure there’s some bug/dependency I’m missing."
  },
  {
    "objectID": "blog/2024/causal-inference-simulation/index.html",
    "href": "blog/2024/causal-inference-simulation/index.html",
    "title": "Causal Inference for Casuals",
    "section": "",
    "text": "Code\n# Load Libraries\npacman::p_load(\n  \"dplyr\", # Data Manipulation\n  \"ggdag\", # Visualizing DAGs\n  \"ggplot2\", # Data Visualization\n  \"scales\", # Plotting Percentage Values\n  install = FALSE\n)\n\n# Define a Custom Theme - Taken From Andrew Heiss's Blogs\nblog_theme &lt;- function() {\n  theme_bw() +  # Start with theme_bw\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      plot.title = element_text(face = \"bold\"),\n      axis.title = element_text(face = \"bold\"),\n      strip.text = element_text(face = \"bold\"),\n      strip.background = element_rect(fill = \"grey80\", color = NA),\n      legend.title = element_text(face = \"bold\")\n    )\n}"
  },
  {
    "objectID": "blog/2024/causal-inference-simulation/index.html#mediators",
    "href": "blog/2024/causal-inference-simulation/index.html#mediators",
    "title": "Causal Inference for Casuals",
    "section": "Mediators",
    "text": "Mediators\nThe first type of variable that we want to avoid controlling for is the mediator. A mediator is any variable that is, in part caused by \\(X\\) which then, in part, causes a change in \\(Y\\).\n\n\nCode\n# Set Up the DAG\nsimple_dag &lt;- dagify(\n  Y ~ Z + X + M,\n  X ~ Z,\n  M ~ X,\n  exposure = \"X\",\n  outcome = \"Y\",\n  coords = list(x = c(Y = 7, X = 4, Z = 5.5, M = 5.5),\n                y = c(Y = 2, X = 2, Z = 3, M = 1.5)),\n  labels = c(Y = \"Y\", X = \"X\", Z = \"Z\", M = \"M\")\n)\n\n# Convert DAG to a Tidy Object for Plotting\nsimple_dag_tidy &lt;- simple_dag %&gt;% \n  tidy_dagitty() %&gt;%\n  mutate(status = case_when(\n    name == \"X\" ~ \"exposure\",\n    name == \"Y\" ~ \"outcome\",\n    name == \"M\" ~ \"mediator\",\n    TRUE ~ \"other\"\n  ))\n\nstatus_colors &lt;- c(exposure = \"#6f1a1a\", outcome = \"#384d35\", mediator = \"#7d8d67\")\n\n# Create Plot\nggplot(simple_dag_tidy, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point(aes(color = status)) +\n  geom_label(aes(label = label, fill = status),\n             color = \"white\", fontface = \"bold\", nudge_y = 0.15) +\n  scale_color_manual(values = status_colors, na.value = \"grey20\") +\n  scale_fill_manual(values = status_colors, na.value = \"grey20\") +\n  guides(color = \"none\", fill = \"none\") +\n  theme_dag()\n\n\n\n\n\nDAG Incluing a Mediator\n\n\n\n\nControlling for a mediator should be avoided because, if we control for it, our estimated effect of \\(X\\) on \\(Y\\) will be a deflated version of the causal effect of interest because controlling for the mediator effectively removes the part of the causal effect of \\(X\\) on \\(Y\\) that is mediated through \\(M\\). We can see this clearly with the following simulated regressions.\n\nset.seed(1234)\n\n# Simulate Data Where X-Y Relationship is Confounded\n# Number of Observations\nn &lt;- 3000\n\n# Simulate the Confounder as a Continuous Variable\nZ &lt;- rnorm(n, mean = 50, sd = 10)\n\n# Simulate the Treatment as a Binary Variable That is Impacted by Z\n# Using the Inverse Logit Function\nprob_X &lt;- 1 / (1 + exp(-(-2.5 + (0.05*Z))))\nX &lt;- rbinom(n, size = 1, prob = prob_X)\n\n# Simulate a Mediator That is Impacted by X\nM &lt;- (10*X) + rnorm(n, mean = 0, sd = 10) \n\n# Simulate the Outcome with a Base Number to Start with (50), the Impact of X,\n# the Impact of Z, the Impact of M Mediated by X, and Random Noise. The Estimated Total Effect of X Should Be 30\nY &lt;- 50 + ((20*X) + M) + (5*Z) + rnorm(n, mean = 0, sd = 10) \n\n# Combine This Data\nsim_data &lt;- tibble(\n  n = 1:n,\n  Y = Y,\n  Z = Z,\n  X = X,\n  M = M,\n)\n\n# Run a Biased Model Where the Mediator is Included \nsummary(lm(Y ~ X + Z + M, data = sim_data))\n\n\nCall:\nlm(formula = Y ~ X + Z + M, data = sim_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.299  -6.454  -0.020   6.449  33.509 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 50.08945    0.91749   54.59   &lt;2e-16 ***\nX           20.17165    0.41190   48.97   &lt;2e-16 ***\nZ            5.00013    0.01851  270.16   &lt;2e-16 ***\nM            0.98048    0.01810   54.18   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.805 on 2996 degrees of freedom\nMultiple R-squared:  0.9708,    Adjusted R-squared:  0.9708 \nF-statistic: 3.325e+04 on 3 and 2996 DF,  p-value: &lt; 2.2e-16\n\n# Run an Unbiased Model Where the Mediator is Excluded\nsummary(lm(Y ~ X + Z, data = sim_data))\n\n\nCall:\nlm(formula = Y ~ X + Z, data = sim_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-52.431  -9.296   0.177   9.374  45.548 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 48.52958    1.29011   37.62   &lt;2e-16 ***\nX           30.06503    0.51941   57.88   &lt;2e-16 ***\nZ            5.03174    0.02602  193.35   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.79 on 2997 degrees of freedom\nMultiple R-squared:  0.9423,    Adjusted R-squared:  0.9422 \nF-statistic: 2.446e+04 on 2 and 2997 DF,  p-value: &lt; 2.2e-16\n\n\nNote that, in this blog post, \\(X\\) = 20 is the sort of unbiased causal effect we have been looking for so far. However, I’ve modified that a bit here because our mediator contributes to the causal effect of \\(X\\) on \\(Y\\). In reality, our causal effect of \\(X\\) should be the direct effect of \\(X\\) on \\(Y\\) (20) but it should also include the total effect that is mediated through \\(M\\) (10). In the first model, we see our direct effect is correct, but we are not attempting to estimate the direct effect. The direct effect is accurate because we are controlling for our mediator and our mediator is not confounded. That’s right… if you want to work with a mediator to estimate direct and indirect effects (this practice is referred to as mediation analysis) of \\(X\\) and \\(M\\) on \\(Y\\), you’ve doubled the challenge of dealing with confounding because, not only do you have to make sure that \\(X\\) \\(\\rightarrow\\) \\(Y\\) is unconfounded, you also have to make sure that \\(M\\) \\(\\rightarrow\\) \\(Y\\) is also unconfounded. This is a major reason behind the skepticism for mediation analyses. Regardless, as you can see in the second model where \\(M\\) is excluded, we get the correct total effect of \\(X\\) on \\(Y\\), which includes the effect of \\(X\\) on \\(Y\\) and the effect of \\(M\\) on \\(Y\\) that is caused by \\(X\\)."
  },
  {
    "objectID": "blog/2024/causal-inference-simulation/index.html#colliders",
    "href": "blog/2024/causal-inference-simulation/index.html#colliders",
    "title": "Causal Inference for Casuals",
    "section": "Colliders",
    "text": "Colliders\nColliders are interesting because they are very sneaky. I like to think of them as inverse confounders. You need to control for a confounder (\\(X\\) \\(\\leftarrow\\) \\(Z\\) \\(\\rightarrow\\) \\(Y\\)) but you should not control for a collider (\\(X\\) \\(\\rightarrow\\) \\(C\\) \\(\\leftarrow\\) \\(Y\\)). The crucial difference here is the causal ordering of the third variable as it relates to the treatment and outcome. Whereas a confounder is a common cause of \\(X\\) and \\(Y\\), a collider is a common consequence of \\(X\\) and \\(Y\\). In the following DAG, note that I am not positing that \\(X\\) impacts \\(Y\\). This is intentional.\n\n\nCode\n# Set Up the DAG\nsimple_dag &lt;- dagify(\n  Y ~ Z,\n  X ~ Z,\n  C ~ X + Y,\n  exposure = \"X\",\n  outcome = \"Y\",\n  coords = list(x = c(Y = 7, X = 4, Z = 5.5, C = 5.5),\n                y = c(Y = 2, X = 2, Z = 3, C = 1.5)),\n  labels = c(Y = \"Y\", X = \"X\", Z = \"Z\", C = \"C\")\n)\n\n# Convert DAG to a Tidy Object for Plotting\nsimple_dag_tidy &lt;- simple_dag %&gt;% \n  tidy_dagitty() %&gt;%\n  mutate(status = case_when(\n    name == \"X\" ~ \"exposure\",\n    name == \"Y\" ~ \"outcome\",\n    name == \"C\" ~ \"collider\",\n    TRUE ~ \"other\"\n  ))\n\nstatus_colors &lt;- c(exposure = \"#6f1a1a\", outcome = \"#384d35\", collider = \"#ec9b00\")\n\n# Create Plot\nggplot(simple_dag_tidy, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point(aes(color = status)) +\n  geom_label(aes(label = label, fill = status),\n             color = \"white\", fontface = \"bold\", nudge_y = 0.15) +\n  scale_color_manual(values = status_colors, na.value = \"grey20\") +\n  scale_fill_manual(values = status_colors, na.value = \"grey20\") +\n  guides(color = \"none\", fill = \"none\") +\n  theme_dag()\n\n\n\n\n\nDAG Incluing a Collider\n\n\n\n\nWhy does controlling for a common consequence of treatment and outcome lead to problems? Because it can suggest a spurious association between \\(X\\) and \\(Y\\). In other words, controlling for a collider can create an association between \\(X\\) and \\(Y\\) that does not reflect a causal relationship. This is why I like to think of colliders as inverse confounders. By default, confounders create spurious associations between \\(X\\) and \\(Y\\), and we combat this and remove the spurious association to make causal inferences by controlling for the confounder. In contrast, we can introduce spurious associations as well by controlling for certain variables; common causes.\nBut this is still vague… What is a hypothetical example of this? To keep thing’s grounded, I’ll borrow an example from Rohrer 2018. On a particularly lonely day, an individual may be reminiscing on their former romantic partners. They start thinking about the traits of their former romantic partners, and they begin to notice that their more attractive partners were less intelligent than their comparatively less attractive partners. (In DAG terms, this individual is considering whether Attractiveness \\(\\rightarrow\\) Intelligence). Should this person conclude that, among all potential partners, the more attractive they are, the less intelligent they will be? No! Because this observation is conditioned on a collider. And that collider is the fact they have dated the subjects. Most people are attracted to physically attractive and intelligent people, so whether or not this individual has dated someone is likely influenced by their attractiveness and intelligence (Attractiveness \\(\\rightarrow\\) Dated \\(\\leftarrow\\) Intelligence). We can clearly see that whether or not this individual has dated someone is a collider. By just looking at their former partners, they are introducing a spurious association between physical attraction and intelligence.\nOkay but, among this subset of people, the individual still finds a negative association between physical attraction and intelligence. Maybe it’s not fair to generalize this relationship to the broader dating pool, but how does that explain this individual’s former partners? We can explain this with a couple of reasons. First, it is rare that this individual could find someone else who is both highly attractive and highly intelligent. Even if they could, that person would likely already be taken. Second, this individual is probably unlikely to date someone who is also low in both physical attraction and intelligence. As a result, their former partners varied in both intelligence and physical attraction. Sometimes, a partner was more attractive, but not equally intelligent. At other times, a partner was more intelligent, but not as physically attractive. If this person would not have developed their conclusion based on their prior dating experiences (this is equivalent to controlling for people they have previously dated), they likely would not have found that this negative relationship holds up. In a very roundabout way, we have also described a type of selection bias here. By conditioning on a collider, we are limiting our analysis to a subset of cases that are impacted by treatment and outcome. This serves to bias our analysis and create associations that some might interpret as causal while no such association actually exists. This issue does not become a problem if you simply don’t control for a collider.\nLet’s put this into practice using simulation. In contrast to the mediator example, I am not going to simulate a relationship between \\(X\\) and \\(Y\\). If our model estimates such a relationship, there is a problem.\n\nset.seed(1234)\n\n# Simulate Data with No X-Y Relationship\n# Number of Observations\nn &lt;- 3000\n\n# Simulate the Confounder as a Continuous Variable\nZ &lt;- rnorm(n, mean = 50, sd = 10)\n\n# Simulate the Treatment as a Binary Variable That is Impacted by Z\n# Using the Inverse Logit Function\nprob_X &lt;- 1 / (1 + exp(-(-2.5 + (0.05*Z))))\nX &lt;- rbinom(n, size = 1, prob = prob_X)\n\n# Simulate the Outcome with a Base Number to Start with (50), \n# the Impact of Z, and Random Noise. \nY &lt;- 50 + (5*Z) + rnorm(n, mean = 0, sd = 10) \n\n# Simulate a Collider\nC &lt;- (10*X) + (10*Y) + rnorm(n, mean = 0, sd = 10) \n\n# Combine This Data\nsim_data &lt;- tibble(\n  n = 1:n,\n  Y = Y,\n  Z = Z,\n  X = X,\n  C = C,\n)\n\n# Run a Biased Model Including the Collider\nsummary(lm(Y ~ X + Z + C, data = sim_data))\n\n\nCall:\nlm(formula = Y ~ X + Z + C, data = sim_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3454 -0.6454  0.0037  0.6414  3.3643 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.3664761  0.1262877   2.902  0.00374 ** \nX           -0.9889660  0.0368645 -26.827  &lt; 2e-16 ***\nZ            0.0393890  0.0092391   4.263 2.08e-05 ***\nC            0.0992183  0.0001799 551.507  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9776 on 2996 degrees of freedom\nMultiple R-squared:  0.9996,    Adjusted R-squared:  0.9996 \nF-statistic: 2.735e+06 on 3 and 2996 DF,  p-value: &lt; 2.2e-16\n\n# Run an Unbiased Model Excluding the Collider\nsummary(lm(Y ~ X + Z, data = sim_data))\n\n\nCall:\nlm(formula = Y ~ X + Z, data = sim_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-31.885  -6.900  -0.085   6.837  36.475 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 48.40907    0.92564  52.298   &lt;2e-16 ***\nX            0.09033    0.37268   0.242    0.808    \nZ            5.03224    0.01867 269.502   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.897 on 2997 degrees of freedom\nMultiple R-squared:  0.9626,    Adjusted R-squared:  0.9626 \nF-statistic: 3.855e+04 on 2 and 2997 DF,  p-value: &lt; 2.2e-16\n\n\nAs you can see, when I controlled for the collider, a truly non-existent relationship between \\(X\\) and \\(Y\\) shows up. In contrast, when \\(C\\) is omitted, we don’t observe an effect of \\(X\\) on \\(Y\\) which, in this case, is true! Beware colliders, folks, and think carefully before you control for a variable."
  },
  {
    "objectID": "blog/2024/causal-inference-simulation/index.html#ancestors-of-outcome-and-treatment",
    "href": "blog/2024/causal-inference-simulation/index.html#ancestors-of-outcome-and-treatment",
    "title": "Causal Inference for Casuals",
    "section": "Ancestors of Outcome and Treatment",
    "text": "Ancestors of Outcome and Treatment\nLastly, we get to the ancestors. These types of variables are interesting because they are very common and you could identify a lot of them, but you don’t have to control for any of them. And, if you do control for them, it might not be a huge deal. Why is this the case? After all, there seemed to be pretty major consequences when controlling for a mediator/collider?\nAs you can see in the DAG below, we have two types of ancestors. An ancestor of \\(X\\) and an ancestor of \\(Y\\). Each of these impact their respective node, but are otherwise unconnected to other nodes in the DAG. Because neither is related to both \\(X\\) and \\(Y\\), controlling/not controlling for these should not impact the causal interpretation of your estimate… with a couple of caveats.\n\n\nCode\n# Set Up the DAG\nsimple_dag &lt;- dagify(\n  Y ~ Z + X + AY,\n  X ~ Z + AX,\n  exposure = \"X\",\n  outcome = \"Y\",\n  coords = list(x = c(Y = 7, X = 4, Z = 5.5, AX = 3.25, AY = 7.75),\n                y = c(Y = 2, X = 2, Z = 3, AX = 2.5, AY = 2.5)),\n  labels = c(Y = \"Y\", X = \"X\", Z = \"Z\", AX = \"AX\", AY = \"AY\")\n)\n\n# Convert DAG to a Tidy Object for Plotting\nsimple_dag_tidy &lt;- simple_dag %&gt;% \n  tidy_dagitty() %&gt;%\n  mutate(status = case_when(\n    name == \"X\" ~ \"exposure\",\n    name == \"Y\" ~ \"outcome\",\n    name == \"AX\" ~ \"ax\",\n    name == \"AY\" ~ \"ay\",\n    TRUE ~ \"other\"\n  ))\n\nstatus_colors &lt;- c(exposure = \"#6f1a1a\", outcome = \"#384d35\", ax = \"#a32b2b\", ay = \"#597657\")\n\n# Create Plot\nggplot(simple_dag_tidy, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point(aes(color = status)) +\n  geom_label(aes(label = label, fill = status),\n             color = \"white\", fontface = \"bold\", nudge_y = 0.1) +\n  scale_color_manual(values = status_colors, na.value = \"grey20\") +\n  scale_fill_manual(values = status_colors, na.value = \"grey20\") +\n  guides(color = \"none\", fill = \"none\") +\n  theme_dag()\n\n\n\n\n\nDAG Incluing Ancestors of Treatment and Outcome\n\n\n\n\nTo demonstrate, I am simply simulating a variable that causes a change in \\(X\\) (\\(AX\\)) and a variable that causes a change in \\(Y\\) (\\(AY\\))… and that’s all that these variables do. Then, I run a regression controlling for neither, and a respective regression controlling for each. Let’s see what we get.\n\nset.seed(1234)\n\n# Simulate Data Where X-Y Relationship is Confounded\n# Number of Observations\nn &lt;- 3000\n\n# Simulate the Confounder as a Continuous Variable\nZ &lt;- rnorm(n, mean = 50, sd = 10)\n\n# Simulate Ancestors of Treatment/Outcome\nAX &lt;- rnorm(n, mean = 10, sd = 10)\nAY &lt;- rnorm(n, mean = 10, sd = 10)\n\n# Simulate the Treatment as a Binary Variable That is Impacted by Z and AX\n# Using the Inverse Logit Function\nprob_X &lt;- 1 / (1 + exp(-(-2.5 + (0.05*Z) + (0.1*AX))))\nX &lt;- rbinom(n, size = 1, prob = prob_X)\n\n# Simulate the Outcome with a Base Number to Start with (50), the Impact of X,\n# the Impact of Z, the Impact of AY, and Random Noise. The Estimated Effect of X Should Be 20\nY &lt;- 50 + (20*X) + (5*Z) + (2*AY) + rnorm(n, mean = 0, sd = 10) \n\n# Combine This Data\nsim_data &lt;- tibble(\n  n = 1:n,\n  Y = Y,\n  Z = Z,\n  X = X,\n  AX = AX,\n  AY = AY\n)\n\n# Run a Regression Omitting Both the Ancestor of Treatment and Ancestor of Outcome\nsummary(lm(Y ~ X + Z, data = sim_data))\n\n\nCall:\nlm(formula = Y ~ X + Z, data = sim_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-76.469 -14.940   0.352  14.707  83.582 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 70.43861    2.08184   33.84   &lt;2e-16 ***\nX           18.82075    0.89826   20.95   &lt;2e-16 ***\nZ            5.01715    0.04171  120.28   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 22.21 on 2997 degrees of freedom\nMultiple R-squared:  0.849, Adjusted R-squared:  0.8489 \nF-statistic:  8426 on 2 and 2997 DF,  p-value: &lt; 2.2e-16\n\n# Run a Regression Only Including the Ancestor of Treatment\nsummary(lm(Y ~ X + Z + AX, data = sim_data))\n\n\nCall:\nlm(formula = Y ~ X + Z + AX, data = sim_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-75.912 -15.042   0.284  14.718  83.053 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 70.87527    2.11189  33.560   &lt;2e-16 ***\nX           19.26880    0.96962  19.873   &lt;2e-16 ***\nZ            5.01320    0.04183 119.834   &lt;2e-16 ***\nAX          -0.05481    0.04468  -1.227     0.22    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 22.21 on 2996 degrees of freedom\nMultiple R-squared:  0.8491,    Adjusted R-squared:  0.8489 \nF-statistic:  5619 on 3 and 2996 DF,  p-value: &lt; 2.2e-16\n\n# Run a Regression Only Including the Ancestor of Outcome\nsummary(lm(Y ~ X + Z + AY, data = sim_data))\n\n\nCall:\nlm(formula = Y ~ X + Z + AY, data = sim_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-39.969  -6.847   0.187   6.650  36.293 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 49.81184    0.95232   52.31   &lt;2e-16 ***\nX           19.75974    0.40282   49.05   &lt;2e-16 ***\nZ            5.00853    0.01870  267.80   &lt;2e-16 ***\nAY           1.99510    0.01828  109.15   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.957 on 2996 degrees of freedom\nMultiple R-squared:  0.9697,    Adjusted R-squared:  0.9696 \nF-statistic: 3.192e+04 on 3 and 2996 DF,  p-value: &lt; 2.2e-16\n\n\nWhen neither ancestor is included, we get pretty close to the actual treatment size. Similarly, when either ancestor is included, the estimated effect doesn’t change much and is pretty unbiased. So, what changed? Check out the standard errors! Without controlling for either ancestor, the standard error for the \\(X\\) estimate is 0.9. However, when we control for \\(AX\\), that standard error gets a bit bigger (0.97). In contrast, when we control for \\(AY\\), the standard error shrinks all the way down to 0.4. While controlling for either did not impact our causal effect estimate, including these variables did have an impact on the precision of our estimate. In other words, they respectively increased/decreased statistical uncertainty regarding whether these effect are statistically different from 0. In all cases, the estimated effect of \\(X\\) was still statistically significant, but that is likely due to how simple my simulated data are.\nSo what are those two caveats that I briefly mentioned a second ago. First, don’t always think of \\(AX\\) being bad. As a general rule, you probably should not control for \\(AX\\), but you should think about what your ancestors of treatment are. If, as I simulated, your ancestor of treatment only impacts \\(Y\\) through \\(X\\) and is totally unrelated to any other node in your DAG, you probably have an instrument which is a whole other can of worms that I won’t open here but basically, the TLDR is that you can use this instrument as a form of randomization… Long story but this is where instrumental variables designs are relevant. Although, another caveat is warranted here because identifying actual instruments is very, very hard in practice.\nThe second caveat is one that confuses me. Notice how I made the effect size of \\(AY\\) so small compared to all of the other effect sizes we have been working with thus far? Well, I did that because making \\(AY\\) large (as ancestors of outcome will often be in practice) really screwed up results and made the estimate for \\(X\\) very biased (like, down in the 13-16 range). I don’t know why this is the case. Theoretically, \\(AY\\) has nothing to do with \\(X\\), so including it should not impact the estimate for \\(X\\). Nonetheless, it did and I do not fully understand why. I don’t think I’ve discovered something novel… in reality, something is going on in the math behind the simulation that I’m sure I’m not understanding. Regardless, controlling for ancestors of outcome often results in increase to statistical precision, so it’s not like its the end of the world to have to control for \\(AY\\)."
  },
  {
    "objectID": "blog/2024/dynamic-causal-inference/index.html",
    "href": "blog/2024/dynamic-causal-inference/index.html",
    "title": "An Introduction to Dynamic Causal Inference",
    "section": "",
    "text": "Code\n# Load Libraries\npacman::p_load(\n  \"dplyr\", # Data Manipulation\n  \"ggplot2\", # Data Visualization\n  \"ggtext\", # Labels\n  \"dagitty\", # Creating DAGs\n  \"ggdag\", # Plotting DAGs\n  install = FALSE\n)\n\n# Define a Custom Theme - Taken From Andrew Heiss's Blogs\nblog_theme &lt;- function() {\n  theme_bw() +  # Start with theme_bw\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      plot.title = element_text(face = \"bold\"),\n      axis.title = element_text(face = \"bold\"),\n      strip.text = element_text(face = \"bold\"),\n      strip.background = element_rect(fill = \"grey80\", color = NA),\n      legend.title = element_text(face = \"bold\")\n    )\n}"
  },
  {
    "objectID": "blog/2024/dynamic-causal-inference/index.html#footnotes",
    "href": "blog/2024/dynamic-causal-inference/index.html#footnotes",
    "title": "An Introduction to Dynamic Causal Inference",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote: if you are already familiar with the core concepts of causal inference, feel free to skip down to the section where I start discussing time and why it messes things up.↩︎\nWell… kind of. There are actually a lot of other assumptions that you would need to check for first, like positivity, SUTVA, no measurement error, etc. (which will not be covered here in sufficient detail if you are not familiar with these topics). If you’re not super familiar with these topics, I highly recommend checking out Chatton and Rohrer 2024.↩︎\nI do not condone this attitude… never treat your regressions or research designs this way.↩︎\nHere are some good starting materials for DAGs, simulation, and sensitivity analyses: Rohrer 2018, Blair et al. 2023, Cinelli and Hazlett 2020.↩︎"
  },
  {
    "objectID": "blog/2024/simulating-panel-data/index.html",
    "href": "blog/2024/simulating-panel-data/index.html",
    "title": "Simulating Complex Panel Data to Validate Model Performance in Estimating Dynamic Treatment Effects",
    "section": "",
    "text": "Code\n# Load Libraries\npacman::p_load(\n  \"dplyr\", # Data Manipulation\n  \"ggplot2\", # Data Visualization\n  \"ggdag\", # Visualizing DAGs\n  \"dagitty\", # More DAGs Stuff\n  \"tidyr\", # Re-Shaping Data\n  \"purrr\", # map() Function\n  \"ipw\", # IPW\n  install = FALSE\n)\n\n# Define a Custom Theme - Taken From Andrew Heiss's Blogs\nblog_theme &lt;- function() {\n  theme_bw() +  # Start with theme_bw\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      plot.title = element_text(face = \"bold\"),\n      axis.title = element_text(face = \"bold\"),\n      strip.text = element_text(face = \"bold\"),\n      strip.background = element_rect(fill = \"grey80\", color = NA),\n      legend.title = element_text(face = \"bold\")\n    )\n}\n\n\n\nIntro\nOver the past year, I’ve spent a lot of time reviewing the literature covering the intersection between causal inference and panel (longitudinal) data. This intersection may sound niche, but I’ve almost always worked with panel data for academic, personal, and professional projects, and I’ve found that most of the introductory causal inference educational material teaches causal inference in the cross-sectional setting.\nThis practice makes sense because the moment you introduce time into a cross-sectional data set, making causal inferences becomes more complicated. However, I’ve found that in many fields (including my own), this necessary attention to detail is rarely ever acknowledged (if it is even widely understood). Researchers are well-trained in understanding how panel data creates problems for their standard errors and hypothesis testing. Yet, this extension to causal inference and point estimation still seems to fly under the radar quite a bit.\nSo, what’s the point of this blog? Well, its purpose is twofold. First, I want to show you how to simulate complex panel data that might resemble the data that you could end up working with. If you’re curious about why simulation matters, I highly recommend checking out my previous blog post first. Panel data isn’t something so simple to simulate if (and this is almost always the case) variables from a previous time impact variables in the future. Second, I want to show how conventional methodologies used with panel data (like regression adjustment) and others perform when we have complex panel data and want to estimate dynamic (lagged) effects. I am really trying to drive home the point that a lot of the conventional tools we use are very inappropriate and end up estimating very wrong results!\nWhy is this the case? Check out the following section! (And, if you want, check out my first blog post that also covers this topic as well.)\n\n\nThe Problems with Panel Data\nI’ll keep this part fairly sort because I do reference this point in this blog post, but panel data is problematic for both simulation and estimation.\nFor simulation, panel data is tricky because we have to build in time-varying effects into the simulation. In the simple cross-sectional setting, simulation is pretty easy. For example, consider the following code:\n\nset.seed(12345)\n\n# Establish the Sample Size\nn &lt;- 2000\n\ncross_sectional &lt;- tibble(\n  # Create a Cross-Sectional ID\n  id = 1:n,\n  # Create a Confounder\n  Z = rnorm(n, 0, 1),\n  # Create a Treatment\n  X = (0.5 * Z) + rnorm(n, 0, 1),\n  # Create an Outcome\n  Y = (0.5 * Z) + (0.3 * X) + rnorm(n, 0, 1)\n)\n\nThis is easy enough because the cross-sectional just represents a snapshot. But if we added time as an element, it get’s trickier. We have to specify how prior \\(Z\\) values impact future \\(Z\\), \\(X\\), and \\(Y\\) values. We have to specify how prior \\(X\\) values impact future \\(X\\), \\(Z\\), and \\(Y\\) values. We have to specify how prior \\(Y\\) values impact future \\(Y\\), \\(Z\\), and \\(X\\) values. Keep in mind, that sounds like a lot and we are only working with one confounder in this circumstance.\nStill, assuming just one confounder, you would need to specify one lagged effect for each variable if you’re panel data set included only two time periods. Basically, with two time periods (and assuming all prior values impact all current values), you’d need to specify the following for your three variables:\n\\[\nZ_t = Z_{t-1} + X_{t-1} + Y_{t-1} + \\mu\n\\]\n\\[\nX_t = Z_t + Z_{t-1} + X_{t-1} + Y_{t-1} + \\mu\n\\]\n\\[\nY_t = X_t + Z_t + Z_{t-1} + X_{t-1} + Y_{t-1} + \\mu\n\\]\nWhat if you had three time periods (\\(T\\) = 3)? Well, if you kept simulating using this same approach, you’d have to manually specify this same formula and model \\(X_{t-1}\\), \\(Y_{t-1}\\), \\(Z_{t-1}\\) as a function of \\(X_{t-2}\\), \\(Y_{t-2}\\), \\(Z_{t-2}\\). This is already a headache! As \\(T\\) increases, so does the amount of manual code. Fortunately, there is an easy fix for this that we’ll cover later. Now onto the second problem of panel data.\nPanel data really challenges how our conventional process of “controlling” for things works. Let’s create a couple of DAGs to illustrate why this is the case.\n\n\nCode\ndgp1_dag &lt;- dagitty('dag {\n  \"Z[TIC]\" [pos=\"2.5,2\"]\n  \"X[t-1]\" [pos=\"1,1\"]\n  \"Y[t-1]\" [pos=\"2,1.25\"]\n  \"X[t]\" [pos=\"3,1\"]\n  \"Y[t]\" [pos=\"4,1.25\"]\n  \"Z[TIC]\" -&gt; \"X[t-1]\"\n  \"Z[TIC]\" -&gt; \"Y[t-1]\"\n  \"Z[TIC]\" -&gt; \"X[t]\"\n  \"Z[TIC]\" -&gt; \"Y[t]\"\n  \"X[t-1]\" -&gt; \"Y[t-1]\"\n  \"Y[t-1]\" -&gt; \"X[t]\"\n  \"X[t]\" -&gt; \"Y[t]\"\n  \"X[t-1]\" -&gt; \"X[t]\"\n  \"Y[t-1]\" -&gt; \"Y[t]\"\n}') %&gt;%\n  tidy_dagitty()\n\nggplot(dgp1_dag, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point(color = \"grey80\", size = 16) +\n  geom_dag_text(color = \"black\", size = 5, parse = TRUE) +\n  theme_dag()\n\n\n\n\n\nDAG of Treatment-Outcome Feedback and a Time-Invariant Confounder\n\n\n\n\nThis DAG represents a data generating process (DGP) where treatment (\\(X\\)) impacts the outcome (\\(Y\\)) which subsequently impacts future treatments which impacts future outcomes, etc. In other words, this represents a treatment-outcome feedback loop. In addition, a time-invariant confounder (\\(Z_{TIC}\\)) impacts each treatment and outcome value. If I wanted to estimate the current and lagged effect of \\(X\\) on \\(Y\\) with this DGP, why can’t I just run lm(Y ~ X + lag(X) + Z, data = data)? After all, I am controlling for the confounder, so what’s the problem?\nThe issue is that panel data creates a “damned if you do, damned if you don’t” situation. If you look at this DAG closely, you might notice that something sneaky has happened. There is more than one confounder. Remember what a confounder is: any variable that causally impacts both the treatment and outcome of interest. If we want to estimate the effect of \\(X_{t}\\) on \\(Y_{t}\\), we need to control for \\(Z_{TIC}\\) yes, but pay attention to the \\(Y_{t-1}\\) node. It impacts both \\(X_{t}\\) and \\(Y_{t}\\). In other words, it confounds the relationship between treatment and outcome and, if we fail to control for it, our estimate if biased and confounded.\nSo, just control for it, right? What’s wrong with running lm(Y ~ X + lag(X) + Z + lag(Y), data = data)? Indeed, if you run this, you’d be able to estimate the effect of \\(X_{t}\\) on \\(Y_{t}\\), but you’d no longer be able to estimate the *lagged effect of \\(X\\). Why is that? By controlling for \\(Y{_t-1}\\), you are blocking the portion of the effect of \\(X{_t-1}\\) on \\(Y_{t}\\) that runs through \\(Y{_t-1}\\). To help visualize this, imagine drawing a big red “X” over the \\(Y{_t-1}\\) node. By controlling for it, it’s effect is blocked, which is necessary for making a causal inference about the \\(X_{t} \\rightarrow Y_{t}\\) relationship but simultaneously makes an unbiased causal inference about the \\(X_{t-1} \\rightarrow Y_{t}\\) relationship impossible. Damned if you do, damned if you don’t indeed.\nBut what if you don’t suspect a treatment-outcome feedback? Are you safe then? Nope! Because, in the prior DGP, the confounder was time-invariant. It doesn’t change over time. But a lot of confounders are time-varying and do change over time. The DAG below presents a DGP where a time-varying confounder is present.\n\n\nCode\ndgp3_dag &lt;- dagitty('dag {\n  \"X[t-1]\" [pos=\"1,3\"]\n  \"Y[t-1]\" [pos=\"3,3\"]\n  \"X[t]\" [pos=\"1,1\"]\n  \"Y[t]\" [pos=\"3,1\"]\n  \"Z[t-1]\" [pos=\"2,4\"]\n  \"Z[t]\" [pos=\"2,2\"]\n  \"Z[t-1]\" -&gt; \"X[t-1]\"\n  \"Z[t-1]\" -&gt; \"Y[t-1]\"\n  \"Z[t-1]\" -&gt; \"Z[t]\"\n  \"X[t-1]\" -&gt; \"Y[t-1]\"\n  \"X[t-1]\" -&gt; \"Z[t]\"\n  \"X[t-1]\" -&gt; \"X[t]\"\n  \"Y[t-1]\" -&gt; \"Y[t]\"\n  \"Z[t]\" -&gt; \"X[t]\"\n  \"Z[t]\" -&gt; \"Y[t]\"\n  \"X[t]\" -&gt; \"Y[t]\"\n}') %&gt;%\n  tidy_dagitty()\n\nggplot(dgp3_dag, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point(color = \"grey80\", size = 16) +\n  geom_dag_text(color = \"black\", size = 5, parse = TRUE) +\n  theme_dag()\n\n\n\n\n\nDAG of Time-Varying Confounding\n\n\n\n\nAs you can see, while we don’t have a treatment-outcome feedback loop, we do have a treatment-confounder feedback loop. And, as you could probably guess, this also creates a “damned if you do, damned if you don’t” situation. If I control for \\(Z_{t}\\), I de-bias the \\(X_{t} \\rightarrow Y_{t}\\) relationship but bias the \\(X_{t-1} \\rightarrow Y_{t}\\) relationship. If I decided to omit controlling for \\(Z_{t}\\), then the problem would be vice versa. No conventional adjustment method will solve this problem.\nControlling for a variable in a regression equation (regression adjustment), matching, fixed effects, including lagged variables as covariates, you name it. None of these resolve this issue. And that’s very problematic because time-varying treatments and confounders are probably the norm and not the exception. So, what are you to do with a panel data set where you are interested in the estimation of lagged treatment effects? Before diving into that, let’s resolve the first problem with panel data by demonstrating how you can simulate such complex data.\n\n\nSimulating Panel Data Generating Processes\nFirst, let’s figure out how to simulate a DGP like the one represented in the first DAG where we have treatment-outcome feedback and one time-invariant confounder. Also, massive shout-out to Andrew Heiss and his blog post where I learned about this approach. This first DGP will only include two time periods.\n\nset.seed(12345)\nn &lt;- 2000\n\ndgp1_wide &lt;- tibble(id = 1:n,\n                    # Create a Time Invariant Confounder\n                    Z = rnorm(n, 0, 1)) %&gt;% \n  # Create Lagged and Current Treatment and Outcome Values\n  mutate(X1 = (0.5 * Z) + rnorm(n, 0, 1),\n         Y1 = (0.3 * Z) + (0.4 * X1) + rnorm(n, 0, 1),\n         X2 = (0.5 * Z) + (0.4 * Y1) + (0.4 * X1) + rnorm(n, 0, 1),\n         Y2 = (0.1 * Z) + (0.4 * X2) + (0.4 * Y1) + rnorm(n, 0, 1))\n\nhead(dgp1_wide)\n\n# A tibble: 6 × 6\n     id      Z     X1     Y1      X2     Y2\n  &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1     1  0.586 -0.315 -0.993 -0.579  -2.06 \n2     2  0.709  1.43   0.601  0.142  -1.84 \n3     3 -0.109 -0.631 -0.106 -1.26    0.850\n4     4 -0.453  0.872 -0.456  0.714  -2.28 \n5     5  0.606  1.71  -0.139 -0.0182  0.629\n6     6 -1.82  -0.872 -0.758 -2.80   -3.54 \n\n\nThis is good, but we don’t have a time variable. Instead, we just have cross-sectional variables whose temporal components are separated into different columns. We can fix this by pivoting the data longer.\n\ndgp1_long &lt;- dgp1_wide %&gt;%\n  # Pivot This Wider to Create a Time ID Column\n  pivot_longer(cols = c(X1, Y1, X2, Y2)) %&gt;% \n  separate(name, into = c(\"variable\", \"time\"), sep = 1) %&gt;% \n  pivot_wider(names_from = \"variable\", values_from = \"value\") %&gt;% \n  # Create Lagged Treatment and Outcome Columns\n  group_by(id) %&gt;% \n  mutate(across(c(X, Y), list(lag = lag))) %&gt;% \n  ungroup()\n\nhead(dgp1_long)\n\n# A tibble: 6 × 7\n     id      Z time       X      Y  X_lag  Y_lag\n  &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     1  0.586 1     -0.315 -0.993 NA     NA    \n2     1  0.586 2     -0.579 -2.06  -0.315 -0.993\n3     2  0.709 1      1.43   0.601 NA     NA    \n4     2  0.709 2      0.142 -1.84   1.43   0.601\n5     3 -0.109 1     -0.631 -0.106 NA     NA    \n6     3 -0.109 2     -1.26   0.850 -0.631 -0.106\n\n\nAnd that checks out! But this is only for two time periods. As mentioned earlier, this approach will not work for more time periods, because we would have to specify the lagged effects for each variable for each time period. The following solution shows a way to automate this process while maintaining the same DGP.\n\n# Create a First Year Data Set \ndgp2_first_year &lt;- expand_grid(id = 1:n, time = 1) %&gt;% \n  mutate(Z = rnorm(n, 0, 1),  \n         X = (0.5 * Z) + rnorm(n, 0, 1),\n         Y = (0.3 * Z) + (0.4 * X) + rnorm(n, 0, 1))\n\n# Add Another 9 Empty Time Periods\ndgp2_panel_empty &lt;- dgp2_first_year %&gt;% \n  bind_rows(expand_grid(id = 1:n, time = 2:10)) %&gt;% \n  arrange(id, time)\n\n# Add Noise with a Custom dgp() Function\ndgp2 &lt;- function(df) {\n  for (i in 2:nrow(df)) {\n    df$X[i] &lt;- (0.5 * df$Z[i]) + (0.4 * df$Y[i - 1]) + (0.4 * df$X[i - 1]) + rnorm(1, 0, 1)\n    df$Y[i] &lt;- (0.3 * df$Z[i]) + (0.4 * df$X[i]) + (0.4 * df$Y[i - 1]) + rnorm(1, 0, 1)\n  }\n  df\n}\n\n# Apply the dgp2() Function to the Empty Data Set\ndgp2_long &lt;- dgp2_panel_empty %&gt;% \n  group_by(id) %&gt;% \n  # Make Z Constant Across Time\n  mutate(Z = Z[1]) %&gt;%  # Propagate Z across all rows\n  # Nest the Data Into a Single Cell in Each Row\n  nest() %&gt;% \n  # Run dgp() on the Nested Cell\n  mutate(dgp = map(data, dgp2)) %&gt;% \n  select(-data) %&gt;% \n  # Unnest the Nested dgp()-ed Cells\n  unnest(dgp) %&gt;% \n  # Add Lags\n  mutate(across(c(X, Y), list(lag = lag))) %&gt;% \n  ungroup()\n\nhead(dgp2_long)\n\n# A tibble: 6 × 7\n     id  time     Z      X     Y  X_lag  Y_lag\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     1     1 0.161  0.850 0.773 NA     NA    \n2     1     2 0.161 -0.268 0.783  0.850  0.773\n3     1     3 0.161 -1.02  0.105 -0.268  0.783\n4     1     4 0.161  1.04  1.66  -1.02   0.105\n5     1     5 0.161  0.395 2.09   1.04   1.66 \n6     1     6 0.161  1.59  3.12   0.395  2.09 \n\n\nAnd that worked! Okay, but what if you’d instead be interested in simulating something like the other DGP we discussed without treatment-outcome feedback, but instead featuring a treatment-confounder feedback loop? I’ll also simulate a simpler two-period data set first and then expand it to 10 time periods. First, the two-period simulated data:\n\ndgp3_wide &lt;- tibble(id = 1:n) %&gt;% \n  # Create Lagged and Current Treatment and Outcome Values\n  mutate(Z1 = rnorm(n, 0, 1),\n         X1 = (0.5 * Z1) + rnorm(n, 0, 1),\n         Y1 = (0.3 * Z1) + (0.4 * X1) + rnorm(n, 0, 1),\n         Z2 = (0.6 * Z1) + (0.4 * X1) + rnorm(n, 0, 1),\n         X2 = (0.5 * Z2) + (0.4 * X1) + rnorm(n, 0, 1),\n         Y2 = (0.3 * Z2) + (0.4 * X2) + (0.4 * Y1) + rnorm(n, 0, 1))\n\ndgp3_long &lt;- dgp3_wide %&gt;%\n  # Pivot This Wider to Create a Time ID Column\n  pivot_longer(cols = c(X1, Y1, Z1, X2, Y2, Z2)) %&gt;% \n  separate(name, into = c(\"variable\", \"time\"), sep = 1) %&gt;% \n  pivot_wider(names_from = \"variable\", values_from = \"value\") %&gt;% \n  # Create Lagged Treatment and Outcome Columns\n  group_by(id) %&gt;% \n  mutate(across(c(X, Y, Z), list(lag = lag))) %&gt;% \n  ungroup()\n\nhead(dgp3_long)\n\n# A tibble: 6 × 8\n     id time       X       Y      Z  X_lag   Y_lag  Z_lag\n  &lt;int&gt; &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1     1 1     -0.110 -0.571  -0.276 NA     NA      NA    \n2     1 2     -0.956 -1.60   -0.677 -0.110 -0.571  -0.276\n3     2 1      2.37   0.0209  0.869 NA     NA      NA    \n4     2 2      2.80   2.01    1.93   2.37   0.0209  0.869\n5     3 1     -2.80  -1.99   -1.70  NA     NA      NA    \n6     3 2     -0.386 -1.28   -1.15  -2.80  -1.99   -1.70 \n\n\nAgain, this is simple enough and the only real change we had to make was specifying a dynamic time-varying effect for the confounder. Let’s see what the code looks like with 10 time-periods:\n\n# Create the Data in the First Time Period\ndgp4_first_year &lt;- expand_grid(id = 1:n, time = 1) %&gt;% \n  mutate(Z = rnorm(n, 0, 1), \n         X = (0.5 * Z) + rnorm(n, 0, 1),\n         Y = (0.3 * Z) + (0.4 * X) + rnorm(n, 0, 1))\n\n# Add Another 9 Empty Time Periods\ndgp4_panel_empty &lt;- dgp4_first_year %&gt;% \n  bind_rows(expand_grid(id = 1:n, time = 2:10)) %&gt;% \n  arrange(id, time)\n\n# Add Noise with a Custom dgp() Function\ndgp4 &lt;- function(df) {\n  for (i in 2:nrow(df)) {\n    df$Z[i] &lt;- (0.6 * df$Z[i - 1]) + (0.4 * df$X[i - 1]) + rnorm(1, 0, 1)\n    df$X[i] &lt;- (0.5 * df$Z[i]) + (0.4 * df$Y[i - 1]) + (0.4 * df$X[i - 1]) + rnorm(1, 0, 1)\n    df$Y[i] &lt;- (0.3 * df$Z[i]) + (0.4 * df$X[i]) + (0.4 * df$Y[i - 1]) + rnorm(1, 0, 1)\n  }\n  df\n}\n\n# Apply the dgp() Function to the Empty Data Set\ndgp4_long &lt;- dgp4_panel_empty %&gt;% \n  group_by(id) %&gt;% \n  # Nest the Data Into a Single Cell in Each Row\n  nest() %&gt;% \n  # Run dgp() on the Nested Cell\n  mutate(dgp = map(data, dgp4)) %&gt;% \n  select(-data) %&gt;% \n  # Unnest the Nested dgp()-ed Cells\n  unnest(dgp) %&gt;% \n  # Add Lags\n  mutate(across(c(X, Y, Z), list(lag = lag))) %&gt;% \n  ungroup()\n\nhead(dgp4_long)\n\n# A tibble: 6 × 8\n     id  time      Z      X      Y  X_lag  Y_lag  Z_lag\n  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     1     1 -0.228  0.237 -0.641 NA     NA     NA    \n2     1     2  0.319 -1.66   0.660  0.237 -0.641 -0.228\n3     1     3  0.180 -0.434  2.53  -1.66   0.660  0.319\n4     1     4 -1.66  -0.234  0.575 -0.434  2.53   0.180\n5     1     5 -0.451  0.845 -0.401 -0.234  0.575 -1.66 \n6     1     6  1.12  -0.388  0.707  0.845 -0.401 -0.451\n\n\nAgain, this is largely the same code as the code used to generate DGP 2, the only difference here is that we have to simulate the dynamic nature of the confounder in our dgp4() function. So, now that we’ve got this part of the puzzle figured out and we have four data sets where we know what the actual effects are, let’s test some models and see which generally perform the best at estimating the true effect.\n\n\nEstimating and Testing Panel Data Models\nFirst, we’ll start with the OG - standard regression adjustment, where we model a relationship between the outcome and treatment and adjust for confounding by including the control variables as covariates in the regression equation. Below, I estimate regressions for each DGP with some variation within-DGP. First, I estimate models that are confounded (they do not adjust for \\(Z\\)) in contrast to models that adjust for \\(Z\\). I do this to show that, even when you control for a confounder, that is not enough to get an unbiased estimate with time-varying treatments and confounders. Second, I estimate some models with and without a lagged effect specified. If the complications of panel data create a “damned if you do, damned if you don’t” situation, then perhaps the models only seeking to model the immediate effect of \\(X\\) on \\(Y\\) will perform better.\n\ndgp1_reg_confounded &lt;- lm(Y ~ X, data = dgp1_long)\ndgp1_reg_adjusted &lt;- lm(Y ~ X + Z, data = dgp1_long)\ndgp1_reg_confounded_lag &lt;- lm(Y ~ X + X_lag, data = dgp1_long)\ndgp1_reg_adjusted_lag &lt;- lm(Y ~ X + X_lag + Z, data = dgp1_long)\n\ndgp2_reg_confounded &lt;- lm(Y ~ X, data = dgp2_long)\ndgp2_reg_adjusted &lt;- lm(Y ~ X + Z, data = dgp2_long)\ndgp2_reg_confounded_lag &lt;- lm(Y ~ X + X_lag, data = dgp2_long)\ndgp2_reg_adjusted_lag &lt;- lm(Y ~ X + X_lag + Z, data = dgp2_long)\n\ndgp3_reg_confounded &lt;- lm(Y ~ X, data = dgp3_long)\ndgp3_reg_adjusted &lt;- lm(Y ~ X + Z, data = dgp3_long)\ndgp3_reg_confounded_lag &lt;- lm(Y ~ X + X_lag, data = dgp3_long)\ndgp3_reg_adjusted_lag &lt;- lm(Y ~ X + X_lag + Z, data = dgp3_long)\n\ndgp4_reg_confounded &lt;- lm(Y ~ X, data = dgp4_long)\ndgp4_reg_adjusted &lt;- lm(Y ~ X + Z, data = dgp4_long)\ndgp4_reg_confounded_lag &lt;- lm(Y ~ X + X_lag, data = dgp4_long)\ndgp4_reg_adjusted_lag &lt;- lm(Y ~ X + X_lag + Z, data = dgp4_long)\n\nNext, I specify a series of autoregressive distributed lag (ADL) models with both confounded and adjusted estimates. Basically, this model includes the use of a lagged dependent variable (LDV) as a covariate in the regression model. We probably should not put too much stock in the ADL results, especially for the estimation of lagged effects. Recall the DAGs created to model the DGPs discussed in this blog. \\(Y_{t-1}\\) (the lagged dependent variable) is on the causal pathway that connects \\(X_{t-1}\\) to \\(Y_{t}\\). By including a LDV in the model, we are removing part of the lagged effect of \\(X\\). So, in theory, these should not perform well at estimating lagged effects.\n\ndgp1_adl_confounded &lt;- lm(Y ~ X + X_lag + Y_lag, data = dgp1_long)\ndgp1_adl_adjusted &lt;- lm(Y ~ X + X_lag + Y_lag + Z, data = dgp1_long)\n\ndgp2_adl_confounded &lt;- lm(Y ~ X + X_lag + Y_lag, data = dgp2_long)\ndgp2_adl_adjusted &lt;- lm(Y ~ X + X_lag + Y_lag + Z, data = dgp2_long)\n\ndgp3_adl_confounded &lt;- lm(Y ~ X + X_lag + Y_lag, data = dgp3_long)\ndgp3_adl_adjusted &lt;- lm(Y ~ X + X_lag + Y_lag + Z, data = dgp3_long)\n\ndgp4_adl_confounded &lt;- lm(Y ~ X + X_lag + Y_lag, data = dgp4_long)\ndgp4_adl_adjusted &lt;- lm(Y ~ X + X_lag + Y_lag + Z, data = dgp4_long)\n\nLastly, I attempt to recover the current and lagged treatment effects by using something called (and take a deep breath because it’s a long name) a “marginal structural model with inverse probability weights”. We could (and will) simplify this by using acronyms like MSM with IPW or, more succinctly, IPW. This is for sure the more complicated process, but it should yield the best results. While this blog is not devoted towards explaining MSMs with IPW (that is the next blog I’m working on), I will briefly walk through the estimation of MSMs with IPW. Even if you don’t fully understand the theory of these models, their performance should represent their value in the panel data setting very well.\nFirst, we have to estimate the inverse probability weights… What are these? Basically, they are weights that we construct that represent the inverse of the estimated probability that a given unit at a given time will receive their treatment status. What is the point of that complicated sounding (but less complicated in practice) process? Well, the idea is that we use these weights to create a pseudo-population that, if all confounding factors are accounted for, represents the distribution of treatment if treatment was randomly allocated. We create this pseudo-population by weighting our data using the inverse probability weights.\nIn an observational setting, the probability of any given treatment value will not be identical for treated and non-treated units because units self-select or are selected into their treatment values non-randomly. You can see this in the top plot. However, when weighting units by the inverse probability of treatment, we can see what the distribution of treatment would have looked like if treatment was assigned at random (or, in other words, was assigned at random). This is great for making causal inferences because it mimics the randomization properties of a randomized controlled trial.\nBut how does any of this get around the “damned if you do, damned if you don’t” situation. You might be thinking that this is just another way to adjust for confounding. And while IPW does adjust for confounding (just as regression adjustment, matching, etc. also would), inverse probability weights can also be constructed so that we don’t have the “damned if you do, damned if you don’t” problem. Summarizing this way too simply, IPW is valuable because inverse probability weights are generated at each time point and are modeled based on each unit’s treatment history. Rather than brute-force adjustment of a confounder, IPW provides more nuance and tact and specifically accounts for temporal dynamics in it’s confounding adjustment strategy. There’s a lot more to this than what I just described, but that’s all I have room for in this blog post. Now, on to estimation.\nFirst, I’ll start by constructing the inverse probability weights using the {ipw} package. Before I can create weights, I have to remove NA observations since the {ipw} package does not like a data set with missingness.\n\n# First, Create Data Frames Without NAs for the {ipw} Package\ndgp1_wo_na &lt;- dgp1_long %&gt;%\n  filter(!is.na(X_lag)) %&gt;% \n  mutate(time = as.numeric(time))\n\ndgp2_wo_na &lt;- dgp2_long %&gt;%\n  filter(!is.na(X_lag)) %&gt;% \n  mutate(time = as.numeric(time))\n\ndgp3_wo_na &lt;- dgp3_long %&gt;%\n  filter(!is.na(X_lag)) %&gt;% \n  mutate(time = as.numeric(time))\n\ndgp4_wo_na &lt;- dgp4_long %&gt;%\n  filter(!is.na(X_lag)) %&gt;% \n  mutate(time = as.numeric(time))\n\n# Create Weights for DGP 1\npanel_weights_dgp1 &lt;- ipwtm(\n  exposure = X,\n  family = \"gaussian\", \n  numerator = ~ X_lag + Z, \n  denominator = ~ X_lag + Y_lag + Z, \n  id = id,\n  timevar = time,\n  type = \"all\",\n  corstr = \"ar1\",\n  data = as.data.frame(dgp1_wo_na))\n\n# Store Weights for DGP 1\npanel_weights_dgp1 &lt;- dgp1_wo_na %&gt;% \n  mutate(ipw = panel_weights_dgp1$ipw.weights)\n\nAnd, just like that, inverse probability weights have been generated for DGP 1. However, there’s more nuance to IPW than just that. A common practice in IPW is the process known as truncation. The basic idea is that, when we estimate treatment probabilities and take the inverse, we can end up with a select few crazy large or small weights that throw the results off. A common practice in this case is the process of truncation where we select an artifical cut-off (1% and 5% most commonly) and remove the most extreme IPWs from the sample. I also create IPWs with 1% and 5% truncation.\n\n# Create Weights for DGP 1 - 5% Truncation\npanel_weights_dgp1_95 &lt;- ipwtm(\n  exposure = X,\n  family = \"gaussian\", \n  numerator = ~ X_lag + Z, \n  denominator = ~ X_lag + Y_lag + Z, \n  id = id,\n  timevar = time,\n  type = \"all\",\n  corstr = \"ar1\",\n  trunc = 0.05,\n  data = as.data.frame(dgp1_wo_na))\n\n# Store Weights for DGP 1 - 5% Truncation\npanel_weights_dgp1_95 &lt;- dgp1_wo_na %&gt;% \n  mutate(ipw = panel_weights_dgp1_95$weights.trunc)\n\n# Create Weights for DGP 1 - 1% Truncation\npanel_weights_dgp1_99 &lt;- ipwtm(\n  exposure = X,\n  family = \"gaussian\", \n  numerator = ~ X_lag + Z, \n  denominator = ~ X_lag + Y_lag + Z, \n  id = id,\n  timevar = time,\n  type = \"all\",\n  corstr = \"ar1\",\n  trunc = 0.01,\n  data = as.data.frame(dgp1_wo_na))\n\n# Store Weights for DGP 1 - 1% Truncation\npanel_weights_dgp1_99 &lt;- dgp1_wo_na %&gt;% \n  mutate(ipw = panel_weights_dgp1_99$weights.trunc)\n\nAlright and now, I’ll do all of this all over again to get IPWs for DGPs 2-4. This code is hidden under a code fold because it’s loooong, but feel free to expand it if you want.\n\n\nCode\n# Create Weights for DGP 2\npanel_weights_dgp2 &lt;- ipwtm(\n  exposure = X,\n  family = \"gaussian\", \n  numerator = ~ X_lag + Z, \n  denominator = ~ X_lag + Y_lag + Z, \n  id = id,\n  timevar = time,\n  type = \"all\",\n  corstr = \"ar1\",\n  data = as.data.frame(dgp2_wo_na))\n\n# Store Weights for DGP 2\npanel_weights_dgp2 &lt;- dgp2_wo_na %&gt;% \n  mutate(ipw = panel_weights_dgp2$ipw.weights)\n\n# Create Weights for DGP 2 - 5% Truncation\npanel_weights_dgp2_95 &lt;- ipwtm(\n  exposure = X,\n  family = \"gaussian\", \n  numerator = ~ X_lag + Z, \n  denominator = ~ X_lag + Y_lag + Z, \n  id = id,\n  timevar = time,\n  type = \"all\",\n  corstr = \"ar1\",\n  trunc = 0.05,\n  data = as.data.frame(dgp2_wo_na))\n\n# Store Weights for DGP 2 - 5% Truncation\npanel_weights_dgp2_95 &lt;- dgp2_wo_na %&gt;% \n  mutate(ipw = panel_weights_dgp2_95$weights.trunc)\n\n# Create Weights for DGP 2 - 1% Truncation\npanel_weights_dgp2_99 &lt;- ipwtm(\n  exposure = X,\n  family = \"gaussian\", \n  numerator = ~ X_lag + Z, \n  denominator = ~ X_lag + Y_lag + Z, \n  id = id,\n  timevar = time,\n  type = \"all\",\n  corstr = \"ar1\",\n  trunc = 0.01,\n  data = as.data.frame(dgp2_wo_na))\n\n# Store Weights for DGP 2 - 1% Truncation\npanel_weights_dgp2_99 &lt;- dgp2_wo_na %&gt;% \n  mutate(ipw = panel_weights_dgp2_99$weights.trunc)\n\n# Create Weights for DGP 3\npanel_weights_dgp3 &lt;- ipwtm(\n  exposure = X,\n  family = \"gaussian\", \n  numerator = ~ X_lag + Z, \n  denominator = ~ X_lag + Y_lag + Z, \n  id = id,\n  timevar = time,\n  type = \"all\",\n  corstr = \"ar1\",\n  data = as.data.frame(dgp3_wo_na))\n\n# Store Weights for DGP 3\npanel_weights_dgp3 &lt;- dgp3_wo_na %&gt;% \n  mutate(ipw = panel_weights_dgp3$ipw.weights)\n\n# Create Weights for DGP 3 - 5% Truncation\npanel_weights_dgp3_95 &lt;- ipwtm(\n  exposure = X,\n  family = \"gaussian\", \n  numerator = ~ X_lag + Z, \n  denominator = ~ X_lag + Y_lag + Z, \n  id = id,\n  timevar = time,\n  type = \"all\",\n  corstr = \"ar1\",\n  trunc = 0.05,\n  data = as.data.frame(dgp3_wo_na))\n\n# Store Weights for DGP 3 - 5% Truncation\npanel_weights_dgp3_95 &lt;- dgp3_wo_na %&gt;% \n  mutate(ipw = panel_weights_dgp3_95$weights.trunc)\n\n# Create Weights for DGP 3 - 1% Truncation\npanel_weights_dgp3_99 &lt;- ipwtm(\n  exposure = X,\n  family = \"gaussian\", \n  numerator = ~ X_lag + Z, \n  denominator = ~ X_lag + Y_lag + Z, \n  id = id,\n  timevar = time,\n  type = \"all\",\n  corstr = \"ar1\",\n  trunc = 0.01,\n  data = as.data.frame(dgp3_wo_na))\n\n# Store Weights for DGP 3 - 1% Truncation\npanel_weights_dgp3_99 &lt;- dgp3_wo_na %&gt;% \n  mutate(ipw = panel_weights_dgp3_99$weights.trunc)\n\n# Create Weights for DGP 4\npanel_weights_dgp4 &lt;- ipwtm(\n  exposure = X,\n  family = \"gaussian\", \n  numerator = ~ X_lag + Z, \n  denominator = ~ X_lag + Y_lag + Z, \n  id = id,\n  timevar = time,\n  type = \"all\",\n  corstr = \"ar1\",\n  data = as.data.frame(dgp4_wo_na))\n\n# Store Weights for DGP 4\npanel_weights_dgp4 &lt;- dgp4_wo_na %&gt;% \n  mutate(ipw = panel_weights_dgp4$ipw.weights)\n\n# Create Weights for DGP 4 - 5% Truncation\npanel_weights_dgp4_95 &lt;- ipwtm(\n  exposure = X,\n  family = \"gaussian\", \n  numerator = ~ X_lag + Z, \n  denominator = ~ X_lag + Y_lag + Z, \n  id = id,\n  timevar = time,\n  type = \"all\",\n  corstr = \"ar1\",\n  trunc = 0.05,\n  data = as.data.frame(dgp4_wo_na))\n\n# Store Weights for DGP 4 - 5% Truncation\npanel_weights_dgp4_95 &lt;- dgp4_wo_na %&gt;% \n  mutate(ipw = panel_weights_dgp4_95$weights.trunc)\n\n# Create Weights for DGP 4 - 1% Truncation\npanel_weights_dgp4_99 &lt;- ipwtm(\n  exposure = X,\n  family = \"gaussian\", \n  numerator = ~ X_lag + Z, \n  denominator = ~ X_lag + Y_lag + Z, \n  id = id,\n  timevar = time,\n  type = \"all\",\n  corstr = \"ar1\",\n  trunc = 0.01,\n  data = as.data.frame(dgp4_wo_na))\n\n# Store Weights for DGP 4 - 1% Truncation\npanel_weights_dgp4_99 &lt;- dgp4_wo_na %&gt;% \n  mutate(ipw = panel_weights_dgp4_99$weights.trunc)\n\n\nNow that we have all of the IPWs, we actually have to apply them to a marginal structural model (MSM). Don’t worry, this part is easy. It’s literally as simple as just running a regression (like normal) but including a weights argument and supplying that argument with the IPWs:\n\ndgp1_ipw_notrunc &lt;- lm(Y ~ X + X_lag + Z, data = panel_weights_dgp1, weights = ipw)\ndgp1_ipw_95trunc &lt;- lm(Y ~ X + X_lag + Z, data = panel_weights_dgp1_95, weights = ipw)\ndgp1_ipw_99trunc &lt;- lm(Y ~ X + X_lag + Z, data = panel_weights_dgp1_99, weights = ipw)\n\ndgp2_ipw_notrunc &lt;- lm(Y ~ X + X_lag + Z, data = panel_weights_dgp2, weights = ipw)\ndgp2_ipw_95trunc &lt;- lm(Y ~ X + X_lag + Z, data = panel_weights_dgp2_95, weights = ipw)\ndgp2_ipw_99trunc &lt;- lm(Y ~ X + X_lag + Z, data = panel_weights_dgp2_99, weights = ipw)\n\ndgp3_ipw_notrunc &lt;- lm(Y ~ X + X_lag + Z, data = panel_weights_dgp3, weights = ipw)\ndgp3_ipw_95trunc &lt;- lm(Y ~ X + X_lag + Z, data = panel_weights_dgp3_95, weights = ipw)\ndgp3_ipw_99trunc &lt;- lm(Y ~ X + X_lag + Z, data = panel_weights_dgp3_99, weights = ipw)\n\ndgp4_ipw_notrunc &lt;- lm(Y ~ X + X_lag + Z, data = panel_weights_dgp4, weights = ipw)\ndgp4_ipw_95trunc &lt;- lm(Y ~ X + X_lag + Z, data = panel_weights_dgp4_95, weights = ipw)\ndgp4_ipw_99trunc &lt;- lm(Y ~ X + X_lag + Z, data = panel_weights_dgp4_99, weights = ipw)\n\nOkay, we are almost there. We are almost to the point where we can compare how well each of these dozens of models performed. Before I visually represent this, I am going to store all of these results in a single data frame:\n\n# Store Results in a Data Frame\nresults_list &lt;- list()\n\n# Create a Function to Store Results\nstore_results &lt;- function(dgp, method, model, trunc, confounded) {\n  # Store X Estimate\n  x_est &lt;- coef(model)[2] \n  # Store Lagged X Estimate\n  x_lag_est &lt;- if (\"X_lag\" %in% names(coef(model))) coef(model)[\"X_lag\"] else NA\n  # Store and Separate Confidence Intervals for Estimates\n  ci &lt;- confint(model)\n  ci_lower &lt;- ci[2, 1]  \n  ci_upper &lt;- ci[2, 2]\n  \n  # Store Confidence Intervals for X Lag Estimate\n  # Set Default to NA Since Some Models Don't Estimate Lagged Effects\n  ci_x_lag_lower &lt;- NA\n  ci_x_lag_upper &lt;- NA\n  if (!is.na(x_lag_est)) {\n    ci_x_lag &lt;- confint(model)[\"X_lag\", ] \n    ci_x_lag_lower &lt;- ci_x_lag[1]  \n    ci_x_lag_upper &lt;- ci_x_lag[2]\n  }\n  \n  # Create a New Data Frame to Store Results\n  new_result &lt;- data.frame(\n    dgp = as.factor(dgp),\n    method = method,\n    x_estimate = x_est,\n    # Define the True Effect of X\n    true_x = 0.4,\n    x_lag_estimate = x_lag_est,\n    # Define the True Effects of X Lagged (This Is Different for DGPs 1&3 vs. DGPs 2&4)\n    true_x_lag = ifelse(dgp %in% c(1, 3), 0.16, 0.096),\n    ci_lower = ci_lower,\n    ci_upper = ci_upper,\n    ci_lower_x_lag = ci_x_lag_lower,  \n    ci_upper_x_lag = ci_x_lag_upper,  \n    confounded = as.factor(confounded),\n    truncation = trunc\n  )\n  \n  # Update Results List with Each New Model Run\n  results_list &lt;&lt;- append(results_list, list(new_result))\n}\n\n# Go Through Each Model and Store Result\nstore_results(1, \"Regression Adjustment\", dgp1_reg_adjusted, NA, 0)\nstore_results(2, \"Regression Adjustment\", dgp2_reg_adjusted, NA, 0)\nstore_results(3, \"Regression Adjustment\", dgp3_reg_adjusted, NA, 0)\nstore_results(4, \"Regression Adjustment\", dgp4_reg_adjusted, NA, 0)\n\nstore_results(1, \"Regression Adjustment\", dgp1_reg_confounded, NA, 1)\nstore_results(2, \"Regression Adjustment\", dgp2_reg_confounded, NA, 1)\nstore_results(3, \"Regression Adjustment\", dgp3_reg_confounded, NA, 1)\nstore_results(4, \"Regression Adjustment\", dgp4_reg_confounded, NA, 1)\n\nstore_results(1, \"Regression Adjustment\", dgp1_reg_adjusted_lag, NA, 0)\nstore_results(2, \"Regression Adjustment\", dgp2_reg_adjusted_lag, NA, 0)\nstore_results(3, \"Regression Adjustment\", dgp3_reg_adjusted_lag, NA, 0)\nstore_results(4, \"Regression Adjustment\", dgp4_reg_adjusted_lag, NA, 0)\n\nstore_results(1, \"Regression Adjustment\", dgp1_reg_confounded_lag, NA, 1)\nstore_results(2, \"Regression Adjustment\", dgp2_reg_confounded_lag, NA, 1)\nstore_results(3, \"Regression Adjustment\", dgp3_reg_confounded_lag, NA, 1)\nstore_results(4, \"Regression Adjustment\", dgp4_reg_confounded_lag, NA, 1)\n\nstore_results(1, \"ADL\", dgp1_adl_adjusted, NA, 0)\nstore_results(2, \"ADL\", dgp2_adl_adjusted, NA, 0)\nstore_results(3, \"ADL\", dgp3_adl_adjusted, NA, 0)\nstore_results(4, \"ADL\", dgp4_adl_adjusted, NA, 0)\n\nstore_results(1, \"ADL\", dgp1_adl_confounded, NA, 1)\nstore_results(2, \"ADL\", dgp2_adl_confounded, NA, 1)\nstore_results(3, \"ADL\", dgp3_adl_confounded, NA, 1)\nstore_results(4, \"ADL\", dgp4_adl_confounded, NA, 1)\n\nstore_results(1, \"IPW\", dgp1_ipw_notrunc, NA, 0)\nstore_results(2, \"IPW\", dgp2_ipw_notrunc, NA, 0)\nstore_results(3, \"IPW\", dgp3_ipw_notrunc, NA, 0)\nstore_results(4, \"IPW\", dgp4_ipw_notrunc, NA, 0)\n\nstore_results(1, \"IPW\", dgp1_ipw_95trunc, 0.05, 0)\nstore_results(2, \"IPW\", dgp2_ipw_95trunc, 0.05, 0)\nstore_results(3, \"IPW\", dgp3_ipw_95trunc, 0.05, 0)\nstore_results(4, \"IPW\", dgp4_ipw_95trunc, 0.05, 0)\n\nstore_results(1, \"IPW\", dgp1_ipw_99trunc, 0.01, 0)\nstore_results(2, \"IPW\", dgp2_ipw_99trunc, 0.01, 0)\nstore_results(3, \"IPW\", dgp3_ipw_99trunc, 0.01, 0)\nstore_results(4, \"IPW\", dgp4_ipw_99trunc, 0.01, 0)\n\n# Combine All Results\nall_results &lt;- do.call(rbind, results_list)\n\nAnd now, we can visualize how well these models performed. Note that I am creating two plots - one for each DGP with the first of these plots showing the performance of models for DGPs 1-2 with treatment-outcome feedback and 1 time-invariant confounder and another for DGPs 3-4 with treatment-confounder feedback.\n\n# Visualize Performance of Each Model\n# First Do Some Data Pre-Processing\nall_results_long &lt;- all_results %&gt;%\n  # Pivot This Data Set So That Estimate Type is a Column\n  pivot_longer(cols = c(x_estimate, x_lag_estimate), \n               names_to = \"estimate_type\", \n               values_to = \"estimate_value\") %&gt;%\n  mutate(\n    # If the Estimate Type is 'X_Lag_Estimate' and is NA, Exclude It\n    estimate_type = ifelse(is.na(estimate_value) & estimate_type == \"x_lag_estimate\", NA, estimate_type),\n    # Create a String of Text That IDs What the Model Is\n    model_type = paste0(\n      \"DGP \", dgp,\n      ifelse(!is.na(truncation), \n             paste0(\": \",\n                    ifelse(truncation == 0.05, \"5% Truncated\", \n                           ifelse(truncation == 0.01, \"1% Truncated\", \"No Truncation\"))), \n             \"\")\n    )\n  ) %&gt;%\n  # Remove Rows Where Estimate_Type is NA\n  drop_na(estimate_type)\n\n# Plot Results for DGP 1 and 2\nall_results_long %&gt;%\n  filter(dgp %in% c(1, 2)) %&gt;%\n  # Rename Regression Adjustment for Legend Plotting\n  mutate(method = ifelse(method == \"Regression Adjustment\", \"RA\", method),\n         # Re-Order for Plotting\n         method = factor(method, levels = c(\"RA\", \"ADL\", \"IPW\")),\n         # Re-Name for Plotting\n         confounded = ifelse(confounded == 1, \"Yes\", \"No\")) %&gt;%\n  # Order by Method and DGP So That Estimate from the Same DGP Are Plotted Together\n  arrange(method, dgp) %&gt;%\n  mutate(id = row_number()) %&gt;%\n  ggplot(aes(x = estimate_value, y = id, color = method, shape = factor(confounded))) +\n  # Plot True Effect Size Vertical Lines\n  geom_vline(data = all_results_long %&gt;% filter(estimate_type == \"x_estimate\"),\n             aes(xintercept = 0.4), linetype = \"dashed\", color = \"#a91e11\", size = 0.75) +\n  geom_vline(data = all_results_long %&gt;% filter(estimate_type == \"x_lag_estimate\"),\n             aes(xintercept = 0.16), linetype = \"dashed\", color = \"#a91e11\", size = 0.75) +\n  # Insert geom_point() After So That It Is Pushed Forward In Front of the Vertical Lines\n  geom_point(size = 2.5) +\n  geom_errorbar(aes(xmin = ifelse(estimate_type == \"x_estimate\", ci_lower, ci_lower_x_lag), \n                    xmax = ifelse(estimate_type == \"x_estimate\", ci_upper, ci_upper_x_lag)), \n                width = 0.75, size = 0.75) +\n  # Facet by X_Estimate or X_Lag_Estimate\n  facet_wrap(~estimate_type, scales = \"free_x\", labeller = labeller(\n    estimate_type = c(\n      x_estimate = \"Estimate of X at Time t\", \n      x_lag_estimate = \"Lagged Estimate of X at Time t-1\" \n    )\n  )) +\n  # Add a Label That Defines the Model Type\n  geom_label(aes(label = model_type), vjust = -0.55, hjust = 0.5, size = 3, \n             fill = \"white\", fontface = \"bold\", label.size = 0.25, \n             label.padding = unit(0.2, \"lines\"), show.legend = FALSE) + \n  labs(\n    title = \"\",\n    subtitle = \"Vertical dashed lines reflect the true effect size\",\n    x = \"\",\n    y = \"\"\n  ) +\n  blog_theme() +\n  theme(\n    legend.position = \"bottom\",\n    axis.text.y = element_blank(),  \n    axis.ticks.y = element_blank(),  \n    legend.text = element_text(face = \"bold\"),\n    plot.subtitle = element_text(face = \"bold\"),\n  ) +\n  scale_color_manual(\n    name = \"Adjustment Method\",\n    values = c(\n      \"RA\" = \"#003f5a\", \n      \"ADL\" = \"#fea02f\",                   \n      \"IPW\" = \"#007a7a\"                   \n    ),\n    guide = guide_legend(\n      title.position = \"top\",  \n      title.hjust = 0.5, \n      label.position = \"bottom\",  \n      override.aes = list(linetype = 0)\n    )\n  ) + \nscale_shape_manual(\n  name = \"Is Estimate Confounded?\",\n    values = c(\n      \"No\" = 16, \n      \"Yes\" = 17   \n    ),\n  guide = guide_legend(\n    title.position = \"top\",  \n    title.hjust = 0.5,\n    label.position = \"bottom\",   \n    override.aes = list(linetype = 0)\n  )) +  \n  # Reverse Y Scale So That ID Is Plotted in Descending Order\n  scale_y_reverse() \n\n\n\n\n\n\n\n\nOkay, neat! First, let’s break down what this plot is showing us and then we’ll substantively interpret it. The left-hand panel shows us \\(X_{t} \\rightarrow Y_{t}\\) effect estimates while the right-hand panel shows us \\(X_{t-1} \\rightarrow Y_{t}\\) effect estimates. Point estimates are colored by estimation technique. Dark blue reflects regression adjustment, yellow is ADL, and teal is IPW. Dots are estimates adjusted for confounding while triangles are estimates where the estimate is known to be confounded. On that note as well, please don’t pay too much attention to the confidence intervals! This exercise was more about point estimation and not about hypothesis testing so my standard errors are for sure wrong. Nonetheless, we actually know the true effect size, so just pay attention to the points/triangles. Lastly, the dashed vertical line is the true effect size.\nWe know that the true effect size for \\(X\\) at time \\(t\\) is 0.4 because I directly specified that when simulating data. How did I come up with 0.16 for the lagged effect size for the effect of \\(X_{t-1}\\)}? To do this, I need to identify all the paths that \\(X_{t-1}\\) impacts \\(Y_{t}\\). For DGPs 1 and 2, we have two paths:\n\\[\nX_{t-1} \\rightarrow Y_{t-1} \\rightarrow Y_{t}\n\\]\n\\[\nX_{t-1} \\rightarrow X_{t} \\rightarrow Y_{t}\n\\] We can flesh this out by attaching our known effect sizes at each point:\n\\[\nX_{t-1} (0.4) \\rightarrow Y_{t-1} (0.4) \\rightarrow Y_{t}\n\\]\n\\[\nX_{t-1} (0.4) \\rightarrow X_{t} (0.4) \\rightarrow Y_{t}\n\\] For each of these paths, we take the product of the effect sizes. In other words: \\((0.4 * 0.4) + (0.4 * 0.4)\\) which equals 0.32. So why is the effect size 0.16? I’m not sure! I’m trusting the published research of Thoemmes and Ong (2016), whose research is what DGPs 1-2 are based on. If we are interested in the effect of \\(X_{t-1}\\) through one path then 0.16 makes sense to me. (For example, if we cared about the effect that goes through \\(X_{t-1} (0.4) \\rightarrow X_{t} (0.4) \\rightarrow Y_{t}\\) only). If this was the case, then \\(0.4 * 0.4 = 0.16\\). That makes sense to me, but I don’t understand why the total estimate would not include the effect of \\(X_{t-1}\\) that goes through \\(X_{t-1} (0.4) \\rightarrow Y_{t-1} (0.4) \\rightarrow Y_{t}\\). Well, that’s something for future Brian to figure out, but let’s trust the experts and not the novice who is trying to figure this out!\nSo, what do we find? For the estimation of current effects, regression adjustment (by far the most popular strategy with both panel and cross-sectional data) performs very poorly! In fact, with more data (DGP 2), the estimates trend towards getting worse! Whether or not the inclusion of a lagged \\(X\\) value as a covariate makes the estimate better or worse is a bit unclear. For DGP 1, estimates without the lagged \\(X\\) are less biased but, for DGP 2, estimates without the lagged \\(X\\) are much more biased.\nThe ADL and IPW results perform pretty well for the estimation of the current effect of \\(X\\). As expected, however, the ADL does really bad for lagged effect estimation. Which is exactly what we expected since including a LDV as a covariate blocks a huge part of the lagged treatment effect. Lastly, IPW does pretty good here, although it is interesting that the inclusion of more data (DGP 2) seems to make the estimates more biased. Not sure how to explain that… and I won’t try given that I myself am still a bit unsure on what the total lagged effect is in this circumstance. Next, let’s evaluate model performance for DGPs 3 and 4:\n\n# Plot Results for DGP 3 and 4\nall_results_long %&gt;%\n  filter(dgp %in% c(3, 4)) %&gt;%\n  # Rename Regression Adjustment for Legend Plotting\n  mutate(method = ifelse(method == \"Regression Adjustment\", \"RA\", method),\n         # Re-Order for Plotting\n         method = factor(method, levels = c(\"RA\", \"ADL\", \"IPW\")),\n         # Re-Name for Plotting\n         confounded = ifelse(confounded == 1, \"Yes\", \"No\")) %&gt;%\n  # Order by Method and DGP So That Estimate from the Same DGP Are Plotted Together\n  arrange(method, dgp) %&gt;%\n  mutate(id = row_number()) %&gt;%\n  ggplot(aes(x = estimate_value, y = id, color = method, shape = factor(confounded))) +\n  # Plot True Effect Size Vertical Lines\n  geom_vline(data = all_results_long %&gt;% filter(estimate_type == \"x_estimate\"),\n             aes(xintercept = 0.4), linetype = \"dashed\", color = \"#a91e11\", size = 0.75) +\n  geom_vline(data = all_results_long %&gt;% filter(estimate_type == \"x_lag_estimate\"),\n             aes(xintercept = 0.256), linetype = \"dashed\", color = \"#a91e11\", size = 0.75) +\n  # Insert geom_point() After So That It Is Pushed Forward In Front of the Vertical Lines\n  geom_point(size = 2.5) +\n  geom_errorbar(aes(xmin = ifelse(estimate_type == \"x_estimate\", ci_lower, ci_lower_x_lag), \n                    xmax = ifelse(estimate_type == \"x_estimate\", ci_upper, ci_upper_x_lag)), \n                width = 0.75, size = 0.75) +\n  # Facet by X_Estimate or X_Lag_Estimate\n  facet_wrap(~estimate_type, scales = \"free_x\", labeller = labeller(\n    estimate_type = c(\n      x_estimate = \"Estimate of X at Time t\", \n      x_lag_estimate = \"Lagged Estimate of X at Time t-1\" \n    )\n  )) +\n  # Add a Label That Defines the Model Type\n  geom_label(aes(label = model_type), vjust = -0.55, hjust = 0.5, size = 3, \n             fill = \"white\", fontface = \"bold\", label.size = 0.25, \n             label.padding = unit(0.2, \"lines\"), show.legend = FALSE) + \n  labs(\n    title = \"\",\n    subtitle = \"Vertical dashed lines reflect the true effect size\",\n    x = \"\",\n    y = \"\"\n  ) +\n  blog_theme() +\n  theme(\n    legend.position = \"bottom\",\n    axis.text.y = element_blank(),  \n    axis.ticks.y = element_blank(),  \n    legend.text = element_text(face = \"bold\"),\n    plot.subtitle = element_text(face = \"bold\"),\n  ) +\n  scale_color_manual(\n    name = \"Adjustment Method\",\n    values = c(\n      \"RA\" = \"#003f5a\", \n      \"ADL\" = \"#fea02f\",                   \n      \"IPW\" = \"#007a7a\"                   \n    ),\n    guide = guide_legend(\n      title.position = \"top\",  \n      title.hjust = 0.5, \n      label.position = \"bottom\",  \n      override.aes = list(linetype = 0)\n    )\n  ) + \n  scale_shape_manual(\n    name = \"Is Estimate Confounded?\",\n    values = c(\n      \"No\" = 16, \n      \"Yes\" = 17   \n    ),\n    guide = guide_legend(\n      title.position = \"top\",  \n      title.hjust = 0.5,\n      label.position = \"bottom\",   \n      override.aes = list(linetype = 0)\n    )) +  \n  # Reverse Y Scale So That ID Is Plotted in Descending Order\n  scale_y_reverse()  \n\n\n\n\n\n\n\n\nHere, we see a perhaps better case for IPW than more traditional models. The RA estimates generally perform very poorly for estimating the current effect of \\(X\\). In contrast, as expected, both the ADL and IPW estimates perform well, but can the same be said for the estimation of the lagged effect?\nBefore evaluating, let me explain why the lagged treatment effect is 0.256. Again, to be able to identify the total lagged effect, we need to figure out all of the paths that link \\(X_{t-1}\\) to \\(Y_{t}\\). If you reference the DAG earlier in this blog, you should be able to identify three paths:\n\\[\nX_{t-1} (0.4) \\rightarrow Z_{t} (0.2) \\rightarrow Y_{t}\n\\]\n\\[\nX_{t-1} (0.4) \\rightarrow X_{t} (0.4) \\rightarrow Y_{t}\n\\]\n\\[\nX_{t-1} (0.4) \\rightarrow Z_{t} (0.1) \\rightarrow X_{t} (0.4) \\rightarrow Y_{t}\n\\] Within each path, we take the product, so:\n\\[\nX_{t-1} (0.4) * Z_{t} (0.2) = 0.08\n\\] \\[\nX_{t-1} (0.4) * X_{t} (0.4) = 0.16\n\\] \\[\nX_{t-1} (0.4) * Z_{t} (0.1) * X_{t} (0.4) = 0.016\n\\] If we do \\(0.08 + 0.16 + 0.016\\), we get 0.256, which is where I’m getting the lagged effect of \\(X\\) from. With that explained, how do the models perform with estimating the lagged effect? Surprisingly, RA isn’t terrible and it’s actually slightly better when it’s confounded. This actually makes sense for the lagged effect. Remember, part of the effect of the lagged effect goes through \\(Z_{t}\\). If we control for that confounder, we remove part of the lagged effect. If we don’t control for this confounder, we do not block that part of the lagged effect. Although, we obviously bias the current effect in the process. As expected, the ADL does not do well here. Lastly, IPW looks really solid here, for both the current and lagged effect.\n\n\nConclusion\nOkay, so that was a lot, but I really wanted to drive two key points home. First, panel data is complicated and when we use models to estimate dynamic effects, knowing how to simulate complex panel data is a very valuable tool to know. Second, regression adjustment probably won’t cut it! If you know or suspect time-varying confounding, treatment-outcome feedback loops, etc., you also know beforehand that your statistical estimates could be very, very wrong. Marginal structural models with inverse probability weights can be one solution to the problem posed by complex panel data (although it is not the only solution). However, MSMs with IPW are pretty popular and are probably increasing in popularity. As a result, I plan on making a blog post just dedicated to this methodology, exploring all of the ins-and-outs and simulating data a bit more similar to real-life panel data (i.e. larger \\(T\\), a binary treatment, a lot of diverse confounders, etc.) Thanks for reading!"
  },
  {
    "objectID": "blog/2025/chiefs-refs-bias/index.html",
    "href": "blog/2025/chiefs-refs-bias/index.html",
    "title": "Do NFL Referees Favor the Kansas City Chiefs?",
    "section": "",
    "text": "Code\n# Load Libraries\npacman::p_load(\n  \"dplyr\", # Data Manipulation\n  \"tidyr\", # More Data Manipulation\n  \"ggplot2\", # Data Visualization\n  \"nflverse\", # NFL Data and Visualization\n  \"ggimage\", # More Data Visualization\n  install = FALSE\n)\n\n# Define a Custom Theme - Taken From Andrew Heiss's Blogs\nblog_theme &lt;- function() {\n  theme_bw() +  # Start with theme_bw\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      plot.title = element_text(face = \"bold\"),\n      axis.title = element_text(face = \"bold\"),\n      strip.text = element_text(face = \"bold\"),\n      strip.background = element_rect(fill = \"grey80\", color = NA),\n      legend.title = element_text(face = \"bold\")\n    )\n}\n\n# Clear Cache for NFL Data\nnflreadr::.clear_cache()\n\n\nThe Kansas City Chiefs are heading to their 5th Super Bowl in the past 6 years. This upcoming Super Bowl appearance against the Philadelphia Eagles is a re-match from two years ago, where the Chiefs emerged victorious. If the Chiefs can beat the Eagles again, they will go down in history as the first NFL team to win three consecutive Super Bowls (a “three-peat”).\nDespite this unprecedented greatness, many fans, players, and coaches alike are skeptical. It seems that, week-after-week, a referee call just happens to go in the favor of Kansas City, and sometimes, these calls seem to impact the outcome of the game. This has lead to many fans and members of the NFL to question the integrity of the game.\nIn fairness, I don’t think that anyone is claiming that the Chiefs are cheating. Instead, the claim is that, for some reason, the NFL wants the Chiefs to do well, so they decide to give Kansas City small nudges here and there that go their way and inflate their probability of success. Primarily, skeptics claim the NFL does this by weaponizing the referee system. “It seems like the Chiefs get all of the calls” or “the calls just all happen to go KC’s way” or “the refs don’t call clear penalties against KC like they do towards other teams”. Is any of this true? Or have your eyes deceived you? I think so! And, if you don’t believe me, let’s dig into the data!\nIn this review, we are going to look at four major claims that people make and we’re going to see if the data supports such claims. First, outright, do the refs overall favor the Chiefs above any other team? Second, do the refs help out the Chiefs in “tight spots” when it really matters (3rd downs, 4th downs, overtime, etc.). Third, do the refs go out of their way to protect Patrick Mahomes in particular? And, lastly, does the NFL treat the Chiefs differently because of Taylor Swift and her relationship with Travis Kelce?\n\nClaim 1: Do the Refs Help Out the Chiefs?\nLet’s start looking into this claim by evaluating penalty yard differentials throughout Mahomes’s tenure as KC’s starting quarterback. Why penalty yard differential (PYD)? Here is the logic: while some calls may go your way in a given game, some calls will go the other way. What we care about is this: which team got helped out by the refs more? PYD is a way to evaluate that. If a team has a positive PYD, that means they got more yards awarded to them by the refs than the other team. If there is a negative PYD, the other team got more yards awarded to them from the refs. The larger the PYD, the greater the discrepancy.\nNow, in a single game, you might either get unlucky, or your team was just poorly coached in that game and truly deserved all the penalties. That’s why we’re going to take a look overall, aggregating each game every team played from 2018-2024.\n\n\nCode\n# Load Play-by-Play (PBP) Data\npbp_data_2018_2024 &lt;- load_pbp(seasons = 2018:2024)\n\n# Calculate Penalty Yard Differential by Team\npenalty_differential_2018_2024 &lt;- pbp_data_2018_2024 %&gt;%\n  # Only Keep Penalty Plays\n  filter(penalty == 1 & !is.na(penalty_yards)) %&gt;%\n  # Create a Long Form Version with a \"Team\" Column and a Possession/Defense Dummy\n  gather(key = \"side\", value = \"team\", posteam, defteam) %&gt;%\n  mutate(side_dummy = ifelse(side == \"posteam\", \"offense\", \"defense\")) %&gt;%\n  # Modify Penalty Yards So That If the Team Committed the Penalty, It is Negative\n  # For Them and Positive for Their Opponent\n  mutate(\n    penalty_yards_adjusted = case_when(\n      penalty_team == team ~ -penalty_yards, \n      penalty_team != team ~ penalty_yards, \n      TRUE ~ 0\n    )\n  ) %&gt;%\n  # Collapse to the Game Level\n  group_by(game_id, team, season) %&gt;%\n  summarize(\n    penalty_yard_differential = sum(penalty_yards_adjusted, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  # Collapse to the Team Level\n  group_by(team) %&gt;%\n  summarize(\n    total_penalty_yard_differential = sum(penalty_yard_differential, na.rm = TRUE),\n    .groups = 'drop'\n  )\n\n# Add Team Logos for Plotting\nteam_logos &lt;- nflfastR::teams_colors_logos %&gt;%\n  select(team_abbr, team_logo_espn)\n\n# Merge Penalty Data with Logos\npenalty_differential_2018_2024 &lt;- penalty_differential_2018_2024 %&gt;%\n  left_join(team_logos, by = c(\"team\" = \"team_abbr\"))\n\n# Create a Diverging Bar Chart\nggplot(penalty_differential_2018_2024, aes(x = reorder(team, total_penalty_yard_differential), y = total_penalty_yard_differential)) +\n  geom_bar(stat = \"identity\", aes(fill = total_penalty_yard_differential)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", \n             color = \"black\", linewidth = 1.2) +\n  geom_image(aes(image = team_logo_espn), size = 0.05) +\n  scale_fill_gradient2(\n    low = \"#8b0100\",  \n    high = \"#0f3860\",\n    midpoint = 0\n  ) +\n  labs(\n    x = \"\",\n    y = \"Penalty Yard Differential\"\n  ) +\n  blog_theme() + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")\n\n\n\n\n\nPenalty Yard Differential by Team (2018-2024)\n\n\n\n\nNow, that’s interesting! Not only is KC nowhere near the top of PYD over the past 7 years, they are near the very bottom! Okay, but you might think “back then, the Chiefs weren’t the favorites in the NFL, but now they are. And, I’ve watched their games all year this season and clearly see them getting bailed out by the refs.” Fair enough… so let’s look at PYD for just the 2024 NFL season.\n\n\nCode\n# Load Play-by-Play (PBP) Data\npbp_data_2024 &lt;- load_pbp(seasons = 2024)\n\n# Calculate Penalty Yard Differential by Team\npenalty_differential_2024 &lt;- pbp_data_2024 %&gt;%\n  # Only Keep Penalty Plays\n  filter(penalty == 1 & !is.na(penalty_yards)) %&gt;%\n  # Create a Long Form Version with a \"Team\" Column and a Possession/Defense Dummy\n  gather(key = \"side\", value = \"team\", posteam, defteam) %&gt;%\n  mutate(side_dummy = ifelse(side == \"posteam\", \"offense\", \"defense\")) %&gt;%\n  # Modify Penalty Yards So That If the Team Comitted the Penalty, It is Negative\n  # For Them and Positive for Their Opponent\n  mutate(\n    penalty_yards_adjusted = case_when(\n      penalty_team == team ~ -penalty_yards, \n      penalty_team != team ~ penalty_yards, \n      TRUE ~ 0\n    )\n  ) %&gt;%\n  # Collapse to the Game Level\n  group_by(game_id, team, season) %&gt;%\n  summarize(\n    penalty_yard_differential = sum(penalty_yards_adjusted, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  # Collapse to the Season Level\n  group_by(team) %&gt;%\n  summarize(\n    total_penalty_yard_differential = sum(penalty_yard_differential, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  left_join(team_logos, by = c(\"team\" = \"team_abbr\"))\n\nggplot(penalty_differential_2024, aes(x = reorder(team, total_penalty_yard_differential), \n             y = total_penalty_yard_differential)) +\n  geom_bar(stat = \"identity\", aes(fill = total_penalty_yard_differential)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", \n             color = \"black\", linewidth = 1.2) +\n  geom_image(aes(image = team_logo_espn), size = 0.05) +\n  scale_fill_gradient2(\n    low = \"#8b0100\",  \n    high = \"#0f3860\",\n    midpoint = 0\n  ) +\n  labs(\n    x = \"\",\n    y = \"Penalty Yard Differential\"\n  ) +\n  blog_theme() + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")\n\n\n\n\n\nPenalty Yard Differential by Team (2024)\n\n\n\n\nLook, the Chiefs are 8th in PYD for this season! Yes, they are. However, they are… only 8th… If the NFL is using the refs to help out KC, then are they also doing the same to help out their rivals like Buffalo? What about the Cinderella story of the Vikings this year? They are nowhere close to anyone else in positive PYD. Or, what about my Dallas Cowboys, who have a near identical PYD to the Chiefs? Is the NFL engaged in a conspiracy to help these teams out too? I don’t think so… So, we’ve seen that the Chiefs have a low PYD overall but had a high one this year. What about each year throughout Mahomes’s career?\n\n\nCode\n# Calculate Penalty Yard Differential by Team\npenalty_differential_over_time &lt;- pbp_data_2018_2024 %&gt;%\n  # Only Keep Penalty Plays\n  filter(penalty == 1 & !is.na(penalty_yards)) %&gt;%\n  # Create a Long Form Version with a \"Team\" Column and a Possession/Defense Dummy\n  gather(key = \"side\", value = \"team\", posteam, defteam) %&gt;%\n  mutate(side_dummy = ifelse(side == \"posteam\", \"offense\", \"defense\")) %&gt;%\n  # Modify Penalty Yards So That If the Team Comitted the Penalty, It is Negative\n  # For Them and Positive for Their Opponent\n  mutate(\n    penalty_yards_adjusted = case_when(\n      penalty_team == team ~ -penalty_yards, \n      penalty_team != team ~ penalty_yards, \n      TRUE ~ 0\n    )\n  ) %&gt;%\n  # Collapse to the Game Level\n  group_by(game_id, team, season) %&gt;%\n  summarize(\n    penalty_yard_differential = sum(penalty_yards_adjusted, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  # Collapse to the Season Level\n  group_by(team, season) %&gt;%\n  summarize(\n    total_penalty_yard_differential = sum(penalty_yard_differential, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  left_join(team_logos, by = c(\"team\" = \"team_abbr\")) %&gt;%\n  filter(team == \"KC\")\n\n# Create a Time Series Plot of Chiefs Average and League Average\nggplot(penalty_differential_over_time, aes(x = season, y = total_penalty_yard_differential)) +\n  # Plot Chiefs Line\n  geom_line(\n    color = \"#E31837\",\n    linewidth = 1 \n  ) +\n    geom_hline(yintercept = 0, linetype = \"dashed\", \n             color = \"black\", linewidth = 1.2) +\n  geom_image(aes(image = team_logo_espn), size = 0.1) +  \n  labs(\n    title = \"\",\n    x = \"\",\n    y = \"Penalty Yard Differential\",\n  ) +\n  scale_x_continuous(breaks = seq(2018, 2024, by = 1)) +\n  blog_theme() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"none\"\n  )\n\n\n\n\n\nChiefs Penalty Yard Differential Each Year (2018-2024)\n\n\n\n\nWell, that’s not entirely compelling, is it? The Chiefs are pretty consistently negativley impacted by the refs. In fact, in their three seasons where they won the Super Bowl (2019, 2022, 2023) they had a negative PYD every season. If they beat the Eagles next week, it will be their first Super Bowl victory with a positive PYD. As you can see, this is not helping the anti-Chiefs case out so far. But, let’s take a deeper look into the context of the penalties themselves. After all, a holding call in the 1st Quarter of the game isn’t near as impactful as a holding call on 3rd down in a game-winning drive, right? So, let’s see if the refs help the Chiefs out in tight spots.\n\n\nClaim 2: Do the Refs Help Out the Chiefs in Tight Spots?\nWe’ll start by looking at PYD during the 4th Quarter. Throughout Mahomes’s entire career, have the Chiefs been given a boost by the refs during the 4th Quarter?\n\n\nCode\n# Calculate Penalty Yard Differential by Team\npenalty_differential_2018_2024_4th &lt;- pbp_data_2018_2024 %&gt;%\n  filter(qtr == 4) %&gt;%\n  filter(penalty == 1) %&gt;%\n  # Create a Long Form Version with a \"Team\" Column and a Possession/Defense Dummy\n  gather(key = \"side\", value = \"team\", posteam, defteam) %&gt;%\n  mutate(side_dummy = ifelse(side == \"posteam\", \"offense\", \"defense\")) %&gt;%\n  # Modify Penalty Yards So That If the Team Committed the Penalty, It is Negative\n  # For Them and Positive for Their Opponent\n  mutate(\n    penalty_yards_adjusted = case_when(\n      penalty_team == team ~ -penalty_yards, \n      penalty_team != team ~ penalty_yards, \n      TRUE ~ 0\n    )\n  ) %&gt;%\n  # Collapse to the Game Level\n  group_by(game_id, team, season) %&gt;%\n  summarize(\n    penalty_yard_differential = sum(penalty_yards_adjusted, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  # Collapse to the Team Level\n  group_by(team) %&gt;%\n  summarize(\n    total_penalty_yard_differential = sum(penalty_yard_differential, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  left_join(team_logos, by = c(\"team\" = \"team_abbr\"))\n\n# Create a Diverging Bar Chart\nggplot(penalty_differential_2018_2024_4th, aes(x = reorder(team, total_penalty_yard_differential), y = total_penalty_yard_differential)) +\n  geom_bar(stat = \"identity\", aes(fill = total_penalty_yard_differential)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", \n             color = \"black\", linewidth = 1.2) +\n  geom_image(aes(image = team_logo_espn), size = 0.05) +\n  scale_fill_gradient2(\n    low = \"#8b0100\",  \n    high = \"#0f3860\",\n    midpoint = 0\n  ) +\n  labs(\n    x = \"\",\n    y = \"Penalty Yard Differential\"\n  ) +\n  blog_theme() + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")\n\n\n\n\n\nChiefs Penalty Yard Differential in the 4th Quarter (2018-2024)\n\n\n\n\nYet again, we see the Chiefs near the bottom. But let’s see if that was any different this season in particular when all of the noise around the Chiefs and the refs exploded.\n\n\nCode\n# Calculate Penalty Yard Differential by Team\npenalty_differential_2024_4th &lt;- pbp_data_2024 %&gt;%\n   filter(qtr == 4) %&gt;%\n  # Only Keep Penalty Plays\n  filter(penalty == 1 & !is.na(penalty_yards)) %&gt;%\n  # Create a Long Form Version with a \"Team\" Column and a Possession/Defense Dummy\n  gather(key = \"side\", value = \"team\", posteam, defteam) %&gt;%\n  mutate(side_dummy = ifelse(side == \"posteam\", \"offense\", \"defense\")) %&gt;%\n  # Modify Penalty Yards So That If the Team Comitted the Penalty, It is Negative\n  # For Them and Positive for Their Opponent\n  mutate(\n    penalty_yards_adjusted = case_when(\n      penalty_team == team ~ -penalty_yards, \n      penalty_team != team ~ penalty_yards, \n      TRUE ~ 0\n    )\n  ) %&gt;%\n  # Collapse to the Game Level\n  group_by(game_id, team, season) %&gt;%\n  summarize(\n    penalty_yard_differential = sum(penalty_yards_adjusted, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  # Collapse to the Season Level\n  group_by(team) %&gt;%\n  summarize(\n    total_penalty_yard_differential = sum(penalty_yard_differential, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  left_join(team_logos, by = c(\"team\" = \"team_abbr\"))\n\n# Create a Diverging Bar Chart\nggplot(penalty_differential_2024_4th, aes(x = reorder(team, total_penalty_yard_differential), y = total_penalty_yard_differential)) +\n  geom_bar(stat = \"identity\", aes(fill = total_penalty_yard_differential)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", \n             color = \"black\", linewidth = 1.2) +\n  geom_image(aes(image = team_logo_espn), size = 0.05) +\n  scale_fill_gradient2(\n    low = \"#8b0100\",  \n    high = \"#0f3860\",\n    midpoint = 0\n  ) +\n  labs(\n    x = \"\",\n    y = \"Penalty Yard Differential\"\n  ) +\n  blog_theme() + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")\n\n\n\n\n\nChiefs Penalty Yard Differential in the 4th Quarter (2024)\n\n\n\n\nYep, about what I expected. Even during this season, the refs still give KC’s opponents more penalty yards in the 4th Quarter than the Chiefs. Next, we’re going to shift our attention towards downs. In particular, both 3rd and 4th downs since converting these are very important for sustaining drives and preventing turnover on downs.\n\n\nCode\n# Calculate Penalty Yard Differential by Team\npenalty_differential_2018_2024_3d &lt;- pbp_data_2018_2024 %&gt;%\n  filter(down == 3) %&gt;%\n  # Only Keep Penalty Plays\n  filter(penalty == 1 & !is.na(penalty_yards)) %&gt;%\n  # Create a Long Form Version with a \"Team\" Column and a Possession/Defense Dummy\n  gather(key = \"side\", value = \"team\", posteam, defteam) %&gt;%\n  mutate(side_dummy = ifelse(side == \"posteam\", \"offense\", \"defense\")) %&gt;%\n  # Modify Penalty Yards So That If the Team Committed the Penalty, It is Negative\n  # For Them and Positive for Their Opponent\n  mutate(\n    penalty_yards_adjusted = case_when(\n      penalty_team == team ~ -penalty_yards, \n      penalty_team != team ~ penalty_yards, \n      TRUE ~ 0\n    )\n  ) %&gt;%\n  # Collapse to the Game Level\n  group_by(game_id, team, season) %&gt;%\n  summarize(\n    penalty_yard_differential = sum(penalty_yards_adjusted, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  # Collapse to the Team Level\n  group_by(team) %&gt;%\n  summarize(\n    total_penalty_yard_differential = sum(penalty_yard_differential, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  left_join(team_logos, by = c(\"team\" = \"team_abbr\"))\n\n# Create a Diverging Bar Chart\nggplot(penalty_differential_2018_2024_3d, aes(x = reorder(team, total_penalty_yard_differential), y = total_penalty_yard_differential)) +\n  geom_bar(stat = \"identity\", aes(fill = total_penalty_yard_differential)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", \n             color = \"black\", linewidth = 1.2) +\n  geom_image(aes(image = team_logo_espn), size = 0.05) +\n  scale_fill_gradient2(\n    low = \"#8b0100\",  \n    high = \"#0f3860\",\n    midpoint = 0\n  ) +\n  labs(\n    x = \"\",\n    y = \"Penalty Yard Differential\"\n  ) +\n  blog_theme() + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")\n\n\n\n\n\nChiefs Penalty Yard Differential on 3rd Down (2018-2024)\n\n\n\n\nNo shocker here… KC ranks 3rd to last on PYD on 3rd down throughout Mahomes’s career and, as you can see in the plot below, the Chiefs are basically right in the middle for the 2024 season.\n\n\nCode\n# Calculate Penalty Yard Differential by Team\npenalty_differential_2024_3d &lt;- pbp_data_2024 %&gt;%\n  filter(down == 3) %&gt;%\n  # Only Keep Penalty Plays\n  filter(penalty == 1 & !is.na(penalty_yards)) %&gt;%\n  # Create a Long Form Version with a \"Team\" Column and a Possession/Defense Dummy\n  gather(key = \"side\", value = \"team\", posteam, defteam) %&gt;%\n  mutate(side_dummy = ifelse(side == \"posteam\", \"offense\", \"defense\")) %&gt;%\n  # Modify Penalty Yards So That If the Team Comitted the Penalty, It is Negative\n  # For Them and Positive for Their Opponent\n  mutate(\n    penalty_yards_adjusted = case_when(\n      penalty_team == team ~ -penalty_yards, \n      penalty_team != team ~ penalty_yards, \n      TRUE ~ 0\n    )\n  ) %&gt;%\n  # Collapse to the Game Level\n  group_by(game_id, team, season) %&gt;%\n  summarize(\n    penalty_yard_differential = sum(penalty_yards_adjusted, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  # Collapse to the Season Level\n  group_by(team) %&gt;%\n  summarize(\n    total_penalty_yard_differential = sum(penalty_yard_differential, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  left_join(team_logos, by = c(\"team\" = \"team_abbr\"))\n\n# Create a Diverging Bar Chart\nggplot(penalty_differential_2024_3d, aes(x = reorder(team, total_penalty_yard_differential), y = total_penalty_yard_differential)) +\n  geom_bar(stat = \"identity\", aes(fill = total_penalty_yard_differential)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", \n             color = \"black\", linewidth = 1.2) +\n  geom_image(aes(image = team_logo_espn), size = 0.05) +\n  scale_fill_gradient2(\n    low = \"#8b0100\",  \n    high = \"#0f3860\",\n    midpoint = 0\n  ) +\n  labs(\n    x = \"\",\n    y = \"Penalty Yard Differential\"\n  ) +\n  blog_theme() + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")\n\n\n\n\n\nChiefs Penalty Yard Differential on 3rd Down (2024)\n\n\n\n\nWhat about fourth downs?\n\n\nCode\n# Calculate Penalty Yard Differential by Team\npenalty_differential_2018_2024_4d &lt;- pbp_data_2018_2024 %&gt;%\n  filter(down == 4) %&gt;%\n  # Only Keep Penalty Plays\n  filter(penalty == 1 & !is.na(penalty_yards)) %&gt;%\n  # Create a Long Form Version with a \"Team\" Column and a Possession/Defense Dummy\n  gather(key = \"side\", value = \"team\", posteam, defteam) %&gt;%\n  mutate(side_dummy = ifelse(side == \"posteam\", \"offense\", \"defense\")) %&gt;%\n  # Modify Penalty Yards So That If the Team Committed the Penalty, It is Negative\n  # For Them and Positive for Their Opponent\n  mutate(\n    penalty_yards_adjusted = case_when(\n      penalty_team == team ~ -penalty_yards, \n      penalty_team != team ~ penalty_yards, \n      TRUE ~ 0\n    )\n  ) %&gt;%\n  # Collapse to the Game Level\n  group_by(game_id, team, season) %&gt;%\n  summarize(\n    penalty_yard_differential = sum(penalty_yards_adjusted, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  # Collapse to the Team Level\n  group_by(team) %&gt;%\n  summarize(\n    total_penalty_yard_differential = sum(penalty_yard_differential, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  left_join(team_logos, by = c(\"team\" = \"team_abbr\"))\n\n# Create a Diverging Bar Chart\nggplot(penalty_differential_2018_2024_4d, aes(x = reorder(team, total_penalty_yard_differential), y = total_penalty_yard_differential)) +\n  geom_bar(stat = \"identity\", aes(fill = total_penalty_yard_differential)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", \n             color = \"black\", linewidth = 1.2) +\n  geom_image(aes(image = team_logo_espn), size = 0.05) +\n  scale_fill_gradient2(\n    low = \"#8b0100\",  \n    high = \"#0f3860\",\n    midpoint = 0\n  ) +\n  labs(\n    x = \"\",\n    y = \"Penalty Yard Differential\"\n  ) +\n  blog_theme() + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")\n\n\n\n\n\nChiefs Penalty Yard Differential on 4th Down (2018-2024)\n\n\n\n\nThe Chiefs are dead-last for Mahomes’s career (lol) and right in the center for 2024, which largely tracks with the PYD on 3rd down.\n\n\nCode\n# Calculate Penalty Yard Differential by Team\npenalty_differential_2024_4d &lt;- pbp_data_2024 %&gt;%\n  filter(down == 4) %&gt;%\n  # Only Keep Penalty Plays\n  filter(penalty == 1 & !is.na(penalty_yards)) %&gt;%\n  # Create a Long Form Version with a \"Team\" Column and a Possession/Defense Dummy\n  gather(key = \"side\", value = \"team\", posteam, defteam) %&gt;%\n  mutate(side_dummy = ifelse(side == \"posteam\", \"offense\", \"defense\")) %&gt;%\n  # Modify Penalty Yards So That If the Team Comitted the Penalty, It is Negative\n  # For Them and Positive for Their Opponent\n  mutate(\n    penalty_yards_adjusted = case_when(\n      penalty_team == team ~ -penalty_yards, \n      penalty_team != team ~ penalty_yards, \n      TRUE ~ 0\n    )\n  ) %&gt;%\n  # Collapse to the Game Level\n  group_by(game_id, team, season) %&gt;%\n  summarize(\n    penalty_yard_differential = sum(penalty_yards_adjusted, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  # Collapse to the Season Level\n  group_by(team) %&gt;%\n  summarize(\n    total_penalty_yard_differential = sum(penalty_yard_differential, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  left_join(team_logos, by = c(\"team\" = \"team_abbr\"))\n\n# Create a Diverging Bar Chart\nggplot(penalty_differential_2024_4d, aes(x = reorder(team, total_penalty_yard_differential), y = total_penalty_yard_differential)) +\n  geom_bar(stat = \"identity\", aes(fill = total_penalty_yard_differential)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", \n             color = \"black\", linewidth = 1.2) +\n  geom_image(aes(image = team_logo_espn), size = 0.05) +\n  scale_fill_gradient2(\n    low = \"#8b0100\",  \n    high = \"#0f3860\",\n    midpoint = 0\n  ) +\n  labs(\n    x = \"\",\n    y = \"Penalty Yard Differential\"\n  ) +\n  blog_theme() + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")\n\n\n\n\n\nChiefs Penalty Yard Differential on 4th Down (2024)\n\n\n\n\nBut, what about overtime? Maybe we can finally find some bit of evidence that the refs favor the Chiefs in a high-stakes situation? Nope, as you can see below, the Chiefs are just barely above average for PYD in OT. Note that I am not showing OT data for just the 2024t season because a lot of teams never went into OT this season and, among those who did, they only went once. So, the sample sizes are much too small to be useful. However, looking in the aggregate over 7 seasons yields some insight.\n\n\nCode\n# Calculate Penalty Yard Differential by Team\npenalty_differential_2018_2024_OT &lt;- pbp_data_2018_2024 %&gt;%\n   filter(qtr == 5) %&gt;%\n  # Only Keep Penalty Plays\n  filter(penalty == 1 & !is.na(penalty_yards)) %&gt;%\n  # Create a Long Form Version with a \"Team\" Column and a Possession/Defense Dummy\n  gather(key = \"side\", value = \"team\", posteam, defteam) %&gt;%\n  mutate(side_dummy = ifelse(side == \"posteam\", \"offense\", \"defense\")) %&gt;%\n  # Modify Penalty Yards So That If the Team Committed the Penalty, It is Negative\n  # For Them and Positive for Their Opponent\n  mutate(\n    penalty_yards_adjusted = case_when(\n      penalty_team == team ~ -penalty_yards, \n      penalty_team != team ~ penalty_yards, \n      TRUE ~ 0\n    )\n  ) %&gt;%\n  # Collapse to the Game Level\n  group_by(game_id, team, season) %&gt;%\n  summarize(\n    penalty_yard_differential = sum(penalty_yards_adjusted, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  # Collapse to the Team Level\n  group_by(team) %&gt;%\n  summarize(\n    total_penalty_yard_differential = sum(penalty_yard_differential, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  left_join(team_logos, by = c(\"team\" = \"team_abbr\"))\n\n# Create a Diverging Bar Chart\nggplot(penalty_differential_2018_2024_OT, aes(x = reorder(team, total_penalty_yard_differential), y = total_penalty_yard_differential)) +\n  geom_bar(stat = \"identity\", aes(fill = total_penalty_yard_differential)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", \n             color = \"black\", linewidth = 1.2) +\n  geom_image(aes(image = team_logo_espn), size = 0.05) +\n  scale_fill_gradient2(\n    low = \"#8b0100\",  \n    high = \"#0f3860\",\n    midpoint = 0\n  ) +\n  labs(\n    x = \"\",\n    y = \"Penalty Yard Differential\"\n  ) +\n  blog_theme() + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")\n\n\n\n\n\nPenalty Yard Differential in Over Time (2018-2024)\n\n\n\n\nLastly, to evaluate whether the Chiefs are helped by the refs in tough spots (so far, everything has indicated the opposite), let’s look at penalties that are called when a team is trailing within one score (7 points are less). When the game is close enough, perhaps the refs are more trigger happy to make certain calls or refuse to make certain calls in order to benefit the Chiefs.\n\n\nCode\n# Calculate Penalty Yard Differential by Team\npenalty_differential_2018_2024_trail &lt;- pbp_data_2018_2024 %&gt;%\n  filter(score_differential &gt;= -7 & score_differential &lt;= 0) %&gt;%\n  # Only Keep Penalty Plays\n  filter(penalty == 1 & !is.na(penalty_yards)) %&gt;%\n  # Create a Long Form Version with a \"Team\" Column and a Possession/Defense Dummy\n  gather(key = \"side\", value = \"team\", posteam, defteam) %&gt;%\n  mutate(side_dummy = ifelse(side == \"posteam\", \"offense\", \"defense\")) %&gt;%\n  # Modify Penalty Yards So That If the Team Committed the Penalty, It is Negative\n  # For Them and Positive for Their Opponent\n  mutate(\n    penalty_yards_adjusted = case_when(\n      penalty_team == team ~ -penalty_yards, \n      penalty_team != team ~ penalty_yards, \n      TRUE ~ 0\n    )\n  ) %&gt;%\n  # Collapse to the Game Level\n  group_by(game_id, team, season) %&gt;%\n  summarize(\n    penalty_yard_differential = sum(penalty_yards_adjusted, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  # Collapse to the Team Level\n  group_by(team) %&gt;%\n  summarize(\n    total_penalty_yard_differential = sum(penalty_yard_differential, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  left_join(team_logos, by = c(\"team\" = \"team_abbr\"))\n\n# Create a Diverging Bar Chart\nggplot(penalty_differential_2018_2024_trail, aes(x = reorder(team, total_penalty_yard_differential), y = total_penalty_yard_differential)) +\n  geom_bar(stat = \"identity\", aes(fill = total_penalty_yard_differential)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", \n             color = \"black\", linewidth = 1.2) +\n  geom_image(aes(image = team_logo_espn), size = 0.05) +\n  scale_fill_gradient2(\n    low = \"#8b0100\",  \n    high = \"#0f3860\",\n    midpoint = 0\n  ) +\n  labs(\n    x = \"\",\n    y = \"Penalty Yard Differential\"\n  ) +\n  blog_theme() + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")\n\n\n\n\n\nPenalty Yard Differential When Trailing (2018-2024)\n\n\n\n\nThroughout Mahomes’s career, again, the Chiefs are just barely above average and, for the 2024 season, we actually get a shred of evidence that the Chiefs are positively impacted by ref calls. However, I would stress that, if one wants to make a statement like “the refs favor the Chiefs” based on one statistic, that conviction should also suggest that the NFL is rigging the game in favor of other teams like the Vikings, Colts, Packers, etc. I don’t think anyone thiks that…\n\n\nCode\n# Calculate Penalty Yard Differential by Team\npenalty_differential_2024_trail &lt;- pbp_data_2024 %&gt;%\n  filter(score_differential &gt;= -7 & score_differential &lt;= 0) %&gt;%\n  # Only Keep Penalty Plays\n  filter(penalty == 1 & !is.na(penalty_yards)) %&gt;%\n  # Create a Long Form Version with a \"Team\" Column and a Possession/Defense Dummy\n  gather(key = \"side\", value = \"team\", posteam, defteam) %&gt;%\n  mutate(side_dummy = ifelse(side == \"posteam\", \"offense\", \"defense\")) %&gt;%\n  # Modify Penalty Yards So That If the Team Comitted the Penalty, It is Negative\n  # For Them and Positive for Their Opponent\n  mutate(\n    penalty_yards_adjusted = case_when(\n      penalty_team == team ~ -penalty_yards, \n      penalty_team != team ~ penalty_yards, \n      TRUE ~ 0\n    )\n  ) %&gt;%\n  # Collapse to the Game Level\n  group_by(game_id, team, season) %&gt;%\n  summarize(\n    penalty_yard_differential = sum(penalty_yards_adjusted, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  # Collapse to the Season Level\n  group_by(team) %&gt;%\n  summarize(\n    total_penalty_yard_differential = sum(penalty_yard_differential, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  left_join(team_logos, by = c(\"team\" = \"team_abbr\"))\n\n# Create a Diverging Bar Chart\nggplot(penalty_differential_2024_trail, aes(x = reorder(team, total_penalty_yard_differential), y = total_penalty_yard_differential)) +\n  geom_bar(stat = \"identity\", aes(fill = total_penalty_yard_differential)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", \n             color = \"black\", linewidth = 1.2) +\n  geom_image(aes(image = team_logo_espn), size = 0.05) +\n  scale_fill_gradient2(\n    low = \"#8b0100\",  \n    high = \"#0f3860\",\n    midpoint = 0\n  ) +\n  labs(\n    x = \"\",\n    y = \"Penalty Yard Differential\"\n  ) +\n  blog_theme() + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")\n\n\n\n\n\nPenalty Yard Differential When Trailing (2024)\n\n\n\n\n\n\nClaim 3: Do the Refs Protect Patrick Mahomes?\nAt this point, it should be very obvious that the refs, overall, do not favor the Chiefs and, in addition, they do not favor the Chiefs in tight spots. This is apparent throughout Mahomes’s entire career and in the current 2024 NFL season. But what we’ve looked at so far are team-level stats. Patrick Mahomes is becoming (already is?) the face of the NFL and perhaps the league wants to protect their “golden boy poster child”.\nTo evaluate this, I took a look at roughing the passer and defensive pass interference calls. If the refs wanted to protect Mahomes, surely Mahomes would have a higher rate of roughing the passer calls go in his favor. In addition, if the league wanted to help out the Chiefs, surely pass attempts by Mahomes would result in a higher rate of “fluke” defensive pass interference calls being made. Below, I plot the average number of roughing the passer calls (standardized by quarterback drop-backs) and the average number of defensive pass interference calls (standardized by passing attempts) over the 2018-2024 seasons. In particular, I compare Mahomes to some of his contemporaries who never seem to get heat for calls going their way…\n\n\nCode\nqb_data &lt;- pbp_data_2018_2024 %&gt;%\n  filter(passer %in% c(\"P.Mahomes\", \"J.Allen\", \"L.Jackson\", \"J.Burrow\")) %&gt;%\n  group_by(passer, season) %&gt;%\n  summarize(\n    drop_backs = sum(qb_dropback, na.rm = TRUE),\n    pass_attempts = sum(pass_attempt, na.rm = TRUE),\n    roughing_calls = sum(penalty == 1 & penalty_type == \"Roughing the Passer\", na.rm = TRUE),\n    dpi_calls = sum(penalty == 1 & penalty_type == \"Defensive Pass Interference\", na.rm = TRUE),\n    rc_per_db = (roughing_calls / drop_backs),\n    dpi_per_pa = (dpi_calls / pass_attempts)\n  ) %&gt;%\n  filter(drop_backs &gt;= 100) %&gt;%\n  ungroup()\n\nleague_avg &lt;- pbp_data_2018_2024 %&gt;%\n  group_by(passer, season) %&gt;%\n  summarize(\n    drop_backs = sum(qb_dropback, na.rm = TRUE),\n    pass_attempts = sum(pass_attempt, na.rm = TRUE),\n    roughing_calls = sum(penalty == 1 & penalty_type == \"Roughing the Passer\", na.rm = TRUE),\n    dpi_calls = sum(penalty == 1 & penalty_type == \"Defensive Pass Interference\", na.rm = TRUE),\n    rc_per_db = (roughing_calls / drop_backs),\n    dpi_per_pa = (dpi_calls / pass_attempts)\n  ) %&gt;%\n  filter(drop_backs &gt;= 100) %&gt;%\n  group_by(season) %&gt;%\n  summarise(rc_per_db = mean(rc_per_db),\n            dpi_per_pa = mean(dpi_per_pa)) %&gt;%\n  ungroup() %&gt;%\n  mutate(passer = \"Average\")\n\n# Merge These Data Sources Together\nqb_data &lt;- qb_data %&gt;%\n  full_join(league_avg, by = c(\"passer\", \"season\", \"rc_per_db\", \"dpi_per_pa\")) %&gt;%\n  # Create a Team to Player Logo\n  mutate(team_logo_espn = case_when(\n      passer == \"P.Mahomes\" ~ \"https://a.espncdn.com/i/teamlogos/nfl/500/kc.png\",\n      passer == \"J.Allen\" ~ \"https://a.espncdn.com/i/teamlogos/nfl/500/buf.png\",\n      passer == \"L.Jackson\" ~ \"https://a.espncdn.com/i/teamlogos/nfl/500/bal.png\",\n      passer == \"J.Burrow\" ~ \"https://a.espncdn.com/i/teamlogos/nfl/500/cin.png\",\n      TRUE ~ NA\n  ))\n\nggplot(qb_data, aes(x = season, y = rc_per_db)) +\n  # Plot League Average\n  geom_line(\n    data = league_avg,\n    color = \"grey\",  \n    linetype = \"dashed\",  \n    linewidth = 1\n  ) +\n  geom_point(data = league_avg, size = 3, color = \"grey\") + \n  # Plot Mahomes Line\n  geom_line(\n    data = qb_data %&gt;% filter(passer == \"P.Mahomes\"),\n    color = \"#E31837\",\n    linewidth = 1 \n  ) +\n  # Plot Allen Line\n  geom_line(\n    data = qb_data %&gt;% filter(passer == \"J.Allen\"), \n    color = \"#00338D\",\n    linewidth = 1 \n  ) +\n  # Plot Jackson Line\n  geom_line(\n    data = qb_data %&gt;% filter(passer == \"L.Jackson\"),\n    color = \"#9E7C0C\",\n    linewidth = 1 \n  ) +\n   # Plot Burrow Line\n  geom_line(\n    data = qb_data %&gt;% filter(passer == \"J.Burrow\"),\n    color = \"#fb4f14\",\n    linewidth = 1 \n  ) +\n  geom_image(aes(image = team_logo_espn), size = 0.07) + \n  labs(\n    title = \"\",\n    x = \"\",\n    y = \"Roughing Calls per QB Drop Backs\"\n  ) +\n  scale_x_continuous(breaks = seq(2018, 2024, by = 1)) +\n  blog_theme() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.title = element_text(face = \"bold\")  \n  )\n\n\n\n\n\nRoughing the Passer Calls per Drop Backs\n\n\n\n\nAs you can see, Mahomes goes above and below the average for roughing the passer calls per drop-backs. Certainly, his seasons do not suggest a systematic attempt of the refs to influence games to impact Mahomes. Josh Allen (I would say, among fans, the true “golden boy”) has finished with a lower roughing the passer call rate below Mahomes once over the past seven years! That certainly doesn’t fit the narrative! What about defensive pass intereference (DPI)?\n\n\nCode\nggplot(qb_data, aes(x = season, y = dpi_per_pa)) +\n  # Plot League Average\n  geom_line(\n    data = league_avg,\n    color = \"grey\",  \n    linetype = \"dashed\",  \n    linewidth = 1\n  ) +\n  geom_point(data = league_avg, size = 3, color = \"grey\") + \n  # Plot Mahomes Line\n  geom_line(\n    data = qb_data %&gt;% filter(passer == \"P.Mahomes\"),\n    color = \"#E31837\",\n    linewidth = 1 \n  ) +\n  # Plot Allen Line\n  geom_line(\n    data = qb_data %&gt;% filter(passer == \"J.Allen\"), \n    color = \"#00338D\",\n    linewidth = 1 \n  ) +\n  # Plot Jackson Line\n  geom_line(\n    data = qb_data %&gt;% filter(passer == \"L.Jackson\"),\n    color = \"#9E7C0C\",\n    linewidth = 1 \n  ) +\n   # Plot Burrow Line\n  geom_line(\n    data = qb_data %&gt;% filter(passer == \"J.Burrow\"),\n    color = \"#fb4f14\",\n    linewidth = 1 \n  ) +\n  geom_image(aes(image = team_logo_espn), size = 0.07) + \n  labs(\n    title = \"\",\n    x = \"\",\n    y = \"DPI Calls per Pass Attempt\"\n  ) +\n  scale_x_continuous(breaks = seq(2018, 2024, by = 1)) +\n  blog_theme() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.title = element_text(face = \"bold\")  \n  )\n\n\n\n\n\nDPI Calls per Pass Attempts\n\n\n\n\nWell, this information is even less convenient than the roughing the passer data for the Chiefs skeptics. Since Joe Burrow joined the league in 2020, he has finished with a higher DPI call rate than Mahomes in every season. Josh Allen has finished with a higher rate in 5/7 seasons. And even Lamar Jackson has finished with a higher DPI rate than Mahomes a majority of season (4/7). On Claim #3, it still seems like we fail to find any support for the “refs help out the Chiefs” claim.\n\n\nClaim 4: Does Taylor Swift Impact the Game?\nEver since Taylor Swift and Travis Kelce became an item, NFL fans have claimed that her public association with the league somehow incentives the NFL to violate the integrity of the game. I’m not sure what the logic is here. So long as Swift and Kelce are together, I imagine she will still be attending games and publicly supporting the Chiefs, regardless of how well they perform. But, I still want to evaluate the claim.\nTo do this, I made reference to this article which claims to document each Chiefs game that Taylor Swift has attended. Then, I looked at the PYD for each game that Swift attended and the PYD when Swift was not in attendance. You can see this information in the plot below.\n\n\nCode\n# Load Play-by-Play (PBP) Data\npbp_data_2023_2024 &lt;- load_pbp(seasons = 2023:2024)\n\npenalty_differential_by_game &lt;- pbp_data_2023_2024 %&gt;%\n  # Only Keep Penalty Plays\n  filter(penalty == 1 & !is.na(penalty_yards)) %&gt;%\n  # Create a Long Form Version with a \"Team\" Column and a Possession/Defense Dummy\n  gather(key = \"side\", value = \"team\", posteam, defteam) %&gt;%\n  mutate(side_dummy = ifelse(side == \"posteam\", \"offense\", \"defense\")) %&gt;%\n  # Modify Penalty Yards So That If the Team Comitted the Penalty, It is Negative\n  # For Them and Positive for Their Opponent\n  mutate(\n    penalty_yards_adjusted = case_when(\n      penalty_team == team ~ -penalty_yards, \n      penalty_team != team ~ penalty_yards, \n      TRUE ~ 0\n    )\n  ) %&gt;%\n  # Collapse to the Game Level\n  group_by(game_id, team, season) %&gt;%\n  summarize(\n    penalty_yard_differential = sum(penalty_yards_adjusted, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  left_join(team_logos, by = c(\"team\" = \"team_abbr\")) %&gt;%\n  filter(team == \"KC\") %&gt;%\n  mutate(tay_tay = case_when(\n    game_id == \"2024_21_BUF_KC\" ~ 1,\n    game_id == \"2024_20_HOU_KC\" ~ 1,\n    game_id == \"2024_16_HOU_KC\" ~ 1,\n    game_id == \"2024_13_LV_KC\" ~ 1,\n    game_id == \"2024_10_DEN_KC\" ~ 1,\n    game_id == \"2024_09_TB_KC\" ~ 1,\n    game_id == \"2024_05_NO_KC\" ~ 1,\n    game_id == \"2024_02_CIN_KC\" ~ 1,\n    game_id == \"2024_01_BAL_KC\" ~ 1,\n    game_id == \"2023_22_SF_KC\" ~ 1,\n    game_id == \"2023_21_KC_BAL\" ~ 1,\n    game_id == \"2023_20_KC_BUF\" ~ 1,\n    game_id == \"2023_19_MIA_KC\" ~ 1,\n    game_id == \"2023_17_CIN_KC\" ~ 1,\n    game_id == \"2023_16_LV_KC\" ~ 1,\n    game_id == \"2023_15_KC_NE\" ~ 1,\n    game_id == \"2023_14_BUF_KC\" ~ 1,\n    game_id == \"2023_13_KC_GB\" ~ 1,\n    game_id == \"2023_07_LAC_KC\" ~ 1,\n    game_id == \"2023_06_DEN_KC\" ~ 1,\n    game_id == \"2023_04_KC_NYJ\" ~ 1,\n    game_id == \"2023_03_CHI_KC\" ~ 1,\n    TRUE ~ 0\n  )) %&gt;%\n  mutate(game_id = factor(game_id, levels = game_id),\n         game_number = row_number())\n\navg_differential &lt;- penalty_differential_by_game %&gt;%\n  group_by(tay_tay) %&gt;%\n  summarize(avg_penalty_yard_differential = mean(penalty_yard_differential))\n\nggplot(penalty_differential_by_game, aes(x = game_id, y = penalty_yard_differential)) +\n  geom_hline(yintercept = 0, color = \"black\", linewidth = 1.2) + \n  geom_hline(\n    data = avg_differential,\n    aes(yintercept = avg_penalty_yard_differential, color = factor(tay_tay)),\n    linetype = \"dashed\", linewidth = 1, show.legend = FALSE\n  ) +\n  geom_line(aes(group = 1), color = \"gray70\", linewidth = 1) + \n  geom_point(aes(color = factor(tay_tay)), size = 3.5) + \n  labs(\n    title = \"\",\n    subtitle = \"Horizontal dashed lines indicate averages when Taylor Swift is & is not present.\",\n    x = \"\",\n    y = \"Penalty Yard Differential\",\n    color = \"Was Taylor Swift At This Game?\" \n  ) +\n  scale_color_manual(\n    values = c(\"1\" = \"#8b0100\", \"0\" = \"#0f3860\"), \n    labels = c(\"Yes\", \"No\"), \n    breaks = c(\"1\", \"0\"),\n    guide = guide_legend(title.position = \"top\")\n  ) +\n  scale_y_continuous(breaks = seq(-80, 80, by = 20)) +\n  blog_theme() + \n  theme(\n    plot.subtitle = element_text(face = \"bold\"),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"bottom\",\n    legend.title = element_text(hjust = 0.5),\n    legend.text.align = 0.5\n  )\n\n\n\n\n\nTaylor Swift Attendance and Penalty Yard Differential (2023-2024)\n\n\n\n\nThe colored horizontal lines reflect the average PYD when Taylor Swift is present (red) and when she is not present (blue). This is kind of fun, because you do see a difference and it isn’t entirely negligible. It’s a roughly 40-yard difference which means that, on average throughout the entirety of the 2023-2024 NFL season, the Chiefs got 40 more yards from penalties than their opponents did.\nIs this evidence that, even if the NFL doesn’t rig games for the Chiefs, they are at least more trigger happy to help the Chiefs when Taylor is in attendance? Who knows? I would say that the difference in PYD is certainly interesting, but it’s impossible to know unless you adjust for other factors (I have a whole blog post about this.)\n\n\nWhy You Think the Game is Rigged\nOverall, the overwhelming evidence suggests that the Chiefs are not being helped by the refs. In many cases, they rank very poorly in PYD, in some cases, they are right in the middle and, in a minority of cases, they are near the top in PYD. Further, plenty of other teams consistently record a larger PYD than the Chiefs (such as the Bills), but we fail to hear about any conspiracy regarding these teams. Why? Why might you think the league helps out the Chiefs when the data suggests otherwise?\nThis likely just has to do with both selection and confirmation bias. If you watch a disproportionately higher number of Chiefs games (which, you likely do because they are on prime-time more often), then you see more questionable ref calls being made. As you watch more games over time, you might begin to think that the Chiefs sure do get a lot of calls. However, by not watching other teams as often as the Chiefs, you are cognitively discounting how many other questionable calls are made in the favor of other teams. In other words, the pattern only seems suspicious because you’re not seeing how often these calls are made for other teams.\nThis leads to the second problem, which is confirmation bias. Once you have made the (false) connection that all the calls go the Chiefs’ way, every additional questionable call you see favor the Chiefs only goes to confirm your suspicions. Looking at the game like this makes you susceptible to arriving at false conclusions. You become so focused on the Chiefs, that you fail to see when other teams get away with missed calls or have their opponents called more often or have questionable calls go their way.\nSo, what can you take from this? I would say, stop complaining (because there is no reason to) and enjoy history being made! This team has been a blast to watch and, as a Dallas Cowboys fan and a resident of the greater Tyler, Texas area (where Mahomes grew up), I sincerely hope that the Chiefs do what they do best and send Philly to Cancun."
  },
  {
    "objectID": "blog/2025/functional-form/index.html",
    "href": "blog/2025/functional-form/index.html",
    "title": "The Overlooked Part of Confounder Adjustment",
    "section": "",
    "text": "Code\n# Load Libraries\npacman::p_load(\n  \"dplyr\", # Data Manipulation\n  \"ggplot2\", # Data Visualization\n  \"tidyr\", # Re-Shaping\n  \"scales\", # Scaling Visualizations\n  \"ggtext\", # Colored Text in Plots\n  \"gt\", # Nice Tables\n  install = FALSE\n)\n\n# Define a Custom Theme\nblog_theme &lt;- function() {\n  theme_bw() +  \n    theme(\n      panel.grid.major = element_line(color = \"gray80\", size = 0.3),\n      panel.grid.minor = element_blank(),\n      panel.border = element_blank(),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      plot.title = element_text(face = \"bold\", size = 16, margin = margin(t = 0, r = 0, b = 15, l = 0)),\n      axis.title.x = element_text(face = \"bold\", size = 14, margin = margin(t = 15, r = 0, b = 0, l = 0)),\n      axis.title.y = element_text(face = \"bold\", size = 14, margin = margin(t = 0, r = 15, b = 0, l = 0)),\n      strip.text = element_text(face = \"bold\"),\n      axis.text.x = element_text(face = \"bold\", size = 10), \n      axis.text.y = element_text(face = \"bold\", size = 10), \n      axis.ticks.x = element_blank(), \n      axis.ticks.y = element_blank(), \n      strip.background = element_rect(fill = \"grey80\", color = NA),\n      legend.title = element_text(face = \"bold\", size = 14),\n      legend.text = element_text(face = \"bold\", size = 10, color = \"grey25\"),\n    )\n}\n\n# Establish a Custom Color Scheme\ncolors &lt;- c(\n  \"1\" = \"#133a29\",\n  \"2\" = \"#ab3d29\",\n  \"3\" = \"#f9ba6d\",\n  \"4\" = \"#314318\",\n  \"5\" = \"#63221f\"\n)\n\n\n\nIntro\nI want to start this blog off acknowledging that this blog is exploratory for myself. I am not and will not be attempting to “teach” or anything like that in this post. However, the blog format is a helpful way for me to get my hands dirty and learn something new so, read at your own peril, but also, join me for the journey.\nIf you’ve read my posts before (or are familiar with the causal inference literature… if not, read this blog I wrote, then you know that adjusting for confounders is something you need to do if you’re not running an experiment (for the most part).\nLess appreciated, although always mentioned, is the modeling side of things. That is, because we work with statistical models that have their own assumptions, we need to make sure that, in addition to adjusting for confounders, we also adjust for their correct functional form. But, I’ve always wondered, how consequential is it to adjust for a confounder assuming an entirely linear relationship if the actual relationship between a confounder and the outcome is non-linear?\nWell, that is the entire point of this post. I’m really curious to see how, within this limited example, how the bias changes between an inclusion of a necessary control variable (misspecified) and the inclusion of the control variable correctly specified. I’m going to look at four different scenarios in the process.\nFirst, I’ll take a look at a squared effect, which looks something like the following:\n\n\nCode\nset.seed(1234)\n\nn_obs &lt;- 10000\nexposure_effect &lt;- 1.5\n  \n  for (i in 1:1000) {\n    # Vary the Linear Effect of Z Between 1% and 300% of the Exposure Effect\n    z_linear_effect &lt;- runif(1, 0.01, 3) * 1.5\n    # Make the Curvilinear Effect Always 50% of the Linear Effect\n    z_curv_effect &lt;- 0.5 * z_linear_effect\n    \n    # Generate Variables\n    Z &lt;- rnorm(n_obs, 0, 1)             \n    X &lt;- 0.5 * Z + rnorm(n_obs, 0, 1)   \n    Y &lt;- exposure_effect * X + z_linear_effect * Z + z_curv_effect * Z^2 + rnorm(n_obs, 0, 1)\n  }\n\nsquared_df &lt;- data.frame(Z = Z, Y = Y)\n\nggplot(squared_df, aes(x = Z, y = Y)) +\n  geom_point(alpha = 0.1, size = 0.8, color = \"#133a29\") +\n  geom_smooth(method = \"loess\", se = FALSE, size = 1.5, color = \"#ab3d29\") +\n  xlim(-4, 4) +\n  labs(\n    x = \"Z\",\n    y = \"Y\"\n  ) +\n  blog_theme()\n\n\n\n\n\nQuadratic Confounder-Outcome Effect\n\n\n\n\nThen, I’ll take a look at another polynomial, examining a cubic effect instead:\n\n\nCode\nset.seed(1234)\n\nn_obs &lt;- 10000\nexposure_effect &lt;- 1.5\n\n  for (i in 1:1000) {\n    # Vary the Linear Effect of Z Between 1% and 300% of the Exposure Effect\n    z_linear_effect &lt;- runif(1, 0.01, 3) * exposure_effect\n    # Make the Squared Effect Always 50% of the Linear Effect\n    z_squared_effect &lt;- 0.5 * z_linear_effect\n    # Make the Cubed Effect Always 50% of the Squared Effect\n    z_cubed_effect &lt;- 0.5 * z_squared_effect\n    \n    # Generate Variables\n    Z &lt;- rnorm(n_obs, 0, 1)             \n    X &lt;- 0.5 * Z + rnorm(n_obs, 0, 1)   \n    Y &lt;- exposure_effect * X + z_linear_effect * Z + z_squared_effect * Z^2 +\n         z_cubed_effect * Z^3 + rnorm(n_obs, 0, 1)\n  }\n\ncubed_df &lt;- data.frame(Z = Z, Y = Y)\n\nggplot(cubed_df, aes(x = Z, y = Y)) +\n  geom_point(alpha = 0.1, size = 0.8, color = \"#133a29\") +\n  geom_smooth(method = \"loess\", se = FALSE, size = 1, color = \"#ab3d29\") +\n  xlim(-4, 4) +\n  labs(\n    x = \"Z\",\n    y = \"Y\"\n  ) +\n  blog_theme()\n\n\n\n\n\nCubic Confounder-Outcome Effect\n\n\n\n\nThen, I’ll shift away from polynomials and take a look at interaction effects. First, I’ll dive into interaction effects where the interaction effect remains fixed but the main effect varies, then I’ll do the inverse, fixing the main effect constant but letting the interaction effect vary. Regardless, either situation could produce a confounder-outcome effect that looks like this:\n\n\nCode\nset.seed(1234)\n\nn_obs &lt;- 10000\nexposure_effect &lt;- 1.5\n\n  for (i in 1:1000) {\n    # Fixed Interaction Effect\n    interaction_effect &lt;- 1\n  \n    # Vary Main Effect of Z\n    z_main_effect &lt;- runif(1, 0.01, 3) * exposure_effect\n  \n    # Generate Variables\n    Z &lt;- rnorm(n_obs, 0, 1)\n    X &lt;- 0.5 * Z + rnorm(n_obs, 0, 1)\n    Y &lt;- exposure_effect * X + z_main_effect * Z + interaction_effect * X * Z + rnorm(n_obs, 0, 1)\n  }\n\ninteraction_df &lt;- data.frame(Z = Z, Y = Y, X = X)\n\n# Create Artificial Cut-Off Points for X to See How the Z Effect on Y Varies Across X Levels\ninteraction_df &lt;- interaction_df %&gt;%\n     mutate(X_bin = cut(X,breaks = quantile(X, probs = c(0, 1/3, 2/3, 1)),\n                        labels = c(\"Low X\", \"Medium X\", \"High X\"),\n                        include.lowest = TRUE))\n\nggplot(interaction_df, aes(x = Z, y = Y)) +\n  geom_point(alpha = 0.1, size = 0.8, color = \"#133a29\") +\n  geom_smooth(method = \"lm\", se = FALSE, size = 1, color = \"#ab3d29\") +\n  xlim(-4, 4) +\n  facet_wrap(~X_bin) +\n  labs(\n    x = \"Z\",\n    y = \"Y\"\n  ) +\n  blog_theme()\n\n\n\n\n\nConfounder-Outcome Effect That Interacts with the Exposure\n\n\n\n\n\n\nSquared Confounder-Outcome Simulation\nStarting off, the following simulation code seeks to answer, “if we kept the squared effect of \\(Z\\) on \\(Y\\) constant, but let the linear component of the \\(Z \\rightarrow Y\\) effect vary from 0.1x the size of the \\(X \\rightarrow Y\\) effect to 3x the size of the \\(X \\rightarrow Y\\) effect, how much bias do we see if we 1) omit the confounder entirely, 2) include the confounder but only specify it linearly, and 3) include the confounder and specify its correct functional form”?\nOkay, that’s a long question, but we can summarize it as “if we generated data (\\(n\\) = 1000) 10,000 times, and let the linear confounder effect/exposure effect vary, specify different models that are increasingly wrong, how much bias do we get?”\n\n# Create a Function to Estimate Consequences of Misspecification for A Lot of Runs and Different Scenarios\nsquared_effect_dgp &lt;- function(n_runs = 10000, n_obs = 1000, exposure_effect = 1.5) {\n  \n  set.seed(1234)\n  \n  # Store Results\n  results &lt;- data.frame(\n    sim = 1:n_runs,\n    z_effect_ratio = NA,\n    model1_effect = NA, # Omits Z\n    model2_effect = NA, # Includes Z Without Functional Form\n    model3_effect = NA, # Includes Z with Functional Form\n    model1_bias = NA, \n    model2_bias = NA,\n    model3_bias = NA\n  )\n  \n  for (i in 1:n_runs) {\n    # Vary the Linear Effect of Z Between 1% and 300% of the Exposure Effect\n    z_linear_effect &lt;- runif(1, 0.01, 3) * exposure_effect\n    # Make the Curvilinear Effect Always 50% of the Linear Effect\n    z_curv_effect &lt;- 0.5 * z_linear_effect\n    \n    # Generate Variables\n    Z &lt;- rnorm(n_obs, 0, 1)             \n    X &lt;- 0.5 * Z + rnorm(n_obs, 0, 1)   \n    Y &lt;- exposure_effect * X + z_linear_effect * Z + z_curv_effect * Z^2 + rnorm(n_obs, 0, 1)\n    \n    # Model 1: Omits Z\n    model1 &lt;- lm(Y ~ X)\n    # Model 2: Includes Z Without Functional Form\n    model2 &lt;- lm(Y ~ X + Z)\n    # Model 3: Includes Z with Functional Form\n    model3 &lt;- lm(Y ~ X + Z + I(Z^2))\n    \n    # Extract X Coefficients\n    est1 &lt;- coef(model1)[\"X\"]\n    est2 &lt;- coef(model2)[\"X\"]\n    est3 &lt;- coef(model3)[\"X\"]\n    results$z_effect_ratio[i] &lt;- z_linear_effect / exposure_effect\n    \n    results$model1_bias[i] &lt;- est1 - exposure_effect\n    results$model2_bias[i] &lt;- est2 - exposure_effect\n    results$model3_bias[i] &lt;- est3 - exposure_effect\n  }\n  \n  return(results)\n}\n\n# Run the Function\nsquared_effect_dgp_results &lt;- squared_effect_dgp(n_runs = 10000, n_obs = 1000, exposure_effect = 1.5)\n\n# Plot the Results\nggplot(squared_effect_dgp_results, aes(x = z_effect_ratio)) +\n  geom_point(aes(y = model1_bias, color = \"Model 1: Omits Z\"), alpha = 0.2, size = 0.8) +\n  geom_point(aes(y = model2_bias, color = \"Model 2: Linear Z Only\"), alpha = 0.2, size = 0.8) +\n  geom_point(aes(y = model3_bias, color = \"Model 3: Z and Z²\"), alpha = 0.2, size = 0.8) +\n  scale_color_manual(values = c(\"Model 1: Omits Z\" = \"#63221f\",\n                                \"Model 2: Linear Z Only\" = \"#ab3d29\",\n                                \"Model 3: Z and Z²\" = \"#133a29\")) +\n  labs(\n    title = \"\",\n    subtitle = \"On average &lt;b&gt;&lt;span style='color:#63221f'&gt;omitting Z&lt;/span&gt;&lt;/b&gt; results in bias that grows in one direction while &lt;b&gt;&lt;span style='color:#ab3d29'&gt;including Z linearly&lt;/span&gt;&lt;/b&gt; creates bias&lt;br&gt;that spreads in both a negative and a positive direction while &lt;b&gt;&lt;span style='color:#133a29'&gt;including Z and specifying its&lt;br&gt;non-linear effect&lt;/span&gt;&lt;/b&gt; results in practically no bias.\",\n    x = \"Size of the Linear Z Effect on Y Relative to the X Effect on Y\",\n    y = \"Bias\",\n    color = \"\"\n  ) +\n  blog_theme() +\n  theme(legend.position = \"none\",\n        plot.subtitle = ggtext::element_markdown(size = 11))\n\n\n\n\nBias When Excluding a Squared Effect\n\n\n\n\nCool! What do we see from this? Well, obviously, no matter how large our confounder effect / exposure effect ratio is, as long as we specify our model right. there’s basically no bias, with random variation creating any very small deviations from zero bias.\nOn the other side of the things, as we increase the linear confounder effect / exposure effect ratio for the model excluding \\(Z\\) entirely, the bias increases linearly in a positive manner. This is pretty intuitive when you think about it. If the effect of \\(Z\\) is actually really small, then any residual confounding when we don’t control for it will also be small. So very intuitive but also kind of obvious.\nWhat is more interesting is the model where we include \\(Z\\) but do not specify its correct functional form. As we see, if the size of the confounder effect on \\(Y\\) is about equal to the exposure effect on \\(Y\\) or is maybe even smaller, then not specifying the functional form of \\(Z\\) correctly (at least in this scenario) isn’t that big of a deal. But, as the confounder effect grows, so too does the bias of a functional form mis-specification (in both a positive and negative direction). Still, the bias is not anywhere near as severe as the entire exclusion of \\(Z\\) from the model.\n\n\nCubed Confounder-Outcome Simulation\nHere, I’m doing the exact same thing (keeping the polynomial effects fixed and varying the linear effect) but I’m spicing things up with a \\(Z^3\\) effect in addition to the \\(Z^2\\) effect.\n\n# Create a Function to Estimate Consequences of Misspecification for A Lot of Runs and Different Scenarios\ncubed_effect_dgp &lt;- function(n_runs = 10000, n_obs = 1000, exposure_effect = 1.5) {\n  \n  set.seed(1234)\n  \n  # Store Results\n  results &lt;- data.frame(\n    sim = 1:n_runs,\n    z_effect_ratio = NA,\n    model1_effect = NA, # Omits Z\n    model2_effect = NA, # Includes Z Without Functional Forms\n    model3_effect = NA, # Includes Z with Squared Functional Form\n    model4_effect = NA, # Includes Z with All Functional Forms\n    model1_bias = NA, \n    model2_bias = NA,\n    model3_bias = NA\n  )\n  \n  for (i in 1:n_runs) {\n    # Vary the Linear Effect of Z Between 1% and 300% of the Exposure Effect\n    z_linear_effect &lt;- runif(1, 0.01, 3) * exposure_effect\n    # Make the Squared Effect Always 50% of the Linear Effect\n    z_squared_effect &lt;- 0.5 * z_linear_effect\n    # Make the Cubed Effect Always 50% of the Squared Effect\n    z_cubed_effect &lt;- 0.5 * z_squared_effect\n    \n    # Generate Variables\n    Z &lt;- rnorm(n_obs, 0, 1)             \n    X &lt;- 0.5 * Z + rnorm(n_obs, 0, 1)   \n    Y &lt;- exposure_effect * X + z_linear_effect * Z + z_squared_effect * Z^2 +\n         z_cubed_effect * Z^3 + rnorm(n_obs, 0, 1)\n    \n    # Model 1: Omits Z\n    model1 &lt;- lm(Y ~ X)\n    # Model 2: Includes Z Without Functional Forms\n    model2 &lt;- lm(Y ~ X + Z)\n    # Model 3: Includes Z with Squared Functional Form\n    model3 &lt;- lm(Y ~ X + Z + I(Z^2))\n    # Model 4: Includes Z with All Functional Forms\n    model4 &lt;- lm(Y ~ X + Z + I(Z^2) + I(Z^3))\n    \n    # Extract X Coefficients\n    est1 &lt;- coef(model1)[\"X\"]\n    est2 &lt;- coef(model2)[\"X\"]\n    est3 &lt;- coef(model3)[\"X\"]\n    est4 &lt;- coef(model4)[\"X\"]\n    results$z_effect_ratio[i] &lt;- z_linear_effect / exposure_effect\n    \n    results$model1_bias[i] &lt;- est1 - exposure_effect\n    results$model2_bias[i] &lt;- est2 - exposure_effect\n    results$model3_bias[i] &lt;- est3 - exposure_effect\n    results$model4_bias[i] &lt;- est4 - exposure_effect\n  }\n  \n  return(results)\n}\n\n# Run the Function\ncubed_effect_dgp_results &lt;- cubed_effect_dgp(n_runs = 10000, n_obs = 1000, exposure_effect = 1.5)\n\n# Plot the Results\nggplot(cubed_effect_dgp_results, aes(x = z_effect_ratio)) +\n  geom_point(aes(y = model1_bias, color = \"Model 1: Omits Z\"), alpha = 0.2, size = 0.8) +\n  geom_point(aes(y = model2_bias, color = \"Model 2: Linear Z Only\"), alpha = 0.2, size = 0.8) +\n  geom_point(aes(y = model3_bias, color = \"Model 3: Z and Z²\"), alpha = 0.2, size = 0.8) +\n  geom_point(aes(y = model4_bias, color = \"Model 4: Z and Z² and Z³\"), alpha = 0.2, size = 0.8) +\n  scale_color_manual(values = c(\"Model 1: Omits Z\" = \"#63221f\",\n                                \"Model 2: Linear Z Only\" = \"#ab3d29\",\n                                \"Model 3: Z and Z²\" = \"#f9ba6d\",\n                                \"Model 4: Z and Z² and Z³\" = \"#133a29\")) +\n  labs(\n    title = \"\",\n    subtitle = \"On average &lt;b&gt;&lt;span style='color:#63221f'&gt;omitting Z&lt;/span&gt;&lt;/b&gt; results in bias that grows in one direction while &lt;b&gt;&lt;span style='color:#ab3d29'&gt;including Z linearly&lt;/span&gt;&lt;/b&gt; creates bias&lt;br&gt;that spreads in both a negative and a positive direction which is slighy reduced when &lt;b&gt;&lt;span style='color:#f9ba6d'&gt;a squared term for Z&lt;br&gt;is introduced&lt;/span&gt;&lt;/b&gt; while &lt;b&gt;&lt;span style='color:#133a29'&gt;including Z and specifying all its non-linear effects&lt;/span&gt;&lt;/b&gt; results in practically no bias.\",\n    x = \"Size of the Linear Z Effect on Y Relative to the X Effect on Y\",\n    y = \"Bias\",\n    color = \"\"\n  ) +\n  blog_theme() +\n  theme(legend.position = \"none\",\n        plot.subtitle = ggtext::element_markdown(size = 11))\n\n\n\n\nBias When Excluding a Cubed Effect\n\n\n\n\nAnd, as you can see, adding a cubed effect doesn’t change much (again, please keep in mind that I’m fixing the polynomial effects and that this simulation exercise is not systematic… there’s a lot of nuance here that I’m not covering). The biggest gap in bias is the total failure to adjust for \\(Z\\) entirely. Bias increases as the confounder effect size grows and confounder effects are mis-specified, but adding polynomials helps reduce the bias.\nOne thing that might go unnoticed though is the y-axis here. Look at how it goes up to 4 instead of 2 (like the squared effect)! The exposure effect in the squared and cubed effect remains the same, but the bias has increased by about double on the extreme end of the confounder effect / exposure effect size. I’m not really sure why, and there’s a chance I’ve built this into the data generating process without realizing, but that’s also interesting. Well, interesting to a point… we would never worry about that much bias if we already knew about \\(Z\\) and to adjust for it in the first place. Again, adjusting for \\(Z\\) without specifying any polynomials still results bias but dramatically less.\n\n\nInteractive Confounder-Outcome Simulation (Varying Main Effect)\nNext, I’m moving away from polynomials and shifting to interaction effects. Here, the main effect is varying but the interaction effect is remaining constant.\n\ninteraction_dgp_varying_z_main &lt;- function(n_runs = 10000, n_obs = 1000, exposure_effect = 1.5) {\n  set.seed(1234)\n  \n  results &lt;- data.frame(\n    sim = 1:n_runs,\n    z_effect_ratio = NA,\n    model1_effect = NA,  # Omits Z\n    model2_effect = NA,  # Includes Z only\n    model3_effect = NA,  # Includes Z and X:Z\n    model1_bias = NA,\n    model2_bias = NA,\n    model3_bias = NA\n  )\n  \n  for (i in 1:n_runs) {\n    # Keep the Interaction Effect Fixed\n    interaction_effect &lt;- 1\n    \n    # Vary the Main Effect of Z\n    z_main_effect &lt;- runif(1, 0.01, 3) * exposure_effect\n    \n    # Generate Variables\n    Z &lt;- rnorm(n_obs, 0, 1)\n    X &lt;- 0.5 * Z + rnorm(n_obs, 0, 1)\n    Y &lt;- exposure_effect * X + z_main_effect * Z + interaction_effect * X * Z + rnorm(n_obs, 0, 1)\n    \n    # Models\n    model1 &lt;- lm(Y ~ X)      \n    model2 &lt;- lm(Y ~ X + Z)            \n    model3 &lt;- lm(Y ~ X + Z + X:Z)        \n    \n    # Extract X Coefficients\n    est1 &lt;- coef(model1)[\"X\"]\n    est2 &lt;- coef(model2)[\"X\"]\n    est3 &lt;- coef(model3)[\"X\"]\n    \n    # Store Results\n    results$z_effect_ratio[i] &lt;- z_main_effect / exposure_effect\n    results$model1_bias[i] &lt;- est1 - exposure_effect\n    results$model2_bias[i] &lt;- est2 - exposure_effect\n    results$model3_bias[i] &lt;- est3 - exposure_effect\n  }\n  \n  return(results)\n}\n\ninteraction_results_varying_z_main &lt;- interaction_dgp_varying_z_main(n_runs = 10000, n_obs = 1000, exposure_effect = 1.5)\n\nggplot(interaction_results_varying_z_main, aes(x = z_effect_ratio)) +\n  geom_point(aes(y = model1_bias, color = \"Model 1: Omits Z\"), alpha = 0.2, size = 0.8) +\n  geom_point(aes(y = model2_bias, color = \"Model 2: Adds Z Only\"), alpha = 0.2, size = 0.8) +\n  geom_point(aes(y = model3_bias, color = \"Model 3: Adds Z and Interaction\"), alpha = 0.2, size = 0.8) +\n  scale_color_manual(values = c(\n    \"Model 1: Omits Z\" = \"#63221f\",\n    \"Model 2: Adds Z Only\" = \"#ab3d29\",\n    \"Model 3: Adds Z and Interaction\" = \"#133a29\"\n  )) +\n  labs(\n    title = \"\",\n    subtitle = \"On average &lt;b&gt;&lt;span style='color:#63221f'&gt;omitting Z&lt;/span&gt;&lt;/b&gt; results in bias that grows in one direction while &lt;b&gt;&lt;span style='color:#ab3d29'&gt;including Z linearly&lt;/span&gt;&lt;/b&gt; creates bias in&lt;br&gt;both a negative and a positive direction while &lt;b&gt;&lt;span style='color:#133a29'&gt;including Z and specifying the interactive effect&lt;/span&gt;&lt;/b&gt; results&lt;br&gt;in practically no bias.\",\n    x = \"Size of the Main Z Effect on Y Relative to the X Effect on Y\",\n    y = \"Bias\",\n    color = \"\"\n  )  +\n  blog_theme() +\n  theme(legend.position = \"none\",\n        plot.subtitle = ggtext::element_markdown(size = 11))\n\n\n\n\nBias When Excluding an Interaction Effect with the Main Effect Varying\n\n\n\n\nIt looks very familiar to the cubed effect simulation exercise. But, a key difference here is the size of the bias across the x-axis for the model that includes the confounder but mis-specifies its effect on \\(Y\\). It’s basically constant across the entire range of the confounder effect / exposure effect ratio. Substantively, at least for this data generating process, it means that, on average, a failure to specify an interactive effect for \\(Z\\) results in the same bias regardless of how large the confounder effect / exposure effect ratio is.\n\n\nInteractive Confounder-Outcome Simulation (Varying Interactive Effect)\nAnd lastly, I’ll be taking a look at the consequences of failing to adjust for an interactive effect where the main effect size is fixed but the interaction effect is changing.\n\ninteraction_dgp_varying_z_interaction &lt;- function(n_runs = 10000, n_obs = 1000, exposure_effect = 1.5) {\n  set.seed(1234)\n  \n  results &lt;- data.frame(\n    sim = 1:n_runs,\n    z_effect_ratio = NA,\n    model1_effect = NA, # Omits Z\n    model2_effect = NA, # Includes Z\n    model3_effect = NA, # Includes Z and X:Z\n    model1_bias = NA,\n    model2_bias = NA,\n    model3_bias = NA\n  )\n  \n  for (i in 1:n_runs) {\n    # Vary Interaction Effect Size Relative to the Exposure Effect \n    interaction_effect &lt;- runif(1, 0.01, 3) * exposure_effect\n    \n    # Keep the Main Effect Constant\n    z_main_effect &lt;- 1\n    \n    # Generate Variables\n    Z &lt;- rnorm(n_obs, 0, 1)\n    X &lt;- 0.5 * Z + rnorm(n_obs, 0, 1)\n    Y &lt;- exposure_effect * X + z_main_effect * Z + interaction_effect * X * Z + rnorm(n_obs, 0, 1)\n    \n    # Fit models\n    model1 &lt;- lm(Y ~ X)\n    model2 &lt;- lm(Y ~ X + Z)\n    model3 &lt;- lm(Y ~ X + Z + X:Z)\n    \n    # Extract X Coefficients\n    est1 &lt;- coef(model1)[\"X\"]\n    est2 &lt;- coef(model2)[\"X\"]\n    est3 &lt;- coef(model3)[\"X\"]\n    \n    # Store results\n    results$z_effect_ratio[i] &lt;- interaction_effect / exposure_effect\n    results$model1_bias[i] &lt;- est1 - exposure_effect\n    results$model2_bias[i] &lt;- est2 - exposure_effect\n    results$model3_bias[i] &lt;- est3 - exposure_effect\n  }\n  \n  return(results)\n}\n\ninteraction_results_varying_z_interactive &lt;- interaction_dgp_varying_z_interaction(n_runs = 10000, n_obs = 1000, exposure_effect = 1.5)\n\nggplot(interaction_results_varying_z_interactive, aes(x = z_effect_ratio)) +\n  geom_point(aes(y = model1_bias, color = \"Model 1: Omits Z\"), alpha = 0.2, size = 0.8) +\n  geom_point(aes(y = model2_bias, color = \"Model 2: Adds Z Only\"), alpha = 0.2, size = 0.8) +\n  geom_point(aes(y = model3_bias, color = \"Model 3: Adds Z and Interaction\"), alpha = 0.2, size = 0.8) +\n  scale_color_manual(values = c(\n    \"Model 1: Omits Z\" = \"#63221f\",\n    \"Model 2: Adds Z Only\" = \"#ab3d29\",\n    \"Model 3: Adds Z and Interaction\" = \"#133a29\"\n  )) +\n  labs(\n    title = \"\",\n    subtitle = \"On average &lt;b&gt;&lt;span style='color:#63221f'&gt;omitting Z&lt;/span&gt;&lt;/b&gt; results in bias that grows in one direction while &lt;b&gt;&lt;span style='color:#ab3d29'&gt;including Z linearly&lt;/span&gt;&lt;/b&gt; creates bias in&lt;br&gt;both a negative and a positive direction while &lt;b&gt;&lt;span style='color:#133a29'&gt;including Z and specifying the interactive effect&lt;/span&gt;&lt;/b&gt; results&lt;br&gt;in practically no bias.\",\n    x = \"Size of the Interactive Z Effect on Y Relative to the X Effect on Y\",\n    y = \"Bias\",\n    color = \"\"\n  )  +\n  blog_theme() +\n  theme(legend.position = \"none\",\n        plot.subtitle = ggtext::element_markdown(size = 11))\n\n\n\n\nBias When Excluding an Interaction Effect with the Interaction Effect Varying\n\n\n\n\nWell now we have something that is totally different! For both biased models (excluding \\(Z\\) and excluding \\(Z\\) interacted with \\(X\\)), the degree to which they increase with bias increases at about the same rate as the confounder effect / exposure effect ratio increases. However, they are separated basically by their starting point on the y-axis. That’s interesting!\nAnother interesting key point of difference here is the amount of variability that happens as the x-axis increases. The bias really bunches together when the confounder effect is about 0.1-0.5x the size of the exposure effect, but noticeably increases as the confounder effect approaches the exposure effect before all mayhem breaks out. I also find this very interesting!\nAs a final point though, it’s important to take a look at the y-axis though and realize that the magnitude of bias in this scenario is really not all that great (hence the -1, 1 y-axis range). But, this range is probably more indicative of the interaction effects I specified rather than some property of omitting interaction effects in general.\n\n\nConclusion\nSo, what can you take away from this blog? Well, for the most part, again, within the context of this simulation exercise, while getting the functional form specified correctly is important, the bias of excluding a confounder outright seems to have a way larger consequence. Which is not surprising at all. However, the final analysis of interaction effects kind of shows that this might not always be true… there were cases when excluding the confounder exhibited less bias than including the confounder without its interaction effect. I wonder if there’s something to that or if its just a relic of my simulation exercise or some sort of oversight on my end…\nAnd this leads to the ultimate caveat of this blog. This blog was purely exploratory for me. I found writing this fun, but not definitive. So please keep that in mind!"
  },
  {
    "objectID": "blog/2025/pokemon-nlp/index.html",
    "href": "blog/2025/pokemon-nlp/index.html",
    "title": "Predicting Pokemon Types with Pokedex Text Entries",
    "section": "",
    "text": "Code\n# Load Libraries\npacman::p_load(\n  \"dplyr\", # Data Manipulation\n  \"readxl\", # Reading Excel Files\n  \"tidyr\", # Pivoting\n  \"ggplot2\", # Data Visualization\n  \"stringr\", # Working with Strings\n  \"purrr\", # Mapping\n  \"forcats\", # Factors\n  \"ggdist\", # Cool Distribution Plots\n  \"tibble\", # rownames_to_columns\n  \"stringr\", # Working with Texts\n  \"tidytext\", # NLP\n  \"quanteda\", # NLP\n  \"tm\", # NLP\n  \"SnowballC\", # NLP\n  \"caret\", # Machine Learning\n  \"stopwords\", # Stop Words\n  \"seededlda\", # Seeded LDA\n  \"tinytable\", # Tables\n  install = FALSE\n)\n\n# Define a Custom Theme\nblog_theme &lt;- function() {\n  theme_bw() +  \n    theme(\n      panel.grid.major = element_line(color = \"gray80\", size = 0.3),\n      panel.grid.minor = element_blank(),\n      panel.border = element_blank(),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      plot.title = element_text(face = \"bold\", size = 16, margin = margin(t = 0, r = 0, b = 15, l = 0)),\n      axis.title.x = element_text(face = \"bold\", size = 14, margin = margin(t = 15, r = 0, b = 0, l = 0)),\n      axis.title.y = element_text(face = \"bold\", size = 14, margin = margin(t = 0, r = 15, b = 0, l = 0)),\n      strip.text = element_text(face = \"bold\"),\n      axis.text.x = element_text(face = \"bold\", size = 10), \n      axis.text.y = element_text(face = \"bold\", size = 10), \n      axis.ticks.x = element_blank(), \n      axis.ticks.y = element_blank(), \n      strip.background = element_rect(fill = \"grey80\", color = NA),\n      legend.title = element_text(face = \"bold\", size = 14),\n      legend.text = element_text(face = \"bold\", size = 10, color = \"grey25\"),\n    )\n}\n\n# Establish a Custom Color Scheme\ncolors &lt;- c(\n  \"#0a697d\",\n  \"#0091af\",\n  \"#ddb067\",\n  \"#c43d56\",\n  \"#ab2a42\"\n)\n\n# Establish a Pokemon Color Scheme\ntype_colors &lt;- c(\n  normal = \"#A8A77A\",\n  fire = \"#EE8130\",\n  water = \"#6390F0\",\n  electric = \"#F7D02C\",\n  grass = \"#7AC74C\",\n  ice = \"#96D9D6\",\n  fighting = \"#C22E28\",\n  poison = \"#A33EA1\",\n  ground = \"#E2BF65\",\n  flying = \"#A98FF3\",\n  psychic = \"#F95587\",\n  bug = \"#A6B91A\",\n  rock = \"#B6A136\",\n  ghost = \"#735797\",\n  dragon = \"#6F35FC\",\n  dark = \"#705746\",\n  steel = \"#B7B7CE\",\n  fairy = \"#D685AD\"\n)\n\n# Establish a Custom Naming Scheme\ntype_names &lt;- c(\n  grass = \"Grass\",\n  fire = \"Fire\",\n  water = \"Water\",\n  normal = \"Normal\",\n  electric = \"Electric\",\n  ice = \"Ice\",\n  fighting = \"Fighting\",\n  poison = \"Poison\",\n  ground = \"Ground\",\n  flying = \"Flying️\",\n  psychic = \"Psychic\",\n  bug = \"Bug\",\n  rock = \"Rock\",\n  ghost = \"Ghost\",\n  dragon = \"Dragon\",\n  dark = \"Dark\",\n  steel = \"Steel️\",\n  fairy = \"Fairy\"\n)\n\noptions(scipen = 999)\n\n\nEven if you’re not familiar with a lot of data science terminology, you’ve potentially heard of the term “natural language processing” (NLP). Most likely, you’ve probably heard of it within the context of AI/large language models (LLMs). Regardless, natural language processing really just refers to any statistical/machine learning/AI approach that seeks to uncover information and insights from human language. This can range from something as simple as a count of specific words in a document all the way to ChatGPT.\nSomewhere in the middle of that wide distribution is the field of topic modeling. Topic modeling is a sub-field of NLP that is applicable when a researcher has a large body of text (books, chapters, open-text survey responses, Word documents, emails, etc.) and wants to extract some of the themes/topics referenced in this body (a corpus) without manually reading every item in that corpus. There’s a lot of different ways that you can do topic modeling, but typically when you mention it, the methodology of automatically extracting topics out of a corpus is driven by an algorithm developed for this very purpose (often, you will hear of something called “Latent Dirichlet Allocation”).\nIn my line of work, I deal a lot with survey data. All of us have taken surveys, so you’re probably familiar with the concept of an open-text survey question. For example, you could easily imagine something like “please describe anything else related to X that was not covered in this survey” and you’re given a text box to fill in (if you so choose, a lot of people just leave it blank or fill in “NA” if they don’t have anything to share/can’t be bothered to write anything down).\nThese open-text survey questions are great in one sense because they don’t force the respondents to match their answer to some set of discrete options the survey programmer designed. However, the difficult part is that it’s really hard to process and make sense of a lot of open-text survey responses. With discrete responses, it’s really easy since the response is converted to a numeric output. But how do you make sense of hundreds or thousands of open-text responses? You could manually code all of these responses but… have fun with that and let me know when you and your double checker coder are done at Christmas. Naturally, this scenario is a natural use-case for topic modeling. Consequently, I’ve used topic modeling algorithms a lot in my work for this very purpose.\nBut… is topic modeling the best way to handle problems like this anymore? Perhaps some of you have already thought, “why not feed the open-text response into ChatGPT and let it tell you what it’s about rather than having to learn these complex and, oftentimes, finicky topic modeling algorithms?” Well, one reason you might not do this is for privacy concerns. People are probably responding to these open-text items with an expectation that it won’t be fed into an LLM and, depending on who you are working for, you might have client restrictions that prevent you from feeding data into an LLM anyways. Another issue would be reproducibility. With a topic modeling algorithm, you can design the algorithm in such a way that the results are consistent and reproducible every time you run the same line of code. I’m not so sure you’re going to get the same properties out of querying an LLM (to be fair, you probably wouldn’t get the same results every time using human coders either).\nStill Mellon et al. (2024) published an interesting study that compared the predictive accuracy of open-text survey topics when using human coders, LLMs, and topic models. A key takeaway is that some LLMs perform almost as well as human coders and often outperform topic modeling algorithms. I’m really interested in this because I’ve often seen firsthand the struggles that topic modeling algorithms can have. However, I have not been able to test out the LLM approach in my professional work for contract-related reasons. So, I decided to learn about this in a personal project… But what data set should I use?\nWell I thought about this and considered just going to a popular social science survey and giving that a try… but that is too close to what I do for work and so I wanted to do something fun instead. Which is pretty much why I am using a Pokemon data set that sort of mimics what you’ll run into with an open-text survey response. In this blog, I’m going to be testing out a bunch of different ways to see if I can automatically classify a Pokemon’s type based on its Pokedex entry. Pokedex entries are often fairly sparse, with only 1-2 sentences for each Pokemon. That doesn’t sound like a lot (and it’s not) but it resembles the reality of many open-text responses where survey respondents often answer in incomplete sentences or, if you’re lucky, a handful of sentences. That’s just a natural challenge with analyzing open-text survey data, so I wanted to re-create that with a more laid back substitute.\n\nIntroducing the Pokemon Data Set\nYou can find a lot of different Pokemon data sets online, but I needed to find one that had Pokedex entries. Ideally, I would have liked to have found a data set that contained all Pokedex entries (for those unfamiliar, each generation of Pokemon games creates a new Pokedex entry for the Pokemon from previous generations). But, I could not find anything like that. Plus, it sort of creates bias in the analysis since older Pokemon will have more entries, which basically translates to more data, so no matter what we use, we’d always see bias that predicts older Pokemon’s types better than the types of newer Pokemon. So, I used this data set from Kaggle which contains Pokedex entries for all 1,025 Pokemon.\nIn the code chunk below, I am doing a couple of things to manipulate this data to be what I need it to be. For example, because some Pokemon have more than one type, I need to create another column that captures what type(s) Pokemon have. I do this by creating a dummy for each type (this will matter for making predictions later). Lastly, some Pokedex entries have the Pokemon’s name in the entry… which feels a little like cheating. Fortunately, when this happens, the Pokemon’s name is in full caps, so I use some regex to omit any string of text in a Pokemon’s Pokedex entry if it contains the Pokemon’s name.\n\npokedex &lt;- read.csv(\"data/pokedex.csv\")\n\npokedex &lt;- pokedex |&gt; \n  # Create Type Binary Columns\n  mutate(\n    type_grass = ifelse(str_detect(type, \"grass\"), 1, 0),\n    type_fire = ifelse(str_detect(type, \"fire\"), 1, 0),\n    type_water = ifelse(str_detect(type, \"water\"), 1, 0),\n    type_normal = ifelse(str_detect(type, \"normal\"), 1, 0),\n    type_electric = ifelse(str_detect(type, \"electric\"), 1, 0),\n    type_ice = ifelse(str_detect(type, \"ice\"), 1, 0),\n    type_fighting = ifelse(str_detect(type, \"fighting\"), 1, 0),\n    type_poison = ifelse(str_detect(type, \"poison\"), 1, 0),\n    type_ground = ifelse(str_detect(type, \"ground\"), 1, 0),\n    type_flying = ifelse(str_detect(type, \"flying\"), 1, 0),\n    type_psychic = ifelse(str_detect(type, \"psychic\"), 1, 0),\n    type_bug = ifelse(str_detect(type, \"bug\"), 1, 0),\n    type_rock = ifelse(str_detect(type, \"rock\"), 1, 0),\n    type_ghost = ifelse(str_detect(type, \"ghost\"), 1, 0),\n    type_dragon = ifelse(str_detect(type, \"dragon\"), 1, 0),\n    type_dark = ifelse(str_detect(type, \"dark\"), 1, 0),\n    type_steel = ifelse(str_detect(type, \"steel\"), 1, 0),\n    type_fairy = ifelse(str_detect(type, \"fairy\"), 1, 0)\n  ) |&gt; \n  # Clean \"info\" Column to Purge Identifying Information (Like the Pokemon's Name Which Is in All Caps In This Data Set)\n  mutate(info = str_remove_all(info, \"\\\\b[A-Z]{2,}\\\\b\"))\n\nAnd just to make sure everything’s right, I’m going to take a look at defense statistics for each type. And… it looks like everything checks out with the distribution of defense statistics being higher in the steel, rock, ground, and dragon types. (Ghost type at #5 though?? Interesting).\n\n\nCode\n# Create a Data Frame of Level 1, 2, and 3 Evolution\npokedex &lt;- pokedex |&gt; \n  group_by(evo_set) |&gt; \n  mutate(\n    level1 = as.integer(row_number() == 1),\n    level2 = as.integer(row_number() == 2),\n    level3 = as.integer(row_number() == 3)\n  ) |&gt; \n  ungroup()\n\n# Create a Long-Form Data Set for Typing\npokedex_long &lt;- pokedex |&gt; \n  pivot_longer(\n    cols = starts_with(\"type_\"),\n    names_to = \"type_name\",\n    values_to = \"has_type\"\n  ) |&gt; \n  # This Omits Copies Where a Given Pokemon Does Not Belong to a Given Type\n  # For Dual-Typing, Two Copies Are Made for the Pokemon\n  filter(has_type == 1) |&gt; \n  mutate(type_name = str_remove(type_name, \"type_\"))\n\n# Distribution Plot by Defense\nggplot(pokedex_long, aes(\n  x = defense,\n  y = fct_reorder(type_name, defense, .fun = mean),\n  fill = type_name\n  )) +\n  stat_halfeye(\n    adjust = 0.7,\n    point_interval = NULL\n  ) +\n  scale_y_discrete(labels = type_names) +  \n  scale_fill_manual(values = type_colors) + \n  blog_theme() +\n  labs(\n    title = \"\",\n    x = \"Defense\",\n    y = \"\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\nDistribution of Defense Stats Across Typing\n\n\n\n\n\n\nStrategy\nPrior to getting into classification strategies, we need to clean up the text data. Below, I am just creating a basic text data cleaning function that does some of the basics like standardizing punctuation, setting all text to lower-case, removing stop words, etc.\n\n# Store Stop Words for Filtering\nstop_words_data &lt;- stop_words$word\n\n# Create a Function to Clean Text\nclean_text &lt;- function(text) {\n  text |&gt;\n    # Set to Lowercase\n    str_to_lower() |&gt; \n    # Remove Possessive \"'s\"\n    str_replace_all(\"'s\\\\b\", \"\") |&gt;   \n    # Remove Plural Possessives\n    str_replace_all(\"\\\\bs'\\\\b\", \"\") |&gt;   \n    # Remove Punctuation\n    str_replace_all(\"[^a-z\\\\s]\", \" \") |&gt;  \n    # Remove Extra Space\n    str_squish() |&gt;  \n    # Tokenize\n    str_split(\" \") |&gt;                              \n    unlist() |&gt;\n    # Remove Stop Words\n    discard(~ .x %in% stop_words_data) |&gt;   \n    # Stem Worlds\n    wordStem(language = \"en\") |&gt;   \n    # Combine Everything Back Into a String\n    str_c(collapse = \" \")                      \n}\n\ntidy_pokemon &lt;- pokedex |&gt;\n  mutate(info = map_chr(info, clean_text)) \n\nNow, there’s a lot of different ways that we can go about trying to use Pokedex entries to classify a Pokemon’s type. The simplest method is string matching and the logic is very intuitive. Basically, for each type, we define a set of key words/terms that are assumed to be heavily associated with a given Pokemon type. Then, if the string response for a Pokedex entry contains a keyword for a given type, we predict that Pokemon as being of that given type. Super simple, but not perfect. If we define our key words too narrowly, we run the risk of a lot of false negatives. In contrast, if we are very liberal with our definition of key words, we run the risk of predicting a lot of false positives.\n\n# Establish Keywords for Each Type\ngrass_keys &lt;- c(\"grass\", \"leaf\", \"tree\", \"plant\", \"seed\", \"bloom\", \"flower\", \"forest\", \"leav\", \"grow\", \"petal\", \"aroma\", \"fruit\", \"vine\", \"branch\", \"bud\", \"cotton\", \"appl\", \"root\")\nfire_keys &lt;- c(\"fire\", \"hot\", \"flame\", \"burn\", \"ember\", \"heat\", \"fieri\", \"firebal\", \"fahrenheit\", \"magma\")\nwater_keys &lt;- c(\"water\", \"shell\", \"swim\", \"ocean\", \"sea\", \"fin\", \"bubbl\", \"depth\")\nnormal_keys &lt;- c(\"normal\", \"common\", \"fur\")\nelectric_keys &lt;- c(\"electr\", \"light\", \"thunder\", \"lightn\", \"spark\", \"charg\", \"shock\")\nice_keys &lt;- c(\"ice\", \"freez\", \"frozen\", \"snow\", \"ici\", \"cold\", \"snowi\", \"frigid\")\nfighting_keys &lt;- c(\"fight\", \"punch\", \"kick\", \"martial\", \"muscl\", \"strong\", \"strength\", \"fist\")\npoison_keys &lt;- c(\"poison\", \"venom\", \"sting\", \"toxic\", \"gas\", \"fluid\", \"acid\", \"spray\")\nground_keys &lt;- c(\"ground\", \"underground\", \"earthquak\", \"mud\", \"sand\", \"dig\")\nflying_keys &lt;- c(\"fli\", \"wing\", \"feather\", \"beak\", \"bird\", \"wind\", \"sky\", \"flap\", \"cloud\", \"air\")\npsychic_keys &lt;- c(\"psychic\", \"brain\", \"space\", \"control\", \"dream\", \"futur\", \"intellig\")\nbug_keys &lt;- c(\"bug\", \"silk\", \"stinger\", \"insect\", \"honey\", \"pollen\", \"shell\")\nrock_keys &lt;- c(\"rock\", \"hard\", \"fossil\", \"ancient\", \"stone\", \"boulder\", \"coal\")\nghost_keys &lt;- c(\"ghost\", \"spirit\", \"curs\", \"soul\", \"possess\")\ndragon_keys &lt;- c(\"dragon\", \"scale\", \"power\", \"wing\", \"tusk\")\ndark_keys &lt;- c(\"dark\", \"night\", \"fear\")\nsteel_keys &lt;- c(\"steel\", \"magnet\", \"iron\", \"metal\", \"armor\", \"hammer\")\nfairy_keys &lt;- c(\"light\", \"fairi\")\n\n# Create a List of Types\ntypes &lt;- c(\n  \"grass\", \"fire\", \"water\", \"normal\", \"electric\", \"ice\",\n  \"fighting\", \"poison\", \"ground\", \"flying\", \"psychic\", \"bug\",\n  \"rock\", \"ghost\", \"dragon\", \"dark\", \"steel\", \"fairy\"\n)\n\n# Make Predictions Based on String Matching\nsm_preds &lt;- tidy_pokemon |&gt; \n  mutate(\n    method = \"string_matching\",\n    grass_match = if_else(str_detect(info, str_c(grass_keys, collapse = \"|\")), 1, 0),\n    fire_match = if_else(str_detect(info, str_c(fire_keys, collapse = \"|\")), 1, 0),\n    water_match = if_else(str_detect(info, str_c(water_keys, collapse = \"|\")), 1, 0),\n    normal_match = if_else(str_detect(info, str_c(normal_keys, collapse = \"|\")), 1, 0),\n    electric_match = if_else(str_detect(info, str_c(electric_keys, collapse = \"|\")), 1, 0),\n    ice_match = if_else(str_detect(info, str_c(ice_keys, collapse = \"|\")), 1, 0),\n    fighting_match = if_else(str_detect(info, str_c(fighting_keys, collapse = \"|\")), 1, 0),\n    poison_match = if_else(str_detect(info, str_c(poison_keys, collapse = \"|\")), 1, 0),\n    ground_match = if_else(str_detect(info, str_c(ground_keys, collapse = \"|\")), 1, 0),\n    flying_match = if_else(str_detect(info, str_c(flying_keys, collapse = \"|\")), 1, 0),\n    psychic_match = if_else(str_detect(info, str_c(psychic_keys, collapse = \"|\")), 1, 0),\n    bug_match = if_else(str_detect(info, str_c(bug_keys, collapse = \"|\")), 1, 0),\n    rock_match = if_else(str_detect(info, str_c(rock_keys, collapse = \"|\")), 1, 0),\n    ghost_match = if_else(str_detect(info, str_c(ghost_keys, collapse = \"|\")), 1, 0),\n    dragon_match = if_else(str_detect(info, str_c(dragon_keys, collapse = \"|\")), 1, 0),\n    dark_match = if_else(str_detect(info, str_c(dark_keys, collapse = \"|\")), 1, 0),\n    steel_match = if_else(str_detect(info, str_c(steel_keys, collapse = \"|\")), 1, 0),\n    fairy_match = if_else(str_detect(info, str_c(fairy_keys, collapse = \"|\")), 1, 0)\n  )\n\n# Create an Empty Data Frame to Store Results\nstring_matching_metrics &lt;- data.frame()\n\n# Loop Through Each Type for Predictive Metrics\nfor (type in types) {\n  \n  match_col &lt;- paste0(type, \"_match\")\n  true_col &lt;- paste0(\"type_\", type)\n  \n  true &lt;- factor(ifelse(sm_preds[[true_col]] == 1, \"yes\", \"no\"), levels = c(\"yes\", \"no\"))\n  predicted &lt;- factor(ifelse(sm_preds[[match_col]] == 1, \"yes\", \"no\"), levels = c(\"yes\", \"no\"))\n  \n  # Confusion Matrix\n  cm &lt;- confusionMatrix(predicted, true, positive = \"yes\")\n  \n  # Store Metrics\n  string_matching_metrics &lt;- bind_rows(string_matching_metrics, data.frame(\n    type = type,\n    sensitivity = cm$byClass[\"Sensitivity\"],\n    specificity = cm$byClass[\"Specificity\"],\n    predictors = \"string_matching\"\n  ))\n}\n\nBut perhaps if we use a bit more of a complicated method, it will be able to navigate the nuances of the data to navigate this tricky balance. To investigate this, I am using a topic modeling algorithm known as seeded Latent Dirichlet Allocation (SLDA), developed by Watanabe and Baturo (2024). To understand how this works, consider the basic Latent Dirichlet Allocation (LDA) model, a very popular, if not limited, topic modeling approach. LDA assumes that each text entry in a corpus of documents represents some mixture of topics. For example, if we have topics A, B, C, and D, then the contents of Document X could be 20% from Topic A, 50% from Topic B, 25% from Topic C, and 5% from Topic D. Furthermore, each topic itself is a mixture of words. If we are looking at a cooking manual and Topic A is “Baking” then some of the words that might contribute to Topic A might be “baking”, “oven”, “cookies”, “pre-heat”, etc. But, there’s a noticeable problem with LDA.\nWhen executing LDA, the user simply tells the algorithm how many topics to look for (\\(k\\) = 4? 8? 100?). There is no right answer for how many topics there truly are, so there’s some educated guesswork to this. This itself is not a fundamental problem for LDA, but the actual core issue is that standard LDA has no way of telling the user what each topic is. Topic 1 is simply Topic 1 and it’s up to the user to figure out what that topic is actually about. Further, if a corpus truly only consists of 4 topics, but you tell the LDA algorithm to search for 10, it will sure enough find 10, even though 10 topics do not exist, so have fun interpreting your results and making sense of the topics it spits back out at you.\nA lot of researchers in NLP have extended the LDA algorithm, either by enhancing the model directly or developing entirely different topics models. For this blog, I will be working with seeded LDA which really helps to overcome the problem of the algorithm being agnostic on the types of topics in the corpus. This is very useful when you already know what the topics are in a corpus, you just don’t know which topics each and every document in the corpus belongs to. And the {seededlda} R package makes this all very simple. Much like string matching, you can just supply key words/terms to the algorithm to define both the number of topics that the algorithm will look for, but also to strongly inform what those topics are. For example, I am telling the algorithm that “grass” is a topic and the following words (“grass”, “leaf”, “tree”, “plant”, “seed”, “bloom”, “flower”, “forest”) are key components to that topic. With these “seeds” supplied for each type, I will get\\(\\theta\\) (theta) values back, which tell me the predicted topic distribution for each Pokemon.\nBut what are you actually supposed to do with these \\(\\theta\\) values? I don’t think there’s anything tried and true (please correct me if I’m wrong), so I’m going to look at a lot of different things. A simple approach is to develop a simple classification rule using the \\(\\theta\\) values. For example, let’s say we’ve ran our SLDA algorithm and get \\(\\theta\\) values for each type of Bulbasaur. Let’s say they look something like this for the highest 5 \\(\\theta\\) values.\n\nmock_slda &lt;- data.frame(\n  `Type (Topic)` = c(\"Grass\", \"Poison\", \"Bug\", \"Fairy\", \"Normal\"),\n  `Theta` = c(0.46, 0.29, 0.07, 0.02, 0.001),\n  check.names = FALSE\n)\n\ntt(mock_slda) |&gt;\n  style_tt(align = \"c\")                    \n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Type (Topic)\n                Theta\n              \n        \n        \n        \n                \n                  Grass\n                  0.460\n                \n                \n                  Poison\n                  0.290\n                \n                \n                  Bug\n                  0.070\n                \n                \n                  Fairy\n                  0.020\n                \n                \n                  Normal\n                  0.001\n                \n        \n      \n    \n\n\n\nSince we know Pokemon, these mock results make sense. These results tell us that 75% of Bulbasaur’s Pokedex entry comes from either the grass or the poison type. But how do we make predictions based on this? One idea is to just define an arbitrary classification rule using the theta value. For example, we could say that, for a each type, if \\(\\theta\\) &gt;= 0.5, then we assign it to that type. But, such a rule is very punitive when the algorithm is uncertain and spreads \\(\\theta\\) across a lot of types. Further, for dual types, it is systematically biased since the distribution should be more evenly split. A way around this is to just lower the cut-off, i.e. \\(\\theta\\) &gt;= 0.4, \\(\\theta\\) &gt;= 0.3, etc. So, that’s what I do. I’ll be evaluating one strategy by using SLDA \\(\\theta\\) values and assign three very simple classification rules.\n\nset.seed(1234)\n\n# Tokenize the Data for SLDA\ntokens &lt;- tidy_pokemon |&gt; \n  unnest_tokens(word, info) \n\n# Cast a DFM\ndfm &lt;- tokens |&gt; \n  count(id, word) |&gt; \n  cast_dfm(id, word, n)\n\n# Define Seeds\nlda_seeds &lt;- quanteda::dictionary(list(\n  grass = grass_keys,\n  fire = fire_keys,\n  water = water_keys,\n  normal = normal_keys,\n  electric = electric_keys,\n  ice = ice_keys,\n  fighting = fighting_keys,\n  poison = poison_keys,\n  ground = ground_keys,\n  flying = flying_keys,\n  psychic = psychic_keys,\n  bug = bug_keys,\n  rock = rock_keys,\n  ghost = ghost_keys,\n  dragon = dragon_keys,\n  dark = dark_keys,\n  steel = steel_keys,\n  fairy = fairy_keys\n))\n\nseeded_lda &lt;- textmodel_seededlda(dfm, lda_seeds)\n\n# Extract Thetas from SLDA\nslda_theta &lt;- seeded_lda$theta |&gt; \n  as.data.frame() |&gt; \n  rownames_to_column(var = \"id\") |&gt; \n  mutate(id = as.integer(id)) |&gt; \n  left_join(tidy_pokemon, by = \"id\")\n\n# Make Predictions with SLDA 30% Rule\nslda30_preds &lt;- slda_theta |&gt; \n  mutate(\n    method = \"SLDA_30%\",\n    grass_match_slda_30 = if_else(str_detect(info, str_c(grass_keys, collapse = \"|\")) | grass &gt;= 0.3, 1, 0),\n    fire_match_slda_30 = if_else(str_detect(info, str_c(fire_keys, collapse = \"|\")) | fire &gt;= 0.3, 1, 0),\n    water_match_slda_30 = if_else(str_detect(info, str_c(water_keys, collapse = \"|\")) | water &gt;= 0.3, 1, 0),\n    normal_match_slda_30 = if_else(str_detect(info, str_c(normal_keys, collapse = \"|\")) | normal &gt;= 0.3, 1, 0),\n    electric_match_slda_30 = if_else(str_detect(info, str_c(electric_keys, collapse = \"|\")) | electric &gt;= 0.3, 1, 0),\n    ice_match_slda_30 = if_else(str_detect(info, str_c(ice_keys, collapse = \"|\")) | ice &gt;= 0.3, 1, 0),\n    fighting_match_slda_30 = if_else(str_detect(info, str_c(fighting_keys, collapse = \"|\")) | fighting &gt;= 0.3, 1, 0),\n    poison_match_slda_30 = if_else(str_detect(info, str_c(poison_keys, collapse = \"|\")) | poison &gt;= 0.3, 1, 0),\n    ground_match_slda_30 = if_else(str_detect(info, str_c(ground_keys, collapse = \"|\")) | ground &gt;= 0.3, 1, 0),\n    flying_match_slda_30 = if_else(str_detect(info, str_c(flying_keys, collapse = \"|\")) | flying &gt;= 0.3, 1, 0),\n    psychic_match_slda_30 = if_else(str_detect(info, str_c(psychic_keys, collapse = \"|\")) | psychic &gt;= 0.3, 1, 0),\n    bug_match_slda_30 = if_else(str_detect(info, str_c(bug_keys, collapse = \"|\")) | bug &gt;= 0.3, 1, 0),\n    rock_match_slda_30 = if_else(str_detect(info, str_c(rock_keys, collapse = \"|\")) | rock &gt;= 0.3, 1, 0),\n    ghost_match_slda_30 = if_else(str_detect(info, str_c(ghost_keys, collapse = \"|\")) | ghost &gt;= 0.3, 1, 0),\n    dragon_match_slda_30 = if_else(str_detect(info, str_c(dragon_keys, collapse = \"|\")) | dragon &gt;= 0.3, 1, 0),\n    dark_match_slda_30 = if_else(str_detect(info, str_c(dark_keys, collapse = \"|\")) | dark &gt;= 0.3, 1, 0),\n    steel_match_slda_30 = if_else(str_detect(info, str_c(steel_keys, collapse = \"|\")) | steel &gt;= 0.3, 1, 0),\n    fairy_match_slda_30 = if_else(str_detect(info, str_c(fairy_keys, collapse = \"|\")) | fairy &gt;= 0.3, 1, 0)\n  )\n\n# Create an Empty Data Frame to Store Results\ntheta_30_sm_metrics &lt;- data.frame()\n\n# Loop Through Each Type for Predictive Metrics\nfor (type in types) {\n  \n  match_col &lt;- paste0(type, \"_match_slda_30\")\n  true_col &lt;- paste0(\"type_\", type)\n  \n  true &lt;- factor(ifelse(slda30_preds[[true_col]] == 1, \"yes\", \"no\"), levels = c(\"yes\", \"no\"))\n  predicted &lt;- factor(ifelse(slda30_preds[[match_col]] == 1, \"yes\", \"no\"), levels = c(\"yes\", \"no\"))\n  \n  # Confusion Matrix\n  cm &lt;- confusionMatrix(predicted, true, positive = \"yes\")\n  \n  # Store Metrics\n  theta_30_sm_metrics &lt;- bind_rows(theta_30_sm_metrics, data.frame(\n    type = type,\n    sensitivity = cm$byClass[\"Sensitivity\"],\n    specificity = cm$byClass[\"Specificity\"],\n    predictors = \"SLDA_30\"\n  ))\n}\n\n# Make Predictions with SLDA 40% Rule\nslda40_preds &lt;- slda_theta |&gt; \n  mutate(\n    method = \"SLDA_40%\",\n    grass_match_slda_40 = if_else(str_detect(info, str_c(grass_keys, collapse = \"|\")) | grass &gt;= 0.4, 1, 0),\n    fire_match_slda_40 = if_else(str_detect(info, str_c(fire_keys, collapse = \"|\")) | fire &gt;= 0.4, 1, 0),\n    water_match_slda_40 = if_else(str_detect(info, str_c(water_keys, collapse = \"|\")) | water &gt;= 0.4, 1, 0),\n    normal_match_slda_40 = if_else(str_detect(info, str_c(normal_keys, collapse = \"|\")) | normal &gt;= 0.4, 1, 0),\n    electric_match_slda_40 = if_else(str_detect(info, str_c(electric_keys, collapse = \"|\")) | electric &gt;= 0.4, 1, 0),\n    ice_match_slda_40 = if_else(str_detect(info, str_c(ice_keys, collapse = \"|\")) | ice &gt;= 0.4, 1, 0),\n    fighting_match_slda_40 = if_else(str_detect(info, str_c(fighting_keys, collapse = \"|\")) | fighting &gt;= 0.4, 1, 0),\n    poison_match_slda_40 = if_else(str_detect(info, str_c(poison_keys, collapse = \"|\")) | poison &gt;= 0.4, 1, 0),\n    ground_match_slda_40 = if_else(str_detect(info, str_c(ground_keys, collapse = \"|\")) | ground &gt;= 0.4, 1, 0),\n    flying_match_slda_40 = if_else(str_detect(info, str_c(flying_keys, collapse = \"|\")) | flying &gt;= 0.4, 1, 0),\n    psychic_match_slda_40 = if_else(str_detect(info, str_c(psychic_keys, collapse = \"|\")) | psychic &gt;= 0.4, 1, 0),\n    bug_match_slda_40 = if_else(str_detect(info, str_c(bug_keys, collapse = \"|\")) | bug &gt;= 0.4, 1, 0),\n    rock_match_slda_40 = if_else(str_detect(info, str_c(rock_keys, collapse = \"|\")) | rock &gt;= 0.4, 1, 0),\n    ghost_match_slda_40 = if_else(str_detect(info, str_c(ghost_keys, collapse = \"|\")) | ghost &gt;= 0.4, 1, 0),\n    dragon_match_slda_40 = if_else(str_detect(info, str_c(dragon_keys, collapse = \"|\")) | dragon &gt;= 0.4, 1, 0),\n    dark_match_slda_40 = if_else(str_detect(info, str_c(dark_keys, collapse = \"|\")) | dark &gt;= 0.4, 1, 0),\n    steel_match_slda_40 = if_else(str_detect(info, str_c(steel_keys, collapse = \"|\")) | steel &gt;= 0.4, 1, 0),\n    fairy_match_slda_40 = if_else(str_detect(info, str_c(fairy_keys, collapse = \"|\")) | fairy &gt;= 0.4, 1, 0)\n  )\n\n# Create an Empty Data Frame to Store Results\ntheta_40_sm_metrics &lt;- data.frame()\n\n# Loop Through Each Type for Predictive Metrics\nfor (type in types) {\n  \n  match_col &lt;- paste0(type, \"_match_slda_40\")\n  true_col &lt;- paste0(\"type_\", type)\n  \n  true &lt;- factor(ifelse(slda40_preds[[true_col]] == 1, \"yes\", \"no\"), levels = c(\"yes\", \"no\"))\n  predicted &lt;- factor(ifelse(slda40_preds[[match_col]] == 1, \"yes\", \"no\"), levels = c(\"yes\", \"no\"))\n  \n  # Confusion Matrix\n  cm &lt;- confusionMatrix(predicted, true, positive = \"yes\")\n  \n  # Store Metrics\n  theta_40_sm_metrics &lt;- bind_rows(theta_40_sm_metrics, data.frame(\n    type = type,\n    sensitivity = cm$byClass[\"Sensitivity\"],\n    specificity = cm$byClass[\"Specificity\"],\n    predictors = \"SLDA_40\"\n  ))\n}\n\n# Make Predictions with SLDA 50% Rule\nslda50_preds &lt;- slda_theta |&gt; \n  mutate(\n    method = \"SLDA_50%\",\n    grass_match_slda_50 = if_else(str_detect(info, str_c(grass_keys, collapse = \"|\")) | grass &gt;= 0.5, 1, 0),\n    fire_match_slda_50 = if_else(str_detect(info, str_c(fire_keys, collapse = \"|\")) | fire &gt;= 0.5, 1, 0),\n    water_match_slda_50 = if_else(str_detect(info, str_c(water_keys, collapse = \"|\")) | water &gt;= 0.5, 1, 0),\n    normal_match_slda_50 = if_else(str_detect(info, str_c(normal_keys, collapse = \"|\")) | normal &gt;= 0.5, 1, 0),\n    electric_match_slda_50 = if_else(str_detect(info, str_c(electric_keys, collapse = \"|\")) | electric &gt;= 0.5, 1, 0),\n    ice_match_slda_50 = if_else(str_detect(info, str_c(ice_keys, collapse = \"|\")) | ice &gt;= 0.5, 1, 0),\n    fighting_match_slda_50 = if_else(str_detect(info, str_c(fighting_keys, collapse = \"|\")) | fighting &gt;= 0.5, 1, 0),\n    poison_match_slda_50 = if_else(str_detect(info, str_c(poison_keys, collapse = \"|\")) | poison &gt;= 0.5, 1, 0),\n    ground_match_slda_50 = if_else(str_detect(info, str_c(ground_keys, collapse = \"|\")) | ground &gt;= 0.5, 1, 0),\n    flying_match_slda_50 = if_else(str_detect(info, str_c(flying_keys, collapse = \"|\")) | flying &gt;= 0.5, 1, 0),\n    psychic_match_slda_50 = if_else(str_detect(info, str_c(psychic_keys, collapse = \"|\")) | psychic &gt;= 0.5, 1, 0),\n    bug_match_slda_50 = if_else(str_detect(info, str_c(bug_keys, collapse = \"|\")) | bug &gt;= 0.5, 1, 0),\n    rock_match_slda_50 = if_else(str_detect(info, str_c(rock_keys, collapse = \"|\")) | rock &gt;= 0.5, 1, 0),\n    ghost_match_slda_50 = if_else(str_detect(info, str_c(ghost_keys, collapse = \"|\")) | ghost &gt;= 0.5, 1, 0),\n    dragon_match_slda_50 = if_else(str_detect(info, str_c(dragon_keys, collapse = \"|\")) | dragon &gt;= 0.5, 1, 0),\n    dark_match_slda_50 = if_else(str_detect(info, str_c(dark_keys, collapse = \"|\")) | dark &gt;= 0.5, 1, 0),\n    steel_match_slda_50 = if_else(str_detect(info, str_c(steel_keys, collapse = \"|\")) | steel &gt;= 0.5, 1, 0),\n    fairy_match_slda_50 = if_else(str_detect(info, str_c(fairy_keys, collapse = \"|\")) | fairy &gt;= 0.5, 1, 0)\n  )\n\n# Create an Empty Data Frame to Store Results\ntheta_50_sm_metrics &lt;- data.frame()\n\n# Loop Through Each Type for Predictive Metrics\nfor (type in types) {\n  \n  match_col &lt;- paste0(type, \"_match_slda_50\")\n  true_col &lt;- paste0(\"type_\", type)\n  \n  true &lt;- factor(ifelse(slda50_preds[[true_col]] == 1, \"yes\", \"no\"), levels = c(\"yes\", \"no\"))\n  predicted &lt;- factor(ifelse(slda50_preds[[match_col]] == 1, \"yes\", \"no\"), levels = c(\"yes\", \"no\"))\n  \n  # Confusion Matrix\n  cm &lt;- confusionMatrix(predicted, true, positive = \"yes\")\n  \n  # Store Metrics\n  theta_50_sm_metrics &lt;- bind_rows(theta_50_sm_metrics, data.frame(\n    type = type,\n    sensitivity = cm$byClass[\"Sensitivity\"],\n    specificity = cm$byClass[\"Specificity\"],\n    predictors = \"SLDA_50\"\n  ))\n}\n\nAlternatively, we could just plug the \\(\\theta\\) values into a machine learning algorithm and hope that it can pick up on some added nuance in assigning classifications, so we’ll give that a try as well.\n\n# Establish the Cross Validation Parameters\ncv &lt;- trainControl(\n  method = \"cv\",\n  number = 5,\n  classProbs = TRUE,\n  summaryFunction = twoClassSummary,\n  savePredictions = TRUE,\n  sampling = \"smote\"\n)\n\ntype_cols &lt;- paste0(\"type_\", c(\n  \"normal\", \"fire\", \"water\", \"grass\", \"electric\", \"ice\", \"fighting\",\n  \"poison\", \"ground\", \"flying\", \"psychic\", \"bug\", \"rock\",\n  \"ghost\", \"dragon\", \"dark\", \"steel\", \"fairy\"\n))\n\n# Pre Learning Cleaning\nslda_theta &lt;- slda_theta |&gt; \n  # Convert All Outcomes to Factors\n  mutate(across(all_of(type_cols), ~ factor(ifelse(. == 1, \"yes\", \"no\"), levels = c(\"no\", \"yes\"))))\n\n# Train Each Model\ngrass_model_thetas &lt;- train(type_grass ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\nfire_model_thetas &lt;- train(type_fire ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\nwater_model_thetas &lt;- train(type_water ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\nnormal_model_thetas &lt;- train(type_normal ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\nelectric_model_thetas &lt;- train(type_electric ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\nice_model_thetas &lt;- train(type_ice ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\nfighting_model_thetas &lt;- train(type_fighting ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\npoison_model_thetas &lt;- train(type_poison ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\nground_model_thetas &lt;- train(type_ground ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\nflying_model_thetas &lt;- train(type_flying ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\npsychic_model_thetas &lt;- train(type_psychic ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\nbug_model_thetas &lt;- train(type_bug ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\nrock_model_thetas &lt;- train(type_rock ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\nghost_model_thetas &lt;- train(type_ghost ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\ndragon_model_thetas &lt;- train(type_dragon ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\ndark_model_thetas &lt;- train(type_dark ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\nsteel_model_thetas &lt;- train(type_steel ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\nfairy_model_thetas &lt;- train(type_fairy ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\n\n# Store Models in a List\nmodel_list_thetas &lt;- list(\n  grass = grass_model_thetas,\n  fire = fire_model_thetas,\n  water = water_model_thetas,\n  normal = normal_model_thetas,\n  electric = electric_model_thetas,\n  ice = ice_model_thetas,\n  fighting = fighting_model_thetas,\n  poison = poison_model_thetas,\n  ground = ground_model_thetas,\n  flying = flying_model_thetas,\n  psychic = psychic_model_thetas,\n  bug = bug_model_thetas,\n  rock = rock_model_thetas,\n  ghost = ghost_model_thetas,\n  dragon = dragon_model_thetas,\n  dark = dark_model_thetas,\n  steel = steel_model_thetas,\n  fairy = fairy_model_thetas\n)\n\n# Create an Empty Data Frame to Store Results\ntheta_model_metrics &lt;- data.frame()\n\n# Loop Through Each Model\nfor (type in names(model_list_thetas)) {\n  \n  model &lt;- model_list_thetas[[type]]\n  \n  # Get Best Tuning Parameters\n  best_mtry &lt;- model$bestTune$mtry\n  preds_best &lt;- model$pred |&gt; filter(mtry == best_mtry)\n  \n  # Confusion Matrix\n  cm &lt;- confusionMatrix(preds_best$pred, preds_best$obs, positive = \"yes\")\n  \n  # Store Metrics\n  theta_model_metrics &lt;- bind_rows(theta_model_metrics, data.frame(\n    type = type,\n    sensitivity = cm$byClass[\"Sensitivity\"],\n    specificity = cm$byClass[\"Specificity\"],\n    predictors = \"thetas_only\"\n  ))\n}\n\nIn the event that \\(\\theta\\) values alone aren’t sufficient, maybe it’s possible that adding a binary indicator for string matches alongside \\(\\theta\\) values as predictors in a machine learning algorithm aids in predictive accuracy, so I’m going to give that a shot as well.\n\n# Add String Matches to Theta Data Frame\nslda_theta &lt;- slda_theta |&gt; \n  mutate(\n    grass_match = if_else(str_detect(info, str_c(grass_keys, collapse = \"|\")), 1, 0),\n    fire_match = if_else(str_detect(info, str_c(fire_keys, collapse = \"|\")), 1, 0),\n    water_match = if_else(str_detect(info, str_c(water_keys, collapse = \"|\")), 1, 0),\n    normal_match = if_else(str_detect(info, str_c(normal_keys, collapse = \"|\")), 1, 0),\n    electric_match = if_else(str_detect(info, str_c(electric_keys, collapse = \"|\")), 1, 0),\n    ice_match = if_else(str_detect(info, str_c(ice_keys, collapse = \"|\")), 1, 0),\n    fighting_match = if_else(str_detect(info, str_c(fighting_keys, collapse = \"|\")), 1, 0),\n    poison_match = if_else(str_detect(info, str_c(poison_keys, collapse = \"|\")), 1, 0),\n    ground_match = if_else(str_detect(info, str_c(ground_keys, collapse = \"|\")), 1, 0),\n    flying_match = if_else(str_detect(info, str_c(flying_keys, collapse = \"|\")), 1, 0),\n    psychic_match = if_else(str_detect(info, str_c(psychic_keys, collapse = \"|\")), 1, 0),\n    bug_match = if_else(str_detect(info, str_c(bug_keys, collapse = \"|\")), 1, 0),\n    rock_match = if_else(str_detect(info, str_c(rock_keys, collapse = \"|\")), 1, 0),\n    ghost_match = if_else(str_detect(info, str_c(ghost_keys, collapse = \"|\")), 1, 0),\n    dragon_match = if_else(str_detect(info, str_c(dragon_keys, collapse = \"|\")), 1, 0),\n    dark_match = if_else(str_detect(info, str_c(dark_keys, collapse = \"|\")), 1, 0),\n    steel_match = if_else(str_detect(info, str_c(steel_keys, collapse = \"|\")), 1, 0),\n    fairy_match = if_else(str_detect(info, str_c(fairy_keys, collapse = \"|\")), 1, 0)\n  )\n\n# Train Models\ngrass_model_thetas_sm &lt;- train(type_grass ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy + grass_match, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\nfire_model_thetas_sm &lt;- train(type_fire ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy + fire_match, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\nwater_model_thetas_sm &lt;- train(type_water ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy + water_match, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\nnormal_model_thetas_sm &lt;- train(type_normal ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy + normal_match, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\nelectric_model_thetas_sm &lt;- train(type_electric ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy + electric_match, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\nice_model_thetas_sm &lt;- train(type_ice ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy + ice_match, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\nfighting_model_thetas_sm &lt;- train(type_fighting ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy + fighting_match, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\npoison_model_thetas_sm &lt;- train(type_poison ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy + poison_match, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\nground_model_thetas_sm &lt;- train(type_ground ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy + ground_match, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\nflying_model_thetas_sm &lt;- train(type_flying ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy + flying_match, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\npsychic_model_thetas_sm &lt;- train(type_psychic ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy + psychic_match, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\nbug_model_thetas_sm &lt;- train(type_bug ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy + bug_match, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\nrock_model_thetas_sm &lt;- train(type_rock ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy + rock_match, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\nghost_model_thetas_sm &lt;- train(type_ghost ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy + ghost_match, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\ndragon_model_thetas_sm &lt;- train(type_dragon ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy + dragon_match, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\ndark_model_thetas_sm &lt;- train(type_dark ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy + dark_match, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\nsteel_model_thetas_sm &lt;- train(type_steel ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy + steel_match, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\nfairy_model_thetas_sm &lt;- train(type_fairy ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy + fairy_match, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\n\n# Store Models in a List\nmodel_list_thetas_sm &lt;- list(\n  grass = grass_model_thetas_sm,\n  fire = fire_model_thetas_sm,\n  water = water_model_thetas_sm,\n  normal = normal_model_thetas_sm,\n  electric = electric_model_thetas_sm,\n  ice = ice_model_thetas_sm,\n  fighting = fighting_model_thetas_sm,\n  poison = poison_model_thetas_sm,\n  ground = ground_model_thetas_sm,\n  flying = flying_model_thetas_sm,\n  psychic = psychic_model_thetas_sm,\n  bug = bug_model_thetas_sm,\n  rock = rock_model_thetas_sm,\n  ghost = ghost_model_thetas_sm,\n  dragon = dragon_model_thetas_sm,\n  dark = dark_model_thetas_sm,\n  steel = steel_model_thetas_sm,\n  fairy = fairy_model_thetas_sm\n)\n\n# Create an Empty Data Frame to Store Results\ntheta_sm_model_metrics &lt;- data.frame() \n\n# Loop Through Each Model\nfor (type in names(model_list_thetas_sm)) {\n  \n  model &lt;- model_list_thetas_sm[[type]]\n  \n  # Get Best Tuning Parameters\n  best_mtry &lt;- model$bestTune$mtry\n  preds_best &lt;- model$pred |&gt; filter(mtry == best_mtry)\n  \n  # Confusion Matrix\n  cm &lt;- confusionMatrix(preds_best$pred, preds_best$obs, positive = \"yes\")\n  \n  # Extract Metrics\n  theta_sm_model_metrics &lt;- bind_rows(theta_sm_model_metrics, data.frame(\n    type = type,\n    sensitivity = cm$byClass[\"Sensitivity\"],\n    specificity = cm$byClass[\"Specificity\"],\n    predictors = \"thetas_and_string_matching\"\n  ))\n}\n\nLastly, maybe predictions can be enhanced if, alongside \\(\\theta\\) values and string matching dummies, we include Pokemon statistics as predictors (HP, attack, defense, speed, etc.). Speed is heavily associated with types like electric, flying, rock, and steel. Types like psychic, electric, and ghost tend to have higher special attack statistics. So maybe including these as predictors could be helpful.\n\n# Train Models\ngrass_model_thetas_sm_stats &lt;- train(type_grass ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy + grass_match + hp + attack + defense + s_attack + s_defense + speed, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\nfire_model_thetas_sm_stats &lt;- train(type_fire ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy + fire_match + hp + attack + defense + s_attack + s_defense + speed, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\nwater_model_thetas_sm_stats &lt;- train(type_water ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy + water_match + hp + attack + defense + s_attack + s_defense + speed, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\nnormal_model_thetas_sm_stats &lt;- train(type_normal ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy + normal_match + hp + attack + defense + s_attack + s_defense + speed, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\nelectric_model_thetas_sm_stats &lt;- train(type_electric ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy + electric_match + hp + attack + defense + s_attack + s_defense + speed, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\nice_model_thetas_sm_stats &lt;- train(type_ice ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy + ice_match + hp + attack + defense + s_attack + s_defense + speed, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\nfighting_model_thetas_sm_stats &lt;- train(type_fighting ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy + fighting_match + hp + attack + defense + s_attack + s_defense + speed, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\npoison_model_thetas_sm_stats &lt;- train(type_poison ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy + poison_match + hp + attack + defense + s_attack + s_defense + speed, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\nground_model_thetas_sm_stats &lt;- train(type_ground ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy + ground_match + hp + attack + defense + s_attack + s_defense + speed, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\nflying_model_thetas_sm_stats &lt;- train(type_flying ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy + flying_match + hp + attack + defense + s_attack + s_defense + speed, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\npsychic_model_thetas_sm_stats &lt;- train(type_psychic ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy + psychic_match + hp + attack + defense + s_attack + s_defense + speed, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\nbug_model_thetas_sm_stats &lt;- train(type_bug ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy + bug_match + hp + attack + defense + s_attack + s_defense + speed, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\nrock_model_thetas_sm_stats &lt;- train(type_rock ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy + rock_match + hp + attack + defense + s_attack + s_defense + speed, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\nghost_model_thetas_sm_stats &lt;- train(type_ghost ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy + ghost_match + hp + attack + defense + s_attack + s_defense + speed, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\ndragon_model_thetas_sm_stats &lt;- train(type_dragon ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy + dragon_match + hp + attack + defense + s_attack + s_defense + speed, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\ndark_model_thetas_sm_stats &lt;- train(type_dark ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy + dark_match + hp + attack + defense + s_attack + s_defense + speed, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\nsteel_model_thetas_sm_stats &lt;- train(type_steel ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy + steel_match + hp + attack + defense + s_attack + s_defense + speed, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\nfairy_model_thetas_sm_stats &lt;- train(type_fairy ~ grass + fire + water + electric + normal + ice + fighting + poison + ground + flying + psychic + bug + rock + ghost + dragon + dark + steel + fairy + fairy_match + hp + attack + defense + s_attack + s_defense + speed, data = slda_theta, method = \"rf\", metric = \"ROC\", trControl = cv)\n\n# Store Models in a List\nmodel_list_thetas_sm_stats &lt;- list(\n  grass = grass_model_thetas_sm_stats,\n  fire = fire_model_thetas_sm_stats,\n  water = water_model_thetas_sm_stats,\n  normal = normal_model_thetas_sm_stats,\n  electric = electric_model_thetas_sm_stats,\n  ice = ice_model_thetas_sm_stats,\n  fighting = fighting_model_thetas_sm_stats,\n  poison = poison_model_thetas_sm_stats,\n  ground = ground_model_thetas_sm_stats,\n  flying = flying_model_thetas_sm_stats,\n  psychic = psychic_model_thetas_sm_stats,\n  bug = bug_model_thetas_sm_stats,\n  rock = rock_model_thetas_sm_stats,\n  ghost = ghost_model_thetas_sm_stats,\n  dragon = dragon_model_thetas_sm_stats,\n  dark = dark_model_thetas_sm_stats,\n  steel = steel_model_thetas_sm_stats,\n  fairy = fairy_model_thetas_sm_stats\n)\n\n# Create an Empty Data Frame to Store Results\ntheta_sm_stats_model_metrics &lt;- data.frame()\n\n# Loop Through Each Model\nfor (type in names(model_list_thetas_sm_stats)) {\n  \n  model &lt;- model_list_thetas_sm_stats[[type]]\n  \n  # Get Best Tuning Parameters\n  best_mtry &lt;- model$bestTune$mtry\n  preds_best &lt;- model$pred |&gt; filter(mtry == best_mtry)\n  \n  # Confusion Matrix\n  cm &lt;- confusionMatrix(preds_best$pred, preds_best$obs, positive = \"yes\")\n  \n  # Extract metrics\n  theta_sm_stats_model_metrics &lt;- bind_rows(theta_sm_stats_model_metrics, data.frame(\n    type = type,\n    sensitivity = cm$byClass[\"Sensitivity\"],\n    specificity = cm$byClass[\"Specificity\"],\n    predictors = \"thetas_string_matching_and_stats\"\n  ))\n}\n\nLastly, we arrive at the question of, “should we even care about topic modeling at all?”. Why bother with all of this code when I can just give a spreadsheet and a prompt to ChatGPT and ask it predict Pokemon types based on their Pokedex entries? That’s a fair enough point, so I tried that as well with Chat GPT-5 with the following prompt:\n\n“Please read this spreadsheet. Each row is a Pokémon and contains its Pokédex entry. Based on the info column alone, predict up to 2 likely Pokémon types for each entry. Return a table in spreadsheet form. There should be 18 columns stored as a dummy for each type. Preface each type columns with”llm_” and then the type name.”\n\n\n# Import the Spreadsheet Chat GPT Gave Me\nllm_pokemon &lt;- read_excel(\"data/Pokemon_LLM_Types.xlsx\")\n\n# Merge with the Tidy Pokemon Data Set\nllm_pokemon &lt;- full_join(llm_pokemon, tidy_pokemon, by = c(\"id\", \"name\", \"info\"))\n\n# Create an Empty Data Frame to Store Results\nllm_metrics &lt;- data.frame()\n\n# Loop Through Each Type for Predictive Metrics\nfor (type in types) {\n  \n  match_col &lt;- paste0(\"llm_\", type)\n  true_col &lt;- paste0(\"type_\", type)\n  \n  true &lt;- factor(ifelse(llm_pokemon[[true_col]] == 1, \"yes\", \"no\"), levels = c(\"yes\", \"no\"))\n  predicted &lt;- factor(ifelse(llm_pokemon[[match_col]] == 1, \"yes\", \"no\"), levels = c(\"yes\", \"no\"))\n  \n  # Confusion Matrix\n  cm &lt;- confusionMatrix(predicted, true, positive = \"yes\")\n  \n  # Store Metrics\n  llm_metrics &lt;- bind_rows(llm_metrics, data.frame(\n    type = type,\n    sensitivity = cm$byClass[\"Sensitivity\"],\n    specificity = cm$byClass[\"Specificity\"],\n    predictors = \"LLM\"\n  ))\n}\n\nNow, with all of these predictions and different strategies merged into a single data frame, let’s see how they fared.\n\nall_metrics &lt;- full_join(string_matching_metrics, theta_model_metrics, by = c(\"type\", \"sensitivity\", \"specificity\", \"predictors\")) |&gt; \n  full_join(theta_sm_model_metrics, by = c(\"type\", \"sensitivity\", \"specificity\", \"predictors\")) |&gt; \n  full_join(theta_sm_stats_model_metrics, by = c(\"type\", \"sensitivity\", \"specificity\", \"predictors\")) |&gt; \n  full_join(theta_30_sm_metrics, by = c(\"type\", \"sensitivity\", \"specificity\", \"predictors\")) |&gt; \n  full_join(theta_40_sm_metrics, by = c(\"type\", \"sensitivity\", \"specificity\", \"predictors\")) |&gt; \n  full_join(theta_50_sm_metrics, by = c(\"type\", \"sensitivity\", \"specificity\", \"predictors\")) |&gt; \n  full_join(llm_metrics, by = c(\"type\", \"sensitivity\", \"specificity\", \"predictors\"))\n\n\n\nResults\nThe plot below shows the sensitivity (accurate positive predictions / total positives) for each strategy. Because each type is a separate classification task, the balance between positives and negatives (i.e. “is grass type” vs. “is not grass type”) is heavily skewed towards negatives. In this context, by default, just about any approach you use will be pretty conservative and push predictions towards negatives. As a result, I’m more interested in whether any of my approaches can identify the type a Pokemon is rather than the type a Pokemon is not. Each strategy has 18 dots, one for each type.\nAnd, as you can see, yikes! It’s not looking great. I’d arbitrarily feel alright is the dots were clustered in the 60-80% range, but we are not really observing that here. Furthermore, the methods that were the most technically “complex” by far perform the worst. All three strategies that relied on a machine learning algorithm really struggled to detect positive values. Now, part of this could be explained by a lack of technical complexity regarding these algorithms, but I find it interesting that they still perform so poorly. Likewise, the LLM-generated predictions also perform much worse than the simple classification rules that rely on strings or theta values. But again, similar to the ML-based predictions, the LLM-generated predictions could also be improved with more attention focused in the prompts being given.\n\n\nCode\nall_metrics$predictors &lt;- factor(\n  all_metrics$predictors,\n  levels = c(\n    \"LLM\",\n    \"thetas_string_matching_and_stats\",\n    \"thetas_and_string_matching\",\n    \"thetas_only\",\n    \"SLDA_50\",\n    \"SLDA_40\",\n    \"SLDA_30\",\n    \"string_matching\"\n  )\n)\n\nggplot(all_metrics, aes(x = sensitivity, y = predictors, fill = predictors)) +\n  # Add Stacked Dots\n  stat_dots(\n        side = \"top\",\n        scale = 0.6,\n        dotsize = 1,\n        color = \"black\"\n    ) +\n  # Add the Distribution Overlay\n  stat_slab(\n        side = \"top\",\n        scale = 0.9,\n        alpha = 0.5\n    ) +\n  # Custom Colors and Labels\n  scale_fill_manual(\n        values = c(\n            \"string_matching\" = \"#6a040f\",\n            \"thetas_only\" = \"#dc2f02\",\n            \"thetas_and_string_matching\" = \"#dc2f02\",\n            \"thetas_string_matching_and_stats\" = \"#dc2f02\",\n            \"SLDA_30\" = \"#f48c06\",\n            \"SLDA_40\" = \"#f48c06\",\n            \"SLDA_50\" = \"#f48c06\",\n            \"LLM\" = \"#ffba08\"\n        ),\n        labels = c(\n            \"string_matching\" = \"String Matching\",\n            \"thetas_only\" = \"Thetas Only\\nas Predictors\",\n            \"thetas_and_string_matching\" = \"Thetas +\\nString Matching\\nas Predictors\",\n            \"thetas_string_matching_and_stats\" = \"Thetas +\\nString Matching +\\nPoke Stats as\\nPredictors\",\n            \"SLDA_30\" = \"30% Theta\\nClassification\\nRule\",\n            \"SLDA_40\" = \"40% Theta\\nClassification\\nRule\",\n            \"SLDA_50\" = \"50% Theta\\nClassification\\nRule\",\n            \"LLM\" = \"LLM Predictions\"\n        )\n    ) +\n    scale_y_discrete(\n        labels = c(\n            \"string_matching\" = \"String Matching\",\n            \"thetas_only\" = \"Thetas Only\\nas Predictors\",\n            \"thetas_and_string_matching\" = \"Thetas +\\nString Matching\\nas Predictors\",\n            \"thetas_string_matching_and_stats\" = \"Thetas +\\nString Matching +\\nPoke Stats as\\nPredictors\",\n            \"SLDA_30\" = \"30% Theta\\nClassification\\nRule\",\n            \"SLDA_40\" = \"40% Theta\\nClassification\\nRule\",\n            \"SLDA_50\" = \"50% Theta\\nClassification\\nRule\",\n            \"LLM\" = \"LLM Predictions\"\n        )\n    ) +\n    labs(\n        title = \"\",\n        x = \"Sensitivity (True Positive Rate)\",\n        y = \"\"\n    ) +\n  scale_x_continuous(limits = c(0, 0.8)) +\n    blog_theme() +\n    theme(legend.position = \"none\")\n\n\n\n\n\nPredictive Performance of Different Strategies\n\n\n\n\nThe big takeaway here should not be that the simplest approach is always the best. I think it would be fair to make two inferences based on these results. First, a simple approach can actually perform quite well on its own. However, while the floor for predictive performance may be pretty high, the ceiling might not be as high as more technical approaches. This leads to the second point. If you want to use a more technically complex approach (machine learning) or LLMs, it likely will require a bit of up-front lift to ensure that it performs better than simpler classification strategies. This is probably especially true for this example where the text data that we have for each Pokemon is incredibly sparse. I personally don’t think the results would look like if, for example, we had a page a piece for each Pokemon.\nNext, we can evaluate which types each approach was able to predict with higher or lower accuracy. And there’s some neat stuff here. Regardless of the strategy used, some types are much more easy to classify than others. The most prominent examples are the ice, electric, and fire types. I’m not very surprised with this. Personally, while writing out the key terms/seeds, I had a much easier time thinking of these terms when dealing with the “elemental” and less abstract Pokemon types. We have tons of words that we use to describe things like electricity and fire and so it’s easy to find a list of terms that are distinct for those types.\n\n\nCode\nall_metrics$predictors &lt;- factor(\n  all_metrics$predictors,\n  levels = rev(c(\n    \"LLM\",\n    \"thetas_string_matching_and_stats\",\n    \"thetas_and_string_matching\",\n    \"thetas_only\",\n    \"SLDA_50\",\n    \"SLDA_40\",\n    \"SLDA_30\",\n    \"string_matching\"\n  ))\n)\n\nggplot(all_metrics, aes(x = predictors, y = sensitivity, color = type)) +\n  geom_jitter(width = 0.25, height = 0, size = 3, alpha = 0.8) +\n  labs(\n    title = \"\",\n    x = \"\",\n    y = \"Sensitivity\",\n    color = \"Type\"\n  ) + \n  scale_fill_manual(\n    labels = c(\n    \"string_matching\" = \"String Matching\",\n    \"thetas_only\" = \"Thetas Only\\nas Predictors\",\n    \"thetas_and_string_matching\" = \"Thetas +\\nString\\nMatching\\nas Predictors\",\n    \"thetas_string_matching_and_stats\" = \"Thetas +\\nString\\nMatching +\\nPoke Stats as\\nPredictors\",\n    \"SLDA_30\" = \"30% Theta\\nClassification\\nRule\",\n    \"SLDA_40\" = \"40% Theta\\nClassification\\nRule\",\n    \"SLDA_50\" = \"50% Theta\\nClassification\\nRule\",\n    \"LLM\" = \"LLM Predictions\"\n  )\n) +\nscale_x_discrete(\n  labels = c(\n    \"string_matching\" = \"String Matching\",\n    \"thetas_only\" = \"Thetas Only\\nas Predictors\",\n    \"thetas_and_string_matching\" = \"Thetas +\\nString\\nMatching\\nas Predictors\",\n    \"thetas_string_matching_and_stats\" = \"Thetas +\\nString\\nMatching +\\nPoke Stats as\\nPredictors\",\n    \"SLDA_30\" = \"30% Theta\\nClassification\\nRule\",\n    \"SLDA_40\" = \"40% Theta\\nClassification\\nRule\",\n    \"SLDA_50\" = \"50% Theta\\nClassification\\nRule\",\n    \"LLM\" = \"LLM Predictions\"\n  )\n) +\n  scale_color_manual(\n    values = type_colors,   \n    labels = type_names    \n  ) +\n  scale_y_continuous(limits = c(0, 0.8)) +\n  blog_theme()\n\n\n\n\n\nPredictive Performance of Different Strategies by Pokemon Type\n\n\n\n\nBut what about the types that are much more abstract? Like fairy, dragon, and normal? As you can see, and as expected, a lot of the approaches used really struggled to classify these types correctly. After all, what makes a fairy-type Pokemon fairy-type? What about normal types? For normal types in particular, what really makes them normal is just the absence of any elemental or magical powers. A type being defined in negative terms is one that is really hard to classify, at least when solely using the information (Pokedex entries) for this analysis.\n\n\nConclusion\nOverall, can you reliably predict a Pokemon’s type by using information from its Pokedex entries (and, in some cases, its statistics)? Not really. Or at least, not at a casual glance. Further, much like all other things data science-y, your results will only be as good as your data. Likewise, as your data worsens, either in scope or quality, “simpler” strategies are not always the least desired."
  },
  {
    "objectID": "blog/index.html#section",
    "href": "blog/index.html#section",
    "title": "Blog",
    "section": "2025",
    "text": "2025\n\n\n\n\n\n\n\n\n\n\nGetting Comfortable with Expressing Beliefs as Distributions\n\n\n\n\n\n\nbayes\n\n\nvisualization\n\n\n\nThinking about our beliefs as distributions is not super intuitive for most people, which creates a stumbling blog for getting into Bayesian statistics. Check this blog out to break down the mystique!\n\n\n\n\n\nOctober 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting Pokemon Types with Pokedex Text Entries\n\n\n\n\n\n\nnlp\n\n\nllm\n\n\ntopic modeling\n\n\nmachine learning\n\n\npokemon\n\n\n\nI’ve been looking for a way to work with Pokemon data for a while now, so I shoehorned topic modeling and natural language processing in there to finally justify it…\n\n\n\n\n\nSeptember 15, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThree Points for Bayesian Statistics in Conflict Research\n\n\n\n\n\n\nbayes\n\n\ncausal inference\n\n\ncivil conflict\n\n\n\nBayesian inference is rarely used in quantitative conflict research (and a lot of the broader social sciences too). In this blog post, I point out some reasons for why we ought to change that!\n\n\n\n\n\nJuly 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThe Overlooked Part of Confounder Adjustment\n\n\n\n\n\n\ncausal inference\n\n\nregression\n\n\n\nObviously, we all know that we need to adjust for confounders, but what happens when we don’t adjust for them the right way?\n\n\n\n\n\nJune 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBe Wary of Regression-Based Causal Effects! (Or At Least Be Careful)…\n\n\n\n\n\n\ncausal inference\n\n\nregression\n\n\n\nRegression is the backbone of almost all of causal inference. Yet, it is not perfect and it can have unique issues (some of which it won’t even warn you about)! In this blog, we’ll tackle what this hidden potential problem is, how to identify it, and how to respond to it.\n\n\n\n\n\nMay 26, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGive Your Hypotheses Space!\n\n\n\n\n\n\ncausal inference\n\n\ndags\n\n\nsimulation\n\n\n\nIt’s tempting to throw a bunch of variables of interest into a model and evaluate each variable’s ‘impact’ on the outcome, but proceed at your own caution! Check this blog out to see why that approach is most likely not the best idea…\n\n\n\n\n\nMay 15, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDo NFL Referees Favor the Kansas City Chiefs?\n\n\n\n\n\n\nnfl\n\n\nvisualization\n\n\n\nThe back-to-back NFL champions are poised for a third consecutive Super Bowl win (three-peat), which has never been accomplished before. Is such a feat possible due to unfair officiating? Let’s look at the data!\n\n\n\n\n\nJanuary 31, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-1",
    "href": "blog/index.html#section-1",
    "title": "Blog",
    "section": "2024",
    "text": "2024\n\n\n\n\n\n\n\n\n\n\nSimulating Complex Panel Data to Validate Model Performance in Estimating Dynamic Treatment Effects\n\n\n\n\n\n\ncausal inference\n\n\nsimulation\n\n\ndags\n\n\nipw\n\n\nmsm\n\n\npanel data\n\n\n\nMaking causal inferences is hard, and making causal inferences is harder with complex panel data. In this blog post, learn how to test the validity of your panel data models using simulation!\n\n\n\n\n\nDecember 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCausal Inference for Casuals\n\n\n\n\n\n\ncausal inference\n\n\nsimulation\n\n\ndags\n\n\nregression\n\n\n\nLearn about the core concepts of causal inference and the motivations for causal analysis with the help of simulation. Beginners to causal inference welcome!\n\n\n\n\n\nOctober 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting the Outcome of NFL Games in the 2024-2025 Season\n\n\n\n\n\n\nmachine learning\n\n\nnfl\n\n\n\nFollow my attempt to predict the winners and losers of each game in the 2024-2025 NFL season.\n\n\n\n\n\nSeptember 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAn Introduction to Dynamic Causal Inference\n\n\n\n\n\n\ncausal inference\n\n\npanel data\n\n\ndags\n\n\n\nLearn the basics to making causal inferences with panel/longitudinal data.\n\n\n\n\n\nJuly 8, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Research",
    "section": "",
    "text": "My personal research covers the intersection of causal inference, quasi-experimental design, and policy evaluation and a variety of topics that I find interesting, including (but not limited to): peace and conflict, democracy and elections, political and economic development, and the NFL (professional American football)."
  },
  {
    "objectID": "research/index.html#dissertation",
    "href": "research/index.html#dissertation",
    "title": "Research",
    "section": "Dissertation",
    "text": "Dissertation\n\n“Rethinking the Study of Conflict and Peace: Making Causal Inferences in Quantitative Conflict and Peace Research” \n\nManuscript \nCode (Chapter 2) \nCode (Chapter 3)"
  }
]