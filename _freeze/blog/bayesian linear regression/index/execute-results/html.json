{
  "hash": "2d2ad5989a49965133f365f1b9aa9a89",
  "result": {
    "markdown": "---\ntitle: \"Attempting Bayesian Linear Regression\"\ndate: 2024-02-19\ndescription: placeholder\ncategories:\n  - placeholder\n---\n\n\nOver the past year, I have dabbled in the realm of Bayesian statistics. I have found that I really enjoy reading theoretical justifications for Bayesian statistics and I resonate with the critiques of frequentist statistics that Bayesians will offer. But... I have intentionally handicapped my skill level in Bayesian statistics over this past year by focusing on reading Bayesian *texts* and not *doing* Bayesian statistics myself.\n\nThere are a number of reasons for this. But they all fundamentally rest on a fear of failure and a desire to avoid frustration. I don't think anyone will claim that Bayesian statistics is *easier* than the existing paradigm you get taught in grad school. You really have to put the work in to understand Bayesian concepts and then you're rewarded with a more complex research process as a result. That's intimidating! On the latter issue, after 40 hours of work per week and other life stuff, I am pretty tired when the work day is over and I try (as best as I can) to reserve my weekends for rest and spending time with my family. Needless to say, I'm not very patient at the end of the work day/week, so I've been avoiding branching out into Bayesian programming for that reason as well. But here I go. This is my very elementary foray into Bayesian programming, starting off with good ole linear regression.\n\n# Core Arguments for Bayesian Statistics\n\nGiven the aforementioned references to being tired and frustrated with up-skilling, I should probably clarify why I'm putting this effort in at all. After my year of reading Bayesian texts, I've been pretty heavily convicted that both my academic and vocational work could be seriously improved by the adoption of Bayesian methods. This sentiment is motivated by three observations that I briefly detail.\n\nFirst, I'm not sure that any stakeholder (academics, the public, clients, etc.) particularly *care* about the question that frequentist null hypothesis significance testing (NHST) answers. Typically, when we develop a hypothesis and estimate some result, we all want to know about uncertainty pertaining to *that* result. But this is not the norm that is taught in graduate training, it is not the norm in probably any social scientific discipline, and it is not the norm in most business practices. Instead, NHST remains, and the merit of results are judged by how small a p-value is. Rarely do we stop and ask whether a p-value answers a question that is meaningful. A p-value gives us the probability of observing data at least as extreme as ours, under the assumption that the null hypothesis is true. In this way, NHST does not evaluate the probability of *our* hypothesis... and it doesn't even evaluate the probability of the *null hypothesis*. Of course, knowing that your data is unlikely to emerge in a world where we *know* the null is true is kind of useful information. But such information does not provide evidence that formally tests the null, and, in many questions, it is a bit unfair to frame the magnitude and \"significance\" of results by comparing observed data to imaginary data from the straw man world of the \"no effect at all\". Conventional NHST fails against the critic who acknowledges that there could be a tiny effect, but wants to know if the true effect is one of any noticeable magnitude.\n\nSecond, NHST is regularly employed in situations for which it makes no sense to apply it. For reference, my academic background is in conflict research. \"What causes conflict to break out?\", \"what causes wars to be prolonged or come to and end?, \"what prevents wars from recurring?\", etc. Unlike other disciplines, a lot of this research is not using samples from a broader population. But its that very assumption that motivates frequentist inference. Under that paradigm, it is understood that a data set for a given study is simply a sample from a broader population. By pure chance, you could have obtained the sample that estimates an effect when, in reality, no such effect exists beyond your sample. This idea motivates the intuition behind estimating a distribution of possible null values and examining where your data falls along this distribution. By pure chance alone, a null world could generate samples that indicate non-null effect sizes when no such effect exists. Is your result comparable to the known possible data that could exist? Yeah... so this entire logic breaks down when we aren't working with samples. Going back to my academic background, our \"samples\" were oftentimes *all* countries in conflict from 1946-2010 (or something like that). There is no consideration of alternative data, because we contain the information concerning the population itself. When I forced myself to interpret my dissertation results as directly as possible, not allowing myself to rely on short-hand terms like \"statistically significant\", I realized how inapplicable frequentist inference appeared for my entire dissertation. Even worse was the realization that the *entire discipline* operated under the frequentist paradigm and there was/is seemingly no intra-discipline commentary on this practice!\n\nLastly, I think NHST just begs for professionals and laymen to misinterpret results. Recall that NHST examines the probability of observing given data under the assumption that the null is true; $P(D | H_O)$. Also note that when researchers state a hypothesis, the hypothesis generally concerns the existence/non-existence of an effect of interest. Implicitly, researchers want to know $P(H | D)$, which is something that cannot be estimated with their standard inferential toolkit. Nonetheless, scholars will use p-values and confidence intervals to interpret their findings as such. I mean, you can't really blame them. Directly stating what a p-value or 95% confidence interval actually tells you begs the question of why we frame/accept/reject findings based on these (commonly misinterpreted) statistics. \n\nOkay, with that taken care of, let's introduce the data I'll be using.\n\n# The V-Dem Data Set and the Research Question\n\nBecause I come from the world of comparative and international politics, the Varieties of Democracy (V-Dem) data set was my best friend in graduate school. It contains so, so much information and I regularly use it for personal projects and as a toy data set for instructional purposes. So, I'm going to take a subset of the data to make this blog more efficient.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'dplyr' was built under R version 4.2.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n\n```{.r .cell-code}\nlibrary(vdemdata)\n```\n:::\n\n\n# Estimation\n\n## Specifying Priors\n\n## Posterior Estimation\n\n## Diagnostics\n\n## Visualizing Marginal Effects\n\n# Conclusion and Caveats\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}