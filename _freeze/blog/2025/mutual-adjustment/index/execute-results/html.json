{
  "hash": "54918d67a56992c791f1f29bba40bbeb",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Give Your Hypotheses Space!\"\ndate: 2025-05-15\ndescription: \"It's tempting to throw a bunch of variables of interest into a model and evaluate each variable's 'impact' on the outcome, but proceed at your own caution! Check this blog out to see why that approach is most likely not the best idea...\"\nimage: \"figures/bad_controls_bias.png\"\ntoc: true\nexecute:\n  warning: false\n  error: false\n  message: false\ntoc-location: \"left\"\ntoc-title: \"Contents\"\ncategories:\n  - causal inference\n  - dags\n  - simulation\n---\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Load Libraries\npacman::p_load(\n  \"dplyr\", # Data Manipulation\n  \"dagitty\", # DAGs\n  \"ggdag\", # Visualizing DAGs\n  \"ggraph\", # More DAG Visualizations\n  \"ggplot2\", # Data Visualization\n  \"tidyr\", # Re-Shaping\n  \"modelsummary\", # Presenting Model Results\n  install = FALSE\n)\n\n# Define a Custom Theme\nblog_theme <- function() {\n  theme_bw() +  \n    theme(\n      panel.grid.major = element_line(color = \"gray80\", size = 0.3),\n      panel.grid.minor = element_blank(),\n      panel.border = element_blank(),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      plot.title = element_text(face = \"bold\", size = 16, margin = margin(t = 0, r = 0, b = 15, l = 0)),\n      axis.title.x = element_text(face = \"bold\", size = 14, margin = margin(t = 15, r = 0, b = 0, l = 0)),\n      axis.title.y = element_text(face = \"bold\", size = 14, margin = margin(t = 0, r = 15, b = 0, l = 0)),\n      strip.text = element_text(face = \"bold\"),\n      axis.text.x = element_text(face = \"bold\", size = 10), \n      axis.text.y = element_text(face = \"bold\", size = 10), \n      axis.ticks.x = element_blank(), \n      axis.ticks.y = element_blank(), \n      strip.background = element_rect(fill = \"grey80\", color = NA),\n      legend.title = element_text(face = \"bold\", size = 14),\n      legend.text = element_text(face = \"bold\", size = 10, color = \"grey25\"),\n    )\n}\n\n# Establish a Custom Color Scheme\ncolors <- c(\n  \"1\" = \"#133a29\",\n  \"2\" = \"#ab3d29\",\n  \"3\" = \"#f9ba6d\",\n  \"4\" = \"#314318\",\n  \"5\" = \"#63221f\"\n)\n```\n:::\n\n\n\n# Intro\n\nIt is tempting for practitioners and clients alike to feel that we can develop a \"model\" that can answer our questions. But the idea of shooting for a single model is flawed, regardless of what you're trying to do. If you are seeking to do machine learning things, there's a lot of virtue in ensemble approaches. From the causal inference approach, it is *very easy* to let a single model do too much and, as a consequence, severely damage the credibility of your results.\n\nThis is especially a problem when asked to do some sort of \"shotgun causal inference\". I have no clue if someone else has used this term before, but the desire for shotgun causal inference is apparent in academic and non-academic settings. It all starts with this question: \"what are the predictors/explanations/factors that impact\", etc. What is the problem here? Well, for one, the task is insurmountable. *All* the things?\n\nIn reality, such questions can really be boiled down to \"find as many variables that you think explain some outcome of interest and test them\". But this is actually a very hard causal inference problem (which is part of the reason why academic hypotheses tend to be narrow and only focus on one potential causal factor for each research project). Even though shotgun causal inference is hard, there is one rule that should never be violated...\n\nWhen testing multiple hypotheses, *do not* throw everything into the same model and assume that this model provides interpretable results. To demonstrate why this should be avoided, let's go over quick refresher on the concept of \"bad controls\" and what they do. This will be important for understanding why we should not test multiple hypotheses within the same model.\n\n# A Refresher on the Consequences of \"Bad Controls\"\n\nFor those not familiar, I go into greater detail on this subject in a [prior blog post](https://brian-lookabaugh.github.io/website-brianlookabaugh/blog/2024/causal-inference-simulation/#dont-be-a-control-freak-overadjustment-bias). However, the TLDR of this point is that different hypotheses require different adjustment sets. Adjustment sets are the unique combination/specification of covariates that are required to be adjusted for to give any $X \\rightarrow Y$ relationship a causal interpretation.\n\nThe problem is that the same covariate can have a different causal role for different hypotheses. Confounders ($X \\leftarrow Z \\rightarrow Y$) should always be controlled for, but what happens when a confounder for the $H_{1} \\rightarrow Y$ relationship is a *bad control* for the $H_{2} \\rightarrow Y$ relationship?\n\nLet's assume that we are interested in testing the $H_{1} \\rightarrow Y$ relationship and we add some other controls that, while necessary for the *other* hypotheses, are *harmful* for identifying $H_{1} \\rightarrow M \\rightarrow Y$. Below, I simulate three different situations where the influence of these bad controls ranges from large to medium to small. I simulate two post-treatment controls (a mediator: $X \\rightarrow Y$ and a collider: $X \\rightarrow C \\leftarrow Y$) and an instance of M-Bias. The idea for this simulation exercise is to show how much our model deviates from estimating the true causal effect when adjusting for other bad covariates.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nn <- 100000\n\n# Data Where \"Bad Controls\" Have a Large Effect\nlarge <- tibble(\n  u1 = rnorm(n, 0, 1),\n  u2 = rnorm(n, 2, 0.5),\n  m_bias = 0.8 * u1 + 0.8 * u2 + rnorm(n, 0, 0.25),\n  confounder = rnorm(n, 0, 1),\n  exposure = confounder * 1.5 + u1 * 0.8 + rnorm(n, 0, 0.5),\n  mediator = exposure * 2 + rnorm(n, 0, 0.5),\n  outcome = (1.5 * exposure + mediator) + confounder * 3 + u2 * 0.8 + rnorm(n, 0, 0.5),\n  collider = exposure * 1.25 + outcome * 0.7 + rnorm(n, 0, 0.5)\n)\n\n# Estimate 4 Regressions Adding Increasingly More Controls\nlarge_unbiased <- lm(outcome ~ exposure + confounder, data = large)\nlarge_mediator <- lm(outcome ~ exposure + confounder + mediator, data = large)\nlarge_collider <- lm(outcome ~ exposure + confounder + mediator + collider, data = large)\nlarge_mbias <- lm(outcome ~ exposure + confounder + mediator + collider + m_bias, data = large)\n\n# Data Where \"Bad Controls\" Have a Medium Effect\nmedium <- tibble(\n  u1 = rnorm(n, 0, 1),\n  u2 = rnorm(n, 2, 0.5),\n  m_bias = 0.4 * u1 + 0.4 * u2 + rnorm(n, 0, 0.25),\n  confounder = rnorm(n, 0, 1),\n  exposure = confounder * 0.75 + u1 * 0.4 + rnorm(n, 0, 0.5),\n  mediator = exposure * 1 + rnorm(n, 0, 0.5),\n  outcome = (1.5 * exposure + mediator) + confounder * 3 + u2 * 0.4 + rnorm(n, 0, 0.5),\n  collider = exposure * 0.625 + outcome * 0.7 + rnorm(n, 0, 0.5)\n)\n\n# Estimate 4 Regressions Adding Increasingly More Controls\nmedium_unbiased <- lm(outcome ~ exposure + confounder, data = medium)\nmedium_mediator <- lm(outcome ~ exposure + confounder + mediator, data = medium)\nmedium_collider <- lm(outcome ~ exposure + confounder + mediator + collider, data = medium)\nmedium_mbias <- lm(outcome ~ exposure + confounder + mediator + collider + m_bias, data = medium)\n\n# Data Where \"Bad Controls\" Have a Small Effect Effect\nsmall <- tibble(\n  u1 = rnorm(n, 0, 1),\n  u2 = rnorm(n, 2, 0.5),\n  m_bias = 0.2 * u1 + 0.2 * u2 + rnorm(n, 0, 0.25),\n  confounder = rnorm(n, 0, 1),\n  exposure = confounder * 0.375 + u1 * 0.2 + rnorm(n, 0, 0.5),\n  mediator = exposure * 0.5 + rnorm(n, 0, 0.5),\n  outcome = (1.5 * exposure + mediator) + confounder * 3 + u2 * 0.2 + rnorm(n, 0, 0.5),\n  collider = exposure * 0.3125 + outcome * 0.7 + rnorm(n, 0, 0.5)\n)\n\nsmall_unbiased <- lm(outcome ~ exposure + confounder, data = small)\nsmall_mediator <- lm(outcome ~ exposure + confounder + mediator, data = small)\nsmall_collider <- lm(outcome ~ exposure + confounder + mediator + collider, data = small)\nsmall_mbias <- lm(outcome ~ exposure + confounder + mediator + collider + m_bias, data = small)\n```\n:::\n\n\n\nAgain, strictly for testing $H_{1}$, the data generating process can be represented with this DAG:\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndgp1_dag <- dagitty('dag {\n    bb=\"0,0,1,1\"\n    \"M_Bias_Node\" [pos=\"0.47,0.35\"]\n    \"Unobserved (Outcome)\" [pos=\"0.545,0.28\"]\n    \"Unobserved (Treatment)\" [pos=\"0.4,0.28\"]\n    Collider [pos=\"0.47,0.56\"]\n    Confounder [pos=\"0.47,0.225\"]\n    Exposure [exposure,pos=\"0.4,0.4\"]\n    Mediator [pos=\"0.47,0.45\"]\n    Outcome [outcome,pos=\"0.545,0.4\"]\n    \"Unobserved (Outcome)\" -> \"M_Bias_Node\"\n    \"Unobserved (Outcome)\" -> Outcome\n    \"Unobserved (Treatment)\" -> \"M_Bias_Node\"\n    \"Unobserved (Treatment)\" -> Exposure\n    Confounder -> Exposure\n    Confounder -> Outcome\n    Exposure -> Collider\n    Exposure -> Mediator\n    Exposure -> Outcome\n    Mediator -> Outcome\n    Outcome -> Collider\n  }') %>%\n  tidy_dagitty() %>%\n  mutate(y = 1 - y, yend = 1 - yend) \n\nggplot(dgp1_dag, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges_link(\n    size = 4,  \n    arrow = arrow(length = unit(0.3, \"cm\"), type = \"closed\"), \n    start_cap = circle(0.05, \"npc\"),  \n    end_cap = circle(0.05, \"npc\")  \n  ) +\n  geom_point(aes(x = x, y = y), color = \"white\", size = 1) +\n  geom_label(aes(\n    label = case_when(\n      name == \"M_Bias_Node\" ~ \"M-Bias\\nCovariate\",\n      name == \"Unobserved (Outcome)\" ~ \"Latent Outcome\\nPredictor\",\n      name == \"Unobserved (Treatment)\" ~ \"Latent Treatment\\nPredictor\",\n      name == \"Collider\" ~ \"Collider\",\n      name == \"Confounder\" ~ \"Confounder\",\n      name == \"Exposure\" ~ \"Exposure\",\n      name == \"Mediator\" ~ \"Mediator\",\n      name == \"Outcome\" ~ \"Outcome\",\n      TRUE ~ name\n    ),\n    fill = case_when(\n      name == \"M_Bias_Node\" ~ \"grey20\",\n      name == \"Unobserved (Outcome)\" ~ \"#ab3d29\",\n      name == \"Unobserved (Treatment)\" ~ \"grey20\",\n      name == \"Collider\" ~ \"grey20\",\n      name == \"Confounder\" ~ \"#63221f\",\n      name == \"Exposure\" ~ \"#314318\",\n      name == \"Mediator\" ~ \"grey20\",\n      name == \"Outcome\" ~ \"#f9ba6d\",\n      TRUE ~ \"grey80\"\n    )\n  ),\n  color = \"white\", fontface = \"bold\", size = 4, label.size = 0.2, label.r = unit(0.2, \"lines\")) +\n  scale_fill_identity() +\n  theme_dag()\n```\n\n::: {.cell-output-display}\n![DAG for Bad Controls DGP](index_files/figure-html/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n\nAnd the difference in bias when I iteraviley add more bad controls can be shown in the following figure:\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\neffects <- tibble(\n   dgp = c(\"Bad Controls with Large Effects\", \"Bad Controls with Medium Effects\", \"Bad Controls with Small Effects\"),\n  unbiased = c(large_unbiased$coefficients[2], medium_unbiased$coefficients[2], small_unbiased$coefficients[2]),\n  mediator = c(large_mediator$coefficients[2], medium_mediator$coefficients[2], small_mediator$coefficients[2]),\n  collider = c(large_collider$coefficients[2], medium_collider$coefficients[2], small_collider$coefficients[2]),\n  mbias = c(large_mbias$coefficients[2], medium_mbias$coefficients[2], small_mbias$coefficients[2])\n) %>%\n  mutate(\n    true_effect = c(3.5, 2.5, 2.0),\n    bias_unbiased = true_effect - unbiased,\n    bias_mediator = true_effect - mediator,\n    bias_collider = true_effect - collider,\n    bias_mbias = true_effect - mbias,\n    effect_size = c(\"large\", \"medium\", \"small\")\n  ) %>%\n  select(effect_size, starts_with(\"bias_\")) %>%\n  pivot_longer(\n    cols = starts_with(\"bias_\"),\n    names_to = \"control_type\",\n    names_prefix = \"bias_\",\n    values_to = \"bias\"\n  )\n\nggplot(effects, aes(x = control_type, y = bias, fill = effect_size)) +\n  geom_col(position = position_dodge(width = 0.8), width = 0.7, alpha = 0.9, color = \"black\", size = 0.75) +\n  scale_x_discrete(\n    limits = c(\"unbiased\", \"mediator\", \"collider\", \"mbias\"),\n    labels = c(\n      \"unbiased\" = \"Only Confounder\",\n      \"mediator\" = \"Confounder +\\nMediator\",\n      \"collider\" = \"Confounder +\\nMediator +\\nCollider\",\n      \"mbias\" = \"Confounder +\\nMediator + Collider +\\nM-Bias\"\n    )\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"large\" = \"#133a29\",\n      \"medium\" = \"#ab3d29\",\n      \"small\" = \"#f9ba6d\"\n    ),\n    labels = c(\n      \"large\" = \"Large Effects\",\n      \"medium\" = \"Medium Effects\",\n      \"small\" = \"Small Effects\"\n    )\n  ) +\n  scale_y_continuous(\n    limits = c(0, 3.5), \n    breaks = seq(0, 3.5, by = 0.5) \n  ) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"black\", size = 1) + \n  geom_hline(yintercept = 3.5, linetype = \"dashed\", color = \"#133a29\", size = 1) +\n  geom_hline(yintercept = 2.5, linetype = \"dashed\", color = \"#ab3d29\", size = 1) +\n  geom_hline(yintercept = 2, linetype = \"dashed\", color = \"#f9ba6d\", size = 1) +\n  geom_label(aes(x = 1, y = 3.35, label = \"True Effect = 3.5\"),\n             fill = \"#133a29\", color = \"white\", fontface = \"bold\", size = 4, label.size = 0.4, label.padding = unit(0.1, \"inches\")) + \n  geom_label(aes(x = 1, y = 2.35, label = \"True Effect = 2.5\"),\n             fill = \"#ab3d29\", color = \"white\", fontface = \"bold\", size = 4, label.size = 0.4, label.padding = unit(0.1, \"inches\")) + \n  geom_label(aes(x = 1, y = 1.85, label = \"True Effect = 2.0\"),\n             fill = \"#f9ba6d\", color = \"white\", fontface = \"bold\", size = 4, label.size = 0.4, label.padding = unit(0.1, \"inches\")) +  \n  \n  labs(\n    title = \"\",\n    x = \"Covariates Included\",\n    y = \"True Effect - Estimated Effect\\n(Bias)\",\n    fill = \"'Bad Control'\\nEffect Size\"\n  ) +\n  blog_theme()\n```\n\n::: {.cell-output-display}\n![Cumulative Bias When Adding More and More Bad Controls](index_files/figure-html/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n\nThis is a lot of information, so what can we get from it? First, for the scenario when the effects of the bad controls are small, the true effect is 2. When the bad controls have a medium-sized effect, the true effect is 2.5. And when the bad controls have a large effect, the true effect is 3.5. The x-axis reports the types of variables that are adjusted for in the model and the y-axis reports the difference between the true effect and the estimated effect. If the difference is 0, then there is no bias since the regression model was able to identify the exact correct causal effect.\n\nAnd that pans out perfectly when we only adjust for *all* confounders (and, unlike the real world, it was very easy to do this, since there are not that many confounders in the scenario I simulated). The moment a mediator variable is included the bias jumps considerably. When a collider is included, the amount of bias (for the \"bad controls have large effects\" model in particular) is almost identical to the size of the true effect itself. That's bad! Inducing M-Bias doesn't seem to make things worse than they already are here, as the bias only changes marginally.\n\nThe takeaways here are that 1) we should be careful with the types of variables that we control for and 2) different $X$s required different adjustment sets. In the following DAG, we can clearly see a super simplified demonstration of this.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndgp2_dag <- dagitty('dag {\n    bb=\"0,0,1,1\"\n    X1 [exposure,pos=\"0.38,0.43\"]\n    X2 [exposure,pos=\"0.47,0.2\"]\n    X3 [exposure,pos=\"0.58,0.43\"]\n    Y [outcome,pos=\"0.47,0.43\"]\n    Z1 [pos=\"0.38,0.25\"]\n    Z2 [pos=\"0.58,0.25\"]\n    Z3 [pos=\"0.47,0.6\"]\n    X1 -> Y\n    X1 -> Z3\n    X2 -> Y\n    X2 -> Z1\n    X3 -> Y\n    Z1 -> X1\n    Z1 -> Y\n    Z2 -> X2\n    Z2 -> Y\n    Z3 -> X3\n    Z3 -> Y\n  }')\n\nggplot(dgp2_dag, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges_link(\n    size = 4,  \n    arrow = arrow(length = unit(0.3, \"cm\"), type = \"closed\"), \n    start_cap = circle(0.05, \"npc\"),  \n    end_cap = circle(0.05, \"npc\")  \n  ) +\n  geom_point(aes(x = x, y = y), color = \"white\", size = 1) +\n  geom_label(aes(\n    label = case_when(\n      name == \"X1\" ~ \"Exposure 1\",\n      name == \"X2\" ~ \"Exposure 2\",\n      name == \"X3\" ~ \"Exposure 3\",\n      name == \"Z1\" ~ \"Coviarate 1\",\n      name == \"Z2\" ~ \"Coviarate 2\",\n      name == \"Z3\" ~ \"Coviarate 3\",\n      name == \"Y\" ~ \"Outcome\",\n      TRUE ~ name\n    ),\n    fill = case_when(\n      name == \"X1\" ~ \"#314318\",\n      name == \"X2\" ~ \"#314318\",\n      name == \"X3\" ~ \"#314318\",\n      name == \"Z1\" ~ \"#63221f\",\n      name == \"Z2\" ~ \"#63221f\",\n      name == \"Z3\" ~ \"#63221f\",\n      name == \"Y\" ~ \"#f9ba6d\",\n      TRUE ~ \"grey80\"\n    )\n  ),\n  color = \"white\", fontface = \"bold\", size = 4, label.size = 0.2, label.r = unit(0.2, \"lines\")) +\n  scale_fill_identity() +\n  theme_dag()\n```\n\n::: {.cell-output-display}\n![DAG with Three Exposures and Adjustment Sets](index_files/figure-html/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n\nAccording to this DAG, if we want to estimate the effect of Exposure 1 on the Outcome, we need to adjust for Covariate 1. But if we were to do this all in one model, we would cause a post-treatment bias for evaluating the $X_{2] \\rightarrow Y$ relationship by blocking the Exposure 2 $\\rightarrow$ Covariate 1 $\\rightarrow$ Outcome path. And if we want to estimate the effect of Exposure 3 on the Outcome, we need to adjust for Covariate 3, but then we can clearly see that adjusting for Covariate 3 blocks the Exposure 1 $\\rightarrow$ Covariate 3 $\\rightarrow$ Outcome path. If you choose to include everything in one model, you are truly damned if you do and damned if you don't!\n\nSo what to do instead? Simply give your hypotheses space! According to this DAG, to test the effect of Exposure 1 on Outcome, we only need to adjust for Covariate 1. To estimate the effect of Exposure 2 on Outcome, we only need to adjust for Covariate 2, etc. There is no one magic model. Trying to include everything in one model is a near guarantee to get very bad, *not true* results.\n\n# Don't Bother Interpreting Your Controls\n\nWhen estimating a regression model, you get a lot of numbers back and it's tempting to try and glean some information from each other. In particular, if we are on the same page and agree that we should only use one model to test each hypothesis, then the vast majority of our coefficients are for our control variables. Surely all that information is useful?\n\nNope! At least, it probably is not, and the reasons discussed above help explain why coefficients for control variables ought to be avoided. Recall that we design models to test *a specific hypothesis*. That is, we use control variables to *de-confound* the relationship of interest (whether that is the effect of $H_{1}$, $H_{2}$, etc.). Subsequently, any other estimate from that model is most likely, *not* de-confounded. To demonstrate this, I am going to simulate some data with one effect of interest that is confounded by five covariates. However, each of these covariates is itself confounded by *another* confounder that confounds the Confouder $\\rightarrow Y$ relationship, but does *not* confound the $X \\rightarrow Y$ relationship.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nn <- 100000\n\ntable2 <- tibble(\n  # Generate a Bunch of Unobserved Variables That Don't Have Anything to Do with X\n  u1 = rnorm(n, 0, 1),\n  u2 = rnorm(n, 1, 0.5),\n  u3 = rnorm(n, 0.5, 0.25),\n  u4 = rnorm(n, 2, 1.5),\n  u5 = rnorm(n, 0.75, 1),\n  # Generate Variables That Confound the X -> Y Relationship\n  z1 = u1 * 0.3 + rnorm(n, 0, 1),\n  z2 = u2 * 0.6 + rnorm(n, 0, 0.25),\n  z3 = u3 * 1.2 + rnorm(n, 0, 0.5),\n  z4 = u4 * -0.5 + rnorm(n, 0, 1),\n  z5 = u5 * -1.5 + rnorm(n, 0, 0.75),\n  # Create an Exposure of Interest\n  x = 0.2 * z1 + 1.6 * z2 + -0.3 * z3 + -0.75 * z4 + 0.15 * z5 + rnorm(n, 0, 0.25),\n  # Create an Outcome That is Impacted by All Variables\n  y = 0.3 * u1 + 0.15 * u2 + 0.5 * u3 + -0.5 * u4 + 1.2 * u5 +\n      0.8 * z1 + -1.7 * z2 + 0.9 * z3 + -0.8 * z4 + -0.7 * z5 +\n      1.25 * x + rnorm(n, 0, 0.15)\n)\n\ntable2_reg <- lm(y ~ x + z1 + z2 + z3 + z4 + z5, data = table2)\n```\n:::\n\n\n\nWe can show this data generating process with this messy DAG (and I'm keeping it messy for a reason!)\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndgp3_dag <- dagitty('dag {\n    bb=\"0,0,1,1\"\n    U1 [pos=\"0.47,0.83\"]\n    U2 [pos=\"0.47,0.28\"]\n    U3 [pos=\"0.52,0.80\"]\n    U4 [pos=\"0.36,0.80\"]\n    U5 [pos=\"0.47,0.54\"]\n    X [exposure,pos=\"0.36,0.58\"]\n    Y [outcome,pos=\"0.57,0.58\"]\n    Z1 [pos=\"0.47,0.73\"]\n    Z2 [pos=\"0.47,0.38\"]\n    Z3 [pos=\"0.57,0.69\"]\n    Z4 [pos=\"0.36,0.69\"]\n    Z5 [pos=\"0.47,0.45\"]\n    U1 -> Y\n    U1 -> Z1\n    U2 -> Y\n    U2 -> Z2\n    U3 -> Y\n    U3 -> Z3\n    U4 -> Y\n    U4 -> Z4\n    U5 -> Y\n    U5 -> Z5\n    X -> Y\n    Z1 -> X\n    Z1 -> Y\n    Z2 -> X\n    Z2 -> Y\n    Z3 -> X\n    Z3 -> Y\n    Z4 -> X\n    Z4 -> Y\n    Z5 -> X\n    Z5 -> Y\n  }')\n\nggplot(dgp3_dag, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges_link(\n    size = 4,  \n    arrow = arrow(length = unit(0.3, \"cm\"), type = \"closed\"), \n    start_cap = circle(0.05, \"npc\"),  \n    end_cap = circle(0.05, \"npc\")  \n  ) +\n  geom_point(aes(x = x, y = y), color = \"white\", size = 1) +\n  geom_label(aes(\n    label = case_when(\n      name == \"X\" ~ \"Exposure\",\n      name == \"Y\" ~ \"Outcome\",\n      name == \"Z1\" ~ \"Confounder 1\",\n      name == \"Z2\" ~ \"Confounder 2\",\n      name == \"Z3\" ~ \"Confounder 3\",\n      name == \"Z4\" ~ \"Confounder 4\",\n      name == \"Z5\" ~ \"Confounder 5\",\n      name == \"U1\" ~ \"Confounder of\\nConfounder 1\",\n      name == \"U2\" ~ \"Confounder of\\nConfounder 2\",\n      name == \"U3\" ~ \"Confounder of\\nConfounder 3\",\n      name == \"U4\" ~ \"Confounder of\\nConfounder 4\",\n      name == \"U5\" ~ \"Confounder of\\nConfounder 5\",\n      TRUE ~ name\n    ),\n    fill = case_when(\n      name == \"X\" ~ \"#314318\",\n      name == \"Y\" ~ \"#f9ba6d\",\n      name == \"Z1\" ~ \"#63221f\",\n      name == \"Z2\" ~ \"#63221f\",\n      name == \"Z3\" ~ \"#63221f\",\n      name == \"Z4\" ~ \"#63221f\",\n      name == \"Z5\" ~ \"#63221f\",\n      name == \"U1\" ~ \"grey20\",\n      name == \"U2\" ~ \"grey20\",\n      name == \"U3\" ~ \"grey20\",\n      name == \"U4\" ~ \"grey20\",\n      name == \"U5\" ~ \"grey20\",\n      TRUE ~ \"grey80\"\n    )\n  ),\n  color = \"white\", fontface = \"bold\", size = 4, label.size = 0.2, label.r = unit(0.2, \"lines\")) +\n  scale_fill_identity() +\n  theme_dag()\n```\n\n::: {.cell-output-display}\n![Messy DAG](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\nAgain, what is this DAG showing us? It's showing us that we need to adjust for Confounders 1-5 in order to de-confound the $X \\rightarrow Y$ relationship. However, if we want to de-confound the Confounder $\\rightarrow Y$ relationship, we need to adjust for Confounder of Confounders 1-5 as well. Now, imagine that we can't control for these confounder of confounders. Perhaps they are concepts that are difficult to measure or that we simply don't have the data on hand (maybe we haven't even though to collect the data!) Or, as demonstrated earlier, perhaps including some confounder of confounder introduces bias for the relationship that we are the most interested in estimating ($X \\rightarrow Y$).\n\nWith that being said, let's estimate a model regressing $Y$ on $X$, $Z1$, $Z2$, $Z3$, $Z4$, and $Z5$ and let's see how well the coefficients align with the vaues that we know to be true from our simulation.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ntrue_values <- c(\n  \"x\"  = 1.25,\n  \"z1\" = 0.8,\n  \"z2\" = -1.7,\n  \"z3\" = 0.9,\n  \"z4\" = -0.8,\n  \"z5\" = -0.7\n)\n\n# Extract Coefficients\nestimates <- coef(table2_reg)[names(true_values)]\n\n# Calculate Bias\nbias_vals <- round(estimates - true_values, 3)\n\n# Combine True Effect and Bias\nextra_columns <- data.frame(\n  \"True Value\" = true_values,\n  \"Bias\" = bias_vals\n)\n\nmodelsummary(\n  models = list(\"Coefficient\" = table2_reg),\n  output = \"markdown\",\n  gof_omit = \".\",\n  coef_map = c(\n    \"x\" = \"Exposure (X)\",\n    \"z1\" = \"Covariate Z1\",\n    \"z2\" = \"Covariate Z2\",\n    \"z3\" = \"Covariate Z3\",\n    \"z4\" = \"Covariate Z4\",\n    \"z5\" = \"Covariate Z5\"\n  ),\n  statistic = NULL,\n  stars = TRUE,\n  add_columns = extra_columns\n)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<!-- preamble start -->\n\n    <script>\n\n      function styleCell_wen1ptwnd5mirrqr8m3l(i, j, css_id) {\n          var table = document.getElementById(\"tinytable_wen1ptwnd5mirrqr8m3l\");\n          var cell = table.rows[i]?.cells[j];  // Safe navigation to avoid errors\n          if (cell) {\n              console.log(`Styling cell at (${i}, ${j}) with class ${css_id}`);\n              cell.classList.add(css_id);\n          } else {\n              console.warn(`Cell at (${i}, ${j}) not found.`);\n          }\n      }\n      function insertSpanRow(i, colspan, content) {\n        var table = document.getElementById('tinytable_wen1ptwnd5mirrqr8m3l');\n        var newRow = table.insertRow(i);\n        var newCell = newRow.insertCell(0);\n        newCell.setAttribute(\"colspan\", colspan);\n        // newCell.innerText = content;\n        // this may be unsafe, but innerText does not interpret <br>\n        newCell.innerHTML = content;\n      }\n      function spanCell_wen1ptwnd5mirrqr8m3l(i, j, rowspan, colspan) {\n        var table = document.getElementById(\"tinytable_wen1ptwnd5mirrqr8m3l\");\n        const targetRow = table.rows[i];\n        const targetCell = targetRow.cells[j];\n        for (let r = 0; r < rowspan; r++) {\n          // Only start deleting cells to the right for the first row (r == 0)\n          if (r === 0) {\n            // Delete cells to the right of the target cell in the first row\n            for (let c = colspan - 1; c > 0; c--) {\n              if (table.rows[i + r].cells[j + c]) {\n                table.rows[i + r].deleteCell(j + c);\n              }\n            }\n          }\n          // For rows below the first, delete starting from the target column\n          if (r > 0) {\n            for (let c = colspan - 1; c >= 0; c--) {\n              if (table.rows[i + r] && table.rows[i + r].cells[j]) {\n                table.rows[i + r].deleteCell(j);\n              }\n            }\n          }\n        }\n        // Set rowspan and colspan of the target cell\n        targetCell.rowSpan = rowspan;\n        targetCell.colSpan = colspan;\n      }\n      // tinytable span after\n      window.addEventListener('load', function () {\n          var cellsToStyle = [\n            // tinytable style arrays after\n          { positions: [ { i: 6, j: 2 }, { i: 6, j: 1 }, { i: 6, j: 3 },  ], css_id: 'tinytable_css_crtq8haail0lkzylytys',}, \n          { positions: [ { i: 1, j: 1 }, { i: 4, j: 1 }, { i: 5, j: 1 }, { i: 2, j: 1 }, { i: 3, j: 1 }, { i: 1, j: 2 }, { i: 2, j: 2 }, { i: 3, j: 2 }, { i: 4, j: 2 }, { i: 5, j: 2 }, { i: 3, j: 3 }, { i: 4, j: 3 }, { i: 1, j: 3 }, { i: 2, j: 3 }, { i: 5, j: 3 },  ], css_id: 'tinytable_css_mv2wr2s2vptaar7oz6a3',}, \n          { positions: [ { i: 0, j: 1 }, { i: 0, j: 3 }, { i: 0, j: 2 },  ], css_id: 'tinytable_css_tr1wczgryomw7pcdbeqp',}, \n          { positions: [ { i: 6, j: 0 },  ], css_id: 'tinytable_css_r7ug5g2kejnyopoaftk5',}, \n          { positions: [ { i: 1, j: 0 }, { i: 2, j: 0 }, { i: 3, j: 0 }, { i: 4, j: 0 }, { i: 5, j: 0 },  ], css_id: 'tinytable_css_25d8udmdeuslmywnvrvv',}, \n          { positions: [ { i: 0, j: 0 },  ], css_id: 'tinytable_css_474vh98tk3dwdv9xcu69',}, \n          ];\n\n          // Loop over the arrays to style the cells\n          cellsToStyle.forEach(function (group) {\n              group.positions.forEach(function (cell) {\n                  styleCell_wen1ptwnd5mirrqr8m3l(cell.i, cell.j, group.css_id);\n              });\n          });\n      });\n    </script>\n\n    <style>\n      /* tinytable css entries after */\n      .table td.tinytable_css_crtq8haail0lkzylytys, .table th.tinytable_css_crtq8haail0lkzylytys { text-align: center; border-bottom: solid #d3d8dc 0.1em; }\n      .table td.tinytable_css_mv2wr2s2vptaar7oz6a3, .table th.tinytable_css_mv2wr2s2vptaar7oz6a3 { text-align: center; }\n      .table td.tinytable_css_tr1wczgryomw7pcdbeqp, .table th.tinytable_css_tr1wczgryomw7pcdbeqp { text-align: center; border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; }\n      .table td.tinytable_css_r7ug5g2kejnyopoaftk5, .table th.tinytable_css_r7ug5g2kejnyopoaftk5 { text-align: left; border-bottom: solid #d3d8dc 0.1em; }\n      .table td.tinytable_css_25d8udmdeuslmywnvrvv, .table th.tinytable_css_25d8udmdeuslmywnvrvv { text-align: left; }\n      .table td.tinytable_css_474vh98tk3dwdv9xcu69, .table th.tinytable_css_474vh98tk3dwdv9xcu69 { text-align: left; border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; }\n    </style>\n    <div class=\"container\">\n      <table class=\"table table-borderless\" id=\"tinytable_wen1ptwnd5mirrqr8m3l\" style=\"width: auto; margin-left: auto; margin-right: auto;\" data-quarto-disable-processing='true'>\n        <thead>\n        \n              <tr>\n                <th scope=\"col\"> </th>\n                <th scope=\"col\">Coefficient</th>\n                <th scope=\"col\">True.Value</th>\n                <th scope=\"col\">Bias</th>\n              </tr>\n        </thead>\n        <tfoot><tr><td colspan='4'>+ p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001</td></tr></tfoot>\n        <tbody>\n                <tr>\n                  <td>Exposure (X)</td>\n                  <td>1.249***</td>\n                  <td>1.250</td>\n                  <td>-0.001</td>\n                </tr>\n                <tr>\n                  <td>Covariate Z1</td>\n                  <td>0.880***</td>\n                  <td>0.800</td>\n                  <td>0.080</td>\n                </tr>\n                <tr>\n                  <td>Covariate Z2</td>\n                  <td>-1.550***</td>\n                  <td>-1.700</td>\n                  <td>0.150</td>\n                </tr>\n                <tr>\n                  <td>Covariate Z3</td>\n                  <td>1.006***</td>\n                  <td>0.900</td>\n                  <td>0.106</td>\n                </tr>\n                <tr>\n                  <td>Covariate Z4</td>\n                  <td>-0.439***</td>\n                  <td>-0.800</td>\n                  <td>0.361</td>\n                </tr>\n                <tr>\n                  <td>Covariate Z5</td>\n                  <td>-1.341***</td>\n                  <td>-0.700</td>\n                  <td>-0.641</td>\n                </tr>\n        </tbody>\n      </table>\n    </div>\n<!-- hack to avoid NA insertion in last line -->\n```\n\n:::\n:::\n\n\n\nAs you might imagine, our coefficient is just about spot on at recovering the average causal effect of $X$ on $Y$. Not shocking since it is confounded by exactly 5 variables and we adjust for all 5. However... things start to go downhill as we try and give a causal interpretation for the control variables...\n\nCovariates 1-3 aren't too bad, although they are for sure biased, but the magnitude of bias isn't nearly as severe as Covariates 4 and 5. But if all the covaraites are biased, does that make them \"good\" or \"bad\" controls? Well, they're bad if you're trying to interpret them, but you should... not be trying to interpret them. What makes a control variable good is it's ability to de-confound the $X \\rightarrow Y$ relationship. Don't ask your controls to do more than that.\n\nIn other terms, folks will also refer to the role of control variables as \"nuisance parameters\". In other words, while adjusting for them is necessary for de-confounding purposes, they themselves have no interpretability. If you're familiar with alternative approaches like matching, this intuition makes more sense because, with matching, there are no control variable parameters to interpret in the first place. The design of matching itself adjusts for the control variables, but because you don't get any statistical output along with those control variables, there is nothing to interpret.\n\nSimilarly, we should treat regression output for controls in a similar manner. Yes, it is there and yes it exists, but it is functionally meaningless. The controls have done their work to get us our coefficient for $X$ and we should not ask for them to do anything more than that. So, why do we publish these super long regression tables with all of these numbers if most of them are just nuisance parameters? I don't know! It's kind of a waste of space! But all you and I can really do is not do this and save more page space and word counts for ourselves!\n\n# Conclusion\n\nIn conclusion, I'll return to the title of this blog and emphasize *give your hypotheses space*! The key takeaways from this are montra are:\n\n1)  Understand that each hypothesis requires its own model (it's own space) and its own unique adjustment set.\n\n2)  When asked to find out what the \"factors that explain $Y$ are\", realize that this is an incredibly complex task that will require a lot of models and a lot of time to think through what each adjustment set will need to be. You cannot throw 30 variables into a model and expect something causally useful.\n\n3)  Only interpret the output for your exposure of interest. Control variable output is causally uninterpretable.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}